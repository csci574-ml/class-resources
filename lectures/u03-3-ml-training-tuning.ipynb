{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports usually needed\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries more specific to this lecture notebook\n",
    "import os.path\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "from ml_python_class.config import DATA_DIR\n",
    "from ml_python_class.custom_funcs import fetch_compressed_data\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook wide global definitions\n",
    "DOWNLOAD_ROOT = 'https://raw.githubusercontent.com/ageron/handson-ml2/master/'\n",
    "HOUSING_URL = DOWNLOAD_ROOT + 'datasets/housing/housing.tgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook wide settings to make plots more readable and visually better to understand\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "mpl.rc('figure', titlesize=18)\n",
    "mpl.rcParams['figure.figsize'] = (10.0, 8.0) # default figure size if not specified in plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. (Chapter 2) End-to-End Machine Learning Project\n",
    "\n",
    "To recap, we will recreate all of the steps to download the data, create test and \n",
    "training data test sets, and transform the data to make it ready for machine learning\n",
    "training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data file 'housing.csv' exists, nothing done.\n"
     ]
    }
   ],
   "source": [
    "# fetch and uncompress housing data if it needs to be downloaded\n",
    "housing_file = os.path.join(DATA_DIR, 'housing.csv')\n",
    "fetch_compressed_data(HOUSING_URL, housing_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv file into a pandas DataFrame\n",
    "housing = pd.read_csv(housing_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 10)\n",
      "(4128, 10)\n"
     ]
    }
   ],
   "source": [
    "# perform a 20%/80% test/train split using stratified shuffle sampling.\n",
    "# We sample based on the temporarily defined income_cat categorical variable, to\n",
    "# ensure we have a similar distribution of incomes in both training and test \n",
    "# data\n",
    "\n",
    "# temporarily create and add an income category with 5 levels for the stratified split\n",
    "housing['income_cat'] = pd.cut(housing['median_income'],\n",
    "                               bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing['income_cat']):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "# remove the income_cat attribute we no longer need\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop('income_cat', axis=1, inplace=True)\n",
    "\n",
    "# result is 80% in train and 20% in test, stratified by income category,\n",
    "# same attributes as in raw data\n",
    "print(strat_train_set.shape)\n",
    "print(strat_test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 9)\n",
      "(16512,)\n"
     ]
    }
   ],
   "source": [
    "# strat_train_set and strat_test_set we will leave untouched so we can start from this\n",
    "# point if needed.\n",
    "\n",
    "# But to simplify things, we will reuse housing as our variable name, copying the \n",
    "# train data set, and splitting out the labels now\n",
    "housing = strat_train_set.drop('median_house_value', axis=1)\n",
    "housing_labels = strat_train_set['median_house_value'].copy()\n",
    "\n",
    "print(housing.shape)\n",
    "print(housing_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our custom transformer to define combined ttributes\n",
    "# indexes of the attributes we are using to create new combined attributes with\n",
    "# For a real project, this type of transformer might be added to the project wide \n",
    "# module for reuse across notebooks\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Create a new Transformer class to add in combined attributes to the housing\n",
    "    dataset.  This transformer inherits from both the sklearn BaseEstimator, \n",
    "    and the TransformerMixin.  The former automatically creates a `fit_transform()` \n",
    "    from our `fit()` and `transform()` methods.  The latter addes in \n",
    "    `get_params()` and `set_params()` methods to the transformer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        \"\"\"Class meta-parameter add_bedrooms_per_room.  By default this \n",
    "        transformer adds this attribute, but we can exclude by setting this \n",
    "        class metaparameter to false.\n",
    "        \"\"\"\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Define a fit() function for this transformer.  We don't really have\n",
    "        anything to do to \"fit\" this transformer in our case.\n",
    "        \"\"\"\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"The purpose of this transformer is to add in the combined attributes\n",
    "        rooms_per_household, population_per_household, and bedrooms_per_room if \n",
    "        desired.\n",
    "        \"\"\"\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        \n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            # we return the original array X with newly created features concatenated to end \n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            # we return the original array only with the default 2 features concatenated to end\n",
    "            return np.c_[X, rooms_per_household, population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally the data transformation pipeline.  In a real project we might pull out\n",
    "# all of the above, and our standard pipeline into a function to delive \n",
    "# the cleaned and transformed training data set for our ML training and tuning\n",
    "# first our numerical pipeline transformer\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then the full pipeline, adding in a transformer to do one-hot encoding on the \n",
    "# categorical attribute\n",
    "num_attribs = list(housing.drop('ocean_proximity', axis=1))\n",
    "cat_attribs = ['ocean_proximity']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "    ('cat', OneHotEncoder(), cat_attribs),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 16)\n"
     ]
    }
   ],
   "source": [
    "# do the transformation to get our data, ready and prepared for training\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "print(housing_prepared.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Select and Train a Model\n",
    "\n",
    "We are finally ready to try and train some ML models.  A lot of your time as a working \n",
    "data scientist would be spent performing the steps to get to this point.  In this class,\n",
    "our focus is in learning about how various ML models work, and the internals of how they \n",
    "are implemented.  So after this example, we will usually start with a data set \n",
    "that has been pretty much prepared and transformed, and get right to trying to \n",
    "train ML models to model the data.\n",
    "\n",
    "We will learn about all of the following ML algorithms, so don't try to understand the \n",
    "details yet.  We will start with the simplest model, a linear regression model.\n",
    "\n",
    "All such models from `sklearn` are full estimator - predictors.  The purpose of a supervised \n",
    "learning ML algorithm is to generate a hypothesis function $h()$ from the training data, and\n",
    "use the learned hypothesis to predict the labels or values of unseen data.\n",
    "\n",
    "So to train `sklearn` models, we first create an instance of the model.  We then `fit()` \n",
    "the model to the training data, giving the prepared training data, and since this is \n",
    "supervised learning, we also give the expected correct labels for the training samples.\n",
    "Once the model has been fitted, we can then evaluate its performance by testing out the\n",
    "quality of its predictions on our test data.\n",
    "\n",
    "First, create a linear regression instance, and \"fit\" it to the training data.  This \n",
    "causes the model to learn (to the best of the algorithms ability) its hypothesis of the\n",
    "transformation of inputs to outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-55649.63398453 -56711.59742892  13734.72084192  -1943.05586355\n",
      "   7343.22979731 -45709.28253579  45453.26277662  74714.15226133\n",
      "   6604.58396628   1043.05452981   9248.31607777 -18015.98870784\n",
      " -55214.71083473 110357.8461062  -22484.65997391 -14642.48658971]\n",
      "236926.06189652678\n"
     ]
    }
   ],
   "source": [
    "# try a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# these are actually the learned or fitted parameters, we will talk about\n",
    "# this later\n",
    "print(lin_reg.coef_)\n",
    "print(lin_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained model.  We can now test out and evaluate how well\n",
    "it makes predictions.\n",
    "\n",
    "We can look at the quality of some of the predictions it makes for the data \n",
    "we used in its training.  For example, lets get the first 5 samples of the \n",
    "training data, and compare the predictions the model makes with the correct \n",
    "labels.  \n",
    "\n",
    "Notice we are using the estimator part of the object here, the `preedict()`\n",
    "function to return predictions for a set of input samples.\n",
    "\n",
    "Also notice that it is critical that new data be put through the \n",
    "data transformation pipeline before we run the prediction.  It will of course \n",
    "not work if you feed in untransformed input to the model, it has learned the \n",
    "mapping from the transformed inputs to the output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [ 85657.90192014 305492.60737488 152056.46122456 186095.70946094\n",
      " 244550.67966089]\n",
      "Labels:  [72100.0, 279600.0, 82700.0, 112500.0, 238300.0]\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "print(\"Predictions: \", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels: \", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the model do in its predictions here?  You may or may not have had \n",
    "some expections on what you would see here.  It works, but the predictions \n",
    "don't exactly look real accurate.  The first prediction is off by over \\\\$76k,\n",
    "which is about 76/210 or about 35%.\n",
    "\n",
    "To evaluate the model we need a more formal measure of its performance.\n",
    "We introduced the RMSE before.  Lets see what the root mean squared error is\n",
    "over the whole training set.  Notice that the metric from `sklearn` only gives\n",
    "the mean squared error, so we have to take the final square root ourself to \n",
    "get the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68627.87390018745\n"
     ]
    }
   ],
   "source": [
    "# measure the regression models RMSE on the training set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "\n",
    "print(lin_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean?  Since we took the square root of the average of the \n",
    "squared differences, this means that on average our models predictions are \n",
    "off by over \\\\$68k.  \n",
    "\n",
    "This is an example of a model underfitting the training data.  It is doing \n",
    "something, but this may not be a good enough predictor to meet our \n",
    "business or data science needs.\n",
    "\n",
    "To fix an underfitting model we can get better or more features, reduce the \n",
    "constraints on the model, or use a better model.  We can't get more data \n",
    "here too easily.  We could try and find some better custom features like the \n",
    "`bedrooms_per_room` feature we found and added. This model is not regularized, \n",
    "so we don't have any constraints we can loosen (we will talk about regularization\n",
    "later in the class).\n",
    "\n",
    "So for the moment we are reduced to trying out a more powerful ML modeling \n",
    "algorithm.  We will study Decision Trees in this class.  A `DecisionTreeRegressor` is potentially a much more powerful model than\n",
    "a linear regression.\n",
    "\n",
    "The pattern will remain the same, create an instance of the `DecisionTreeRegressor`, fit the model, then use `predict()` on data \n",
    "to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try a more complex DecisionTree regressor model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "\n",
    "print(tree_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! absolutely no error at all.  This means that the decision tree \n",
    "gave an absolute perfect prediction for all of the 16k+ samples we used \n",
    "in training.  So we are done right?\n",
    "\n",
    "Well not really, the textbook is leading you down a socratic path here a bit \n",
    "to try and get you to understand the danger of overfitting.  But in general,\n",
    "a model's performance on the data it was trained with means very little.\n",
    "It tells us nothing, really, about how well the model will really do with \n",
    "data it has never seen before.\n",
    "\n",
    "So now we should pull out the test data set for the evaluation of how well \n",
    "the model does on unseen data, right?  Not quite. The pristine test set \n",
    "should only be used for the absolute final evaluation.  So if we are going to\n",
    "train and compare multiple models, we actually need to split the training \n",
    "set into 2 pieces, and evaluate how well the different models do on those\n",
    "parts.  This is known as splitting the training data into training and \n",
    "validation sets.  Then when we compare models to one another, we compare \n",
    "how well they do on the held back validation data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Evaluation Using Cross-Validation\n",
    "\n",
    "We could just use a `train_test_split()` function on the current training\n",
    "data to split into training and validation data.\n",
    "\n",
    "A better alternative is to use what is known as **K-fold cross-validation**.\n",
    "The textbook uses 10-fold (which is pretty standard) as an example. For a\n",
    "10-fold cross-validation, we break the training data up into 10 roughly \n",
    "equal pieces.  Then we fit and evaluate the model 10 different times (and compute the average and variance of the performance across the 10 training \n",
    "runs).  Each run we take out 1 of the 10 folds, train with the data from the \n",
    "other 9 folds, and evaluate performance on the held out fold.\n",
    "\n",
    "This gives a much better estimate of how well a model will generalize \n",
    "on unseen data.  And also, since we train the model multiple times, we will \n",
    "be able to get information about how much variance we can expect to see \n",
    "in model performance across different training runs of a model.\n",
    "\n",
    "`sklearn` makes using K-fold cross-validation relatively straight forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the decision tree regressor using K-fold cross-validation with 10 folds\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring='neg_mean_squared_error', cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few technical details here.  For some reason that I could not tell you \n",
    "why, this function expects a utility function (where greater is better,\n",
    "rather than a cost function where smaller is better).  Thus we \n",
    "compute the negative of the mean squared error, so that larger values are \n",
    "actually closer to 0 and thus better.  To get the actual RMSE result, we just\n",
    "take the negative of the scores and compute the square root as before.\n",
    "\n",
    "But notice all of the details of creating the folds, training the \n",
    "model, and calculating the scores, is handled by this one function. \n",
    "The result is simply a list of scores that we can transform into the RMSE\n",
    "score for each of the 10 folds that we can most easily understand.\n",
    "\n",
    "So lets see how the decision tree really performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [72409.72073515 70089.00713541 68174.13749498 72236.63111625\n",
      " 69764.06408626 76966.65324267 72150.10606627 72206.03561393\n",
      " 67629.77604067 69863.18778633]\n",
      "Mean:  71148.93193179171\n",
      "Standard Devaition:  2542.9643071500714\n"
     ]
    }
   ],
   "source": [
    "def display_scores(scores):\n",
    "    print('Scores: ', scores)\n",
    "    print('Mean: ', scores.mean())\n",
    "    print('Standard Devaition: ', scores.std())\n",
    "    \n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yea, not so impressive now.  A bit worse in fact than the linear regressor, \n",
    "though the linear regressor would probably fall a bit if we also evaluated \n",
    "it with cross-validation.\n",
    "\n",
    "This says that the average error is a bit over \\\\$70k for each prediction.\n",
    "It also say on average prediction errors vary by $\\pm2600$ dollars.\n",
    "The amount of variance you see from cross-validtion can be very \n",
    "valuable in letting you know if one model just got unusually lucky or \n",
    "unlucky in its training.\n",
    "\n",
    "We can do the cross validation for linear regression also, just so we can \n",
    "compare apples with apples for all of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [71762.76364394 64114.99166359 67771.17124356 68635.19072082\n",
      " 66846.14089488 72528.03725385 73997.08050233 68802.33629334\n",
      " 66443.28836884 70139.79923956]\n",
      "Mean:  69104.07998247063\n",
      "Standard Devaition:  2880.3282098180634\n"
     ]
    }
   ],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                             scoring='neg_mean_squared_error', cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final note here.  Though the performance looks about the same, it is good\n",
    "to remember and understand that the models are performing poorly for \n",
    "completely different reasons.\n",
    "\n",
    "We know that the decision tree model is badly overfitting, because it gets \n",
    "perfect performance (no error) if evaluated on the data it was trained with.\n",
    "So you should understand that the scores you get from cross-validation \n",
    "above are the scores on the held back, unseen validation data for each \n",
    "of the k-fold trained models.  If you look at the training error, you \n",
    "would see that the decision tree always gets close to 0 on the data it trained \n",
    "with, while the linear regressor is getting a similar high error both for \n",
    "the data it trains with and the unseen validation data.\n",
    "\n",
    "We next try one last model.  We will also talk about ensemble ML models, and\n",
    "random forests.  A random forest is actually a collection of many\n",
    "smaller decision trees.  Random forests are usually in practice better performers\n",
    "than single decision trees.  We can also more easily tune \n",
    "decision trees to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try a random forest regressor to try and fix overfitting of single decision trees\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [51435.94956346 49047.29320429 46400.44389757 51742.95796793\n",
      " 47706.05790789 52219.87494706 53064.3104359  49722.98658931\n",
      " 48566.76890949 53906.14278283]\n",
      "Mean:  50381.27862057166\n",
      "Standard Devaition:  2334.082837289702\n"
     ]
    }
   ],
   "source": [
    "# warning, this will take significnatly more time than other stuff up to this\n",
    "# point\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring='neg_mean_squared_error', cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final model does show some improvement.  It moves the\n",
    "average error from around \\\\$70k to down to \\\\$50k, which is a significant\n",
    "improvement in performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Fine-Tune Your Model\n",
    "\n",
    "Once you have an idea of a model or models that might be promising, \n",
    "you want to see if  you can tune the models to optimize your \n",
    "performance.\n",
    "\n",
    "Randomly changing meta-parameters or tweaking models will only get you \n",
    "so far.  At some point an experienced data scientist will want to take \n",
    "a more structured approach to fine tuning models to try and improve their \n",
    "performance.\n",
    "\n",
    "We can think of fine tuning as a type of search optimization, where we have\n",
    "different types of models, and meta-parameters we can set for the various \n",
    "models.  We want to explore the space of models and their tuning parameters\n",
    "effectively to find the places where optimum performance occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "Grid search of models and parameters is an attempt to exhaustively \n",
    "search ranges of combinations of parameters in order to evaluate \n",
    "model performance.  \n",
    "\n",
    "Since the `RandomForestRegressor` looks promising, we could ask, might \n",
    "tweaking some of its main meta-parameters result in even better performance?\n",
    "Our textbook picks several meta-parameters for the example.  Again don't \n",
    "worry about what these are right now, we will cover them when we talk\n",
    "about random forests and ensembles a bit.\n",
    "\n",
    "What you should understand from this next example is that we define a list of \n",
    "3 and 4 parameters to try for the first, and a list of 1, 2 and 3 parameters \n",
    "to try the second time.  All combinations of each line in the defined grid \n",
    "will be tried.  Thus we will try the $3 \\times 4 = 12$ combinations of the \n",
    "first grid line, followed by the $1 \\times 2 \\times 3 = 6$ combinations \n",
    "of the second, for a total of 18 different meta-parameter settings.\n",
    "\n",
    "So here we are not only performing a 5-fold cross validation, but we perfrom\n",
    "this over all 18 different defined meta-paramter combinations of the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "             param_grid=[{&#x27;max_features&#x27;: [2, 4, 6, 8],\n",
       "                          &#x27;n_estimators&#x27;: [3, 10, 30]},\n",
       "                         {&#x27;bootstrap&#x27;: [False], &#x27;max_features&#x27;: [2, 3, 4],\n",
       "                          &#x27;n_estimators&#x27;: [3, 10]}],\n",
       "             return_train_score=True, scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "             param_grid=[{&#x27;max_features&#x27;: [2, 4, 6, 8],\n",
       "                          &#x27;n_estimators&#x27;: [3, 10, 30]},\n",
       "                         {&#x27;bootstrap&#x27;: [False], &#x27;max_features&#x27;: [2, 3, 4],\n",
       "                          &#x27;n_estimators&#x27;: [3, 10]}],\n",
       "             return_train_score=True, scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "             param_grid=[{'max_features': [2, 4, 6, 8],\n",
       "                          'n_estimators': [3, 10, 30]},\n",
       "                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n",
       "                          'n_estimators': [3, 10]}],\n",
       "             return_train_score=True, scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GridSearchCV` estimator again allows for inspection.  So after completing\n",
    "the grid search, we can find the scores, and access the best parameters \n",
    "found in the search, and the actual trained best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 6, 'n_estimators': 30}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_features=6, n_estimators=30)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_features=6, n_estimators=30)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_features=6, n_estimators=30)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64288.60991037362 {'max_features': 2, 'n_estimators': 3}\n",
      "55383.01317787567 {'max_features': 2, 'n_estimators': 10}\n",
      "52733.62639273178 {'max_features': 2, 'n_estimators': 30}\n",
      "60022.33538458488 {'max_features': 4, 'n_estimators': 3}\n",
      "52640.81637573584 {'max_features': 4, 'n_estimators': 10}\n",
      "50507.16052391894 {'max_features': 4, 'n_estimators': 30}\n",
      "59372.62245602049 {'max_features': 6, 'n_estimators': 3}\n",
      "52011.90463468856 {'max_features': 6, 'n_estimators': 10}\n",
      "49850.8620770793 {'max_features': 6, 'n_estimators': 30}\n",
      "58562.94570757889 {'max_features': 8, 'n_estimators': 3}\n",
      "52106.56218451456 {'max_features': 8, 'n_estimators': 10}\n",
      "49961.01522948954 {'max_features': 8, 'n_estimators': 30}\n",
      "61977.98146666888 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "54096.181303341196 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "59767.82750262366 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "52180.26932389047 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "58352.1792766127 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "51473.412985458315 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performer with `max_features` of 6 and `n_estimators` of 30\n",
    "made a slight improvement, breaking \\\\$50k for the average error.  \n",
    "But since `n_estimators` as actually the maximum of the specified grid,\n",
    "it might be worthwhile trying larger values of this parameter in another\n",
    "grid search at least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Best Models and their Errors\n",
    "\n",
    "Another step you should perform is to inspect your best models (so far)\n",
    "to peak into their innards and try and understand what is making them\n",
    "work, or not work so well, as the case may be.\n",
    "\n",
    "A common thing to do along these lines is to determine the relative \n",
    "importance of each attribute in making predictions.  Because of how\n",
    "a random forest work, we can get estimates of the importance of each\n",
    "feature of our dataset in the hypothesis function that was formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.10507589e-02, 6.21804717e-02, 4.29286069e-02, 1.72559417e-02,\n",
       "       1.82344784e-02, 1.83276169e-02, 1.61963529e-02, 3.11392396e-01,\n",
       "       5.27303380e-02, 1.07946321e-01, 9.94671176e-02, 1.17547711e-02,\n",
       "       1.63867545e-01, 1.40607032e-05, 2.71783931e-03, 3.93538363e-03])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets associate each score with the attribute name, and sort from the \n",
    "most important to the least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.31139239561472015, 'median_income'),\n",
       " (0.16386754538284967, 'INLAND'),\n",
       " (0.10794632121548285, 'pop_per_hhold'),\n",
       " (0.09946711763389618, 'bedrooms_per_room'),\n",
       " (0.0710507589092017, 'longitude'),\n",
       " (0.06218047170418309, 'latitude'),\n",
       " (0.05273033802682449, 'rooms_per_hhold'),\n",
       " (0.042928606945780594, 'housing_median_age'),\n",
       " (0.01832761688330214, 'population'),\n",
       " (0.018234478436621883, 'total_bedrooms'),\n",
       " (0.017255941657508083, 'total_rooms'),\n",
       " (0.016196352860370213, 'households'),\n",
       " (0.011754771082502837, '<1H OCEAN'),\n",
       " (0.003935383633171487, 'NEAR OCEAN'),\n",
       " (0.0027178393104231495, 'NEAR BAY'),\n",
       " (1.4060703161509433e-05, 'ISLAND')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_room']\n",
    "cat_encoder = full_pipeline.named_transformers_['cat']\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not surprisingly, since we knew `median_income` was most highly correlated \n",
    "with the housing price, it ends up as the most important feature.  But \n",
    "one of the categorical attributes is contributing a lot.  Also the \n",
    "two custom derived features, `rooms_per_houshold` and `bedrooms_per_room` \n",
    "are contributing some to the model as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Your System on the Test Set\n",
    "\n",
    "After tweaking your models for a while, you eventually \n",
    "have a system that is performing sufficiently well.  Now just before \n",
    "moving it to production (or publishing your results) it is time \n",
    "to make the final evaluation on the completely pristine \n",
    "test data set that you have not looked at. \n",
    "\n",
    "There is nothing special here, we will use our pipeline to transform \n",
    "the data, and use the best model we selected for reporting/production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48519.63022047017\n"
     ]
    }
   ],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop('median_house_value', axis=1)\n",
    "y_test = strat_test_set['median_house_value'].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "print(final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also helps to know some statistics.  Given the final \n",
    "performance predictions, we can calculate confidence intervals\n",
    "on our systems performance.  From the variation of the errors \n",
    "we can compute a t test confidence interval, like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46524.94326943, 50435.49035391])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute a 95% confidence interval for the genrealization error\n",
    "from scipy import stats\n",
    "\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test)**2\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "                         loc=squared_errors.mean(),\n",
    "                         scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confidence interval is a very useful final result.  Given the assumption\n",
    "that the errors are roughly normally distributed, we are 95% confident that \n",
    "the true average error of this final model is somewhere within this range.\n",
    "This means that if we use it on live data, we are pretty sure the average \n",
    "predictions are no worse than the upper bounds shown in this \n",
    "confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Module   Versions\n",
      "--------------------   ------------------------------------------------------------\n",
      "         matplotlib:   ['3.7.2']\n",
      "              numpy:   ['1.25.2']\n",
      "             pandas:   ['2.0.3']\n",
      "            seaborn:   ['0.12.2']\n"
     ]
    }
   ],
   "source": [
    "# display version information of library versions used in this notebook\n",
    "from ml_python_class.custom_funcs import version_information\n",
    "version_information()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
