{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports usually needed\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries more specific to this lecture notebook\n",
    "# it is good practice that all imports for most working notebooks usually go at the \n",
    "# beginning.  When learning about libraries like scikit-learn, I will put the imports \n",
    "# at the top, but also demonstrate and re-import the library where discussed below in the \n",
    "# notebook.\n",
    "import os.path\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "from ml_python_class.config import DATA_DIR\n",
    "from ml_python_class.custom_funcs import fetch_compressed_data\n",
    "\n",
    "from zlib import crc32\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook wide global definitions\n",
    "DOWNLOAD_ROOT = 'https://raw.githubusercontent.com/ageron/handson-ml2/master/'\n",
    "HOUSING_URL = DOWNLOAD_ROOT + 'datasets/housing/housing.tgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook wide settings to make plots more readable and visually better to understand\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "mpl.rc('figure', titlesize=18)\n",
    "mpl.rcParams['figure.figsize'] = (10.0, 8.0) # default figure size if not specified in plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. (Chapter 2) End-to-End Machine Learning Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the Data**\n",
    "\n",
    "You will notice that we have abstracted common data acquisition and loading functions to \n",
    "our project module.  The module is imported and the functions loaded by default \n",
    "that are needed for this notebook.  The `ml_python_class` module contains common \n",
    "functions so we don't repeat ourself in notebooks.  Also it will automagically do things like \n",
    "determine the correction location of the `DATA_DIR` where data files should be located, even if \n",
    "we move around or rearrange notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data file 'housing.csv' exists, nothing done.\n"
     ]
    }
   ],
   "source": [
    "# fetch and uncompress housing data if it needs to be downloaded\n",
    "housing_file = os.path.join(DATA_DIR, 'housing.csv')\n",
    "fetch_compressed_data(HOUSING_URL, housing_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv file into a pandas DataFrame\n",
    "housing = pd.read_csv(housing_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 10)\n",
      "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
      "       'total_bedrooms', 'population', 'households', 'median_income',\n",
      "       'median_house_value', 'ocean_proximity'],\n",
      "      dtype='object')\n",
      "Number of samples in the raw dataset:  20640\n",
      "Number of feature attributes in the raw dataset:  10\n"
     ]
    }
   ],
   "source": [
    "# sanity check, does it look like we got what we were expecting?\n",
    "m, n = housing.shape\n",
    "print(housing.shape)\n",
    "print(housing.columns)\n",
    "print(\"Number of samples in the raw dataset: \", m)\n",
    "print(\"Number of feature attributes in the raw dataset: \", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Test Set\n",
    "\n",
    "We skipped over this section in previous notebook as I think it makes more logical sense \n",
    "to introduce it as we start thinking about preparing our data for a machine learning\n",
    "algorithm.  \n",
    "\n",
    "There are some subtlities to splitting our data correctly for training that are touched on in \n",
    "this section.  As a practical concern, I will usually just use `sklearn` functions to \n",
    "randomly split into test and train data sets.  But in a real data analytics project you do \n",
    "need to be more careful and make sure your testing data never ever leaks into anything you use \n",
    "to train or validate your models, even with repeated loading and splitting.\n",
    "\n",
    "That said, we can first roll our own function to split a `numpy` array or a `pandas` \n",
    "`DataFrame` into test and training sets.  The following illustrates the basic ideas.\n",
    "\n",
    "Why do we need to do this?  It is important you understand the reasoning.  It is easy to \n",
    "build a model that makes perfect predictions on all the data it is trained with.  I could just \n",
    "create a lookup table, so any data item I have seen I just return the correct label when you \n",
    "ask me to make a prediction.  This will always result in perfect accuracy, or in perfect \n",
    "prediction of a real valued target label like a housing price.\n",
    "\n",
    "The real difficulty is building a model that **generalizes** well to unseen data.  It is \n",
    "easy to fool ourselves and thing we are building a hypothesis function that generalizes well.\n",
    "To not fool ourself, we have to train with only a part of the data, and then evaluate the \n",
    "performance of our learned hypothesis function on the data that was not seen during training.\n",
    "This will give us a better feel for how well our model will work for other data not yet \n",
    "gathered or in our data set.\n",
    "\n",
    "But you have to be even more careful still.  If you want to try out severl different ML \n",
    "techniques, and if you always split in different ways into different test and train data sets,\n",
    "then some of your models will be training with data that others use for testing, and vice versa.\n",
    "When doing parameter tuning, this kind of **leaking** of things between training and testing can \n",
    "cause you to find metaparameters or use ML models that actually still overfit the data\n",
    "\n",
    "To avoid this type of leaking of information, it is best to absolutely keep some data that is never,\n",
    "ever, ever used for training in any form.  Its only purpose is to evaluate final models \n",
    "for their generalization ability.  This means that while exploring different meta-parameters \n",
    "or different ML models, we often then split the training data once again into training and \n",
    "validation data.  This allows us to evaluate different models and metaparameters against \n",
    "one another, and then get a final judgement on the completely held back test data for final \n",
    "evaluations.\n",
    "\n",
    "So there are some subtle issues involved with train / test / validation data splitting. \n",
    "The simplest approach is to simply randomly select some percentage of the samples to be \n",
    "the training data, and some percentage for final test.  If your data set never changes, you \n",
    "can, as discussed in the text, do something like set the random seed before calling your random \n",
    "number generator.  This guarantees you will get the same random test/train split, as long as \n",
    "you always have the same data ordered in the same way.\n",
    "\n",
    "Pythong makes it easy to roll our own train/test split functions.  We could have also passed in\n",
    "a random seed to the next function, which as you will see is how some of these utility functions \n",
    "work in the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 0, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.permutation(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    \"\"\"Split a numpy/pandas like array of data into a training and test set.\n",
    "    The test_ratio determins the ratio or percentage of the samples selected for \n",
    "    the testing set.  1-test_ratio samples will thus end up in the training set.\n",
    "    The data is split randomly between the train and test sets that are returned.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data - A numpy array or pandas data frame (narray like object) of data,\n",
    "       assumed rows are samples and columns are  the features of the data set.\n",
    "    test_ratio - A float value in range [0.0, 1.0], this will be the ratio of\n",
    "       the samples slected to be in the resulting  test set.  1-test_ratio\n",
    "       samples will be randomly selected to be in the training est.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train, test - A tuple of the data is returned splint into a training \n",
    "       set and a testing set.\n",
    "    \"\"\"\n",
    "    # randomly shuffle an array of indices from 0 to m-1\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    \n",
    "    # so that we can use regular array slicing to select \n",
    "    # test and train sets of the desired size\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    \n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    \n",
    "    # return the split data using fancy indexing to access the randomly\n",
    "    # selected indices of the test and training set\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 10)\n",
      "(4128, 10)\n",
      "4128.0\n"
     ]
    }
   ],
   "source": [
    "# as discussed in text, if we always come to this point having loaded the housing data from \n",
    "# the same file, and the data has not changed, we can always get the same train/test split by\n",
    "# setting a random seed before or in the function, before calling the random number generator\n",
    "np.random.seed(42) # the answer to life, the universe and everything\n",
    "\n",
    "# do the actual split using our homegrown function for an example\n",
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)\n",
    "\n",
    "# what is the expected number of samples in the test set given a 0.2 test_ratio?\n",
    "print(0.2 * m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in this case we got exactly the number of samples, 4128, as we would expect if 20%\n",
    "are randomly put into the test set and the other 80% into the training set.  The random split\n",
    "method above guarantees an exact split (unless this is impossible because of an odd number of \n",
    "values).  Basically the method above randomly shuffles the indexes 0 to 20639, then uses the\n",
    "first 20% of these for the test set, and the last 80% for the train set.\n",
    "\n",
    "If we are worried about information leakage in our training, we might want to try and \n",
    "guarantee that we always get the same items in our test and training sets, even if we \n",
    "shuffle the data, add new data, etc.  One common but relatively simple way to accomplish this \n",
    "is presented in our text.  If each sample has a unique identifier, and if we can hash that \n",
    "identifier, we can use the unique id hash to perform a test / train split, but the \n",
    "determination will be stable as long as we always use the same test / train split ratio and the \n",
    "same hashing function of course.  This is because the unique id does not change, and the hash \n",
    "will always hash this id to the same hash value.  \n",
    "\n",
    "The example implementation below is not guaranteed to get exactly 20% of the values in the \n",
    "test set (if that is our test_ratio), because that will depend on the way things end up \n",
    "being randomly hashed.  But the split will usually be close enough that it doesn't matter.\n",
    "\n",
    "You might want to make sure you understand the following 2 functions as a check of how your \n",
    "python skills are progressing.  We use an example of the `apply()` function from\n",
    "`pandas` `Series` items below.  Basically you can think of apply as implementing a loop \n",
    "that applies the function to each item of the series, and returns this as a new series\n",
    "(e.g. as a new attribute column).  So the `apply()` function below does something like:\n",
    "\n",
    "```python\n",
    "in_test_set = # New series that is empty but of same size as number of samples\n",
    "\n",
    "for i,id_ in enumerate(data[id_column]):\n",
    "    in_test_set[i] = test_set_check(id_, test_ratio)\n",
    "```\n",
    "\n",
    "Conceptually the result is an array or a column of booleanr results that is `True` for each \n",
    "item that ends up hasing to the test set, and is `False` for each item that ends up hasing \n",
    "to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_check(identifier, test_ratio):\n",
    "    \"\"\"Check a unique identifier and return False if the id hash falls\n",
    "    within the bottom test_ratio proprotion of the possible hashes.  Return True \n",
    "    for the id if its has is above this ratio.  This can be used to \n",
    "    reliably split a set of data into a train and test set, and ensure that you always \n",
    "    have the same items in the train set and in the test set given the same \n",
    "    test_ratio.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    identifier - A unique identifier for a data item in a dataset\n",
    "    test_ratio - The ratio of desired test to training items in the resulting split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool - Returns True if the hash of the unique identifier means the item should \n",
    "       be in the test set, and False is returned if the item should end up in the \n",
    "       training set.\n",
    "    \"\"\"\n",
    "    # crc32 hashes the item, we take bitwise & to get least significant bits \n",
    "    # of the resulting hash.  Then the comparison gives true result for those \n",
    "    # id hashes that should be in the test set.\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    \"\"\"Split numpy/pandas like arrays into training and test sets.\n",
    "    Data is split randomly by this funciton, but reliably.  e.g. Given \n",
    "    the same test_ratio, the same set of items will always end up in the \n",
    "    training set and the test set by this split.  This is accomplished \n",
    "    by generating a hash on the unique id of each item, and using the hash \n",
    "    to perform the train/test split.  The hash is random but should always \n",
    "    hash to the same location for the same unique id of a data item.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data - A numpy array or pandas data frame (narray like object) of data,\n",
    "       assumed rows are samples and columns are  the features of the data set.\n",
    "    test_ratio - A float value in range [0.0, 1.0], this will be the ratio of\n",
    "       the samples slected to be in the resulting  test set.  1-test_ratio\n",
    "       samples will be randomly selected to be in the training est.\n",
    "    id_column - The index of the feature column in the data that should be used \n",
    "       as the samples unique id for the split. This can be a string if the \n",
    "       data item is a pandas array that has named columns, or an integer \n",
    "       index for a numpy like array of data.\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    train, test - A tuple of the data is returned splint into a training \n",
    "       set and a testing set.\n",
    "    \"\"\"\n",
    "    # extract the column of unique ids from the data\n",
    "    ids = data[id_column]\n",
    "    \n",
    "    # use the test_set_check function to hash the unique ids and make an evaluation \n",
    "    # for each item whether it is in the test set or not. \n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    \n",
    "    # return the resulting split by unique id determination\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is actually a problem with using a unique id here, as discussed in the text.  We really\n",
    "don't have one.  We could use latitude and longitude, but the coarseness of the determination \n",
    "of district location can mean some of them end up with the same unique id.  This is probably \n",
    "not really an issue here for this data.  The example method used here is to use the `pandas`\n",
    "`reset_index()` to give a unique index number to each sample.  But unless we resave our data \n",
    "with this new column added to the raw data, these indexes can change if we for example delete \n",
    "items in the middle of the data, or move items around.  If the indexes change, where they \n",
    "hash to might change, and thus might end up leaking them to train sets when have been \n",
    "used as test items in other locations.\n",
    "\n",
    "We actually get exactly 4128 items in the test set again, though this is no longer \n",
    "guaranteed as discussed above.  Notice also that we now have 11 column attributes after adding \n",
    "the unique index column to the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 11)\n",
      "(4128, 11)\n"
     ]
    }
   ],
   "source": [
    "# the housing data doens't really have a unique id, so we generate an index column\n",
    "# All of the above work will go in vain if the order of the items in the raw data file \n",
    "# can change between loads, so if adding new items make sure they are added on the end,\n",
    "# or perhaps it is better to explicitly add the unique id index number to the raw data\n",
    "housing_with_id = housing.reset_index()\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 'index')\n",
    "\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, latitude and longitude should be pretty unique if combined, and probably suitable\n",
    "for this application.\n",
    "\n",
    "Notice here we don't get exactly 4128 or 20% of the items in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16322, 12)\n",
      "(4318, 12)\n"
     ]
    }
   ],
   "source": [
    "# another example, could try and create a unique id using latitude and longitude of the districts\n",
    "housing_with_id['id'] = housing['longitude'] * 1000 + housing['latitude']\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 'id')\n",
    "\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Scikit-learn' has lots of utility functions for doing things like this.  The basic train / \n",
    "test split will do exactly what we did in our handmade function.  Notice you can specify the \n",
    "`random_state` to this function, to ensure you get the same split given the same input data \n",
    "and `test_size` ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 10)\n",
      "(4128, 10)\n"
     ]
    }
   ],
   "source": [
    "# sometimes want to roll by hand, for example if it is really crucial that you always use the same items for training,\n",
    "# but can use standard library functions\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more subtle issues come into play here when thinking about the test / train data split.\n",
    "Again if you know some basic statistics and have taken a class you should recognize some of \n",
    "these issues.\n",
    "\n",
    "Basically when we randomly split the data we are taking a random sample.  For some types of \n",
    "machine learning problems it might be crucial that the sample used for training is representative\n",
    "of the data set.  For example, as discussed, we could get unlucky and have no or very few \n",
    "examples of houses in an important category, like for example few examples of houses in our \n",
    "training data with high median incomes.  Usually random sampling will be fine, especially for \n",
    "truly large data sets.  Ours is small enough we might get unlucky.\n",
    "\n",
    "The solution is to use a more targeted sampling method, like **stratified sampling**.\n",
    "So if we believe `median_income` is crucial to our model performance, we might want to \n",
    "guarantee that our train / test sample split results in a training sample with \n",
    "instances that have the same distribution as the overall data set.  For example, we could \n",
    "discretize the `median_income` into categories.  The following uses `pandas` `cut()` \n",
    "method to discretize the continuous `median_income` attribute into 5 categories, and \n",
    "we can see the resulting distribution of incomes for our districts using a \n",
    "histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an income category with 5 levels\n",
    "housing['income_cat'] = pd.cut(housing['median_income'],\n",
    "                               bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the histogram of the housing income categories\n",
    "housing['income_cat'].hist(figsize=(10,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we want our resulting training set to have approximately these same ratios of \n",
    "`median_income` in the training samples, we could do a stratified sample selection \n",
    "using this median income category.  We could roll our own, though it gets a bit \n",
    "tricky to guarantee we get a repeatable result that gives both a training and test set with \n",
    "approxmatly the same ratios for the stratified sampled categories.  \n",
    "\n",
    "So we instead use the `sklearn` `StratifiedShuffleSplit` function, which can do all of this, and\n",
    "can even sample on more than 1 strata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 11)\n",
      "(4128, 11)\n"
     ]
    }
   ],
   "source": [
    "# a more complex example of sampling.  Use the income category to do the train/test split using \n",
    "# stratified shuffling.  This ensure that the same proportions of the categories \n",
    "# will be sampled to the train and test data sets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing['income_cat']):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "print(strat_train_set.shape)\n",
    "print(strat_test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>income_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12655</th>\n",
       "      <td>-121.46</td>\n",
       "      <td>38.52</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>797.0</td>\n",
       "      <td>2237.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>2.1736</td>\n",
       "      <td>72100.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15502</th>\n",
       "      <td>-117.23</td>\n",
       "      <td>33.09</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>6.3373</td>\n",
       "      <td>279600.0</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>-119.04</td>\n",
       "      <td>35.37</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1618.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>667.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2.8750</td>\n",
       "      <td>82700.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14053</th>\n",
       "      <td>-117.13</td>\n",
       "      <td>32.75</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1877.0</td>\n",
       "      <td>519.0</td>\n",
       "      <td>898.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>2.2264</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20496</th>\n",
       "      <td>-118.70</td>\n",
       "      <td>34.28</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3536.0</td>\n",
       "      <td>646.0</td>\n",
       "      <td>1837.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>4.4964</td>\n",
       "      <td>238300.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "12655    -121.46     38.52                29.0       3873.0           797.0   \n",
       "15502    -117.23     33.09                 7.0       5320.0           855.0   \n",
       "2908     -119.04     35.37                44.0       1618.0           310.0   \n",
       "14053    -117.13     32.75                24.0       1877.0           519.0   \n",
       "20496    -118.70     34.28                27.0       3536.0           646.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "12655      2237.0       706.0         2.1736             72100.0   \n",
       "15502      2015.0       768.0         6.3373            279600.0   \n",
       "2908        667.0       300.0         2.8750             82700.0   \n",
       "14053       898.0       483.0         2.2264            112500.0   \n",
       "20496      1837.0       580.0         4.4964            238300.0   \n",
       "\n",
       "      ocean_proximity income_cat  \n",
       "12655          INLAND          2  \n",
       "15502      NEAR OCEAN          5  \n",
       "2908           INLAND          2  \n",
       "14053      NEAR OCEAN          2  \n",
       "20496       <1H OCEAN          3  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out textbook makes a table to prove that the test and training samples have similar distributions\n",
    "of `median_income` samples.  We can get a feel for if this is true or not by using `value_counts()`\n",
    "to get the ratios of our income category in the original data before split, and in the resulting \n",
    "stratified train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income_cat\n",
       "3    0.350581\n",
       "2    0.318847\n",
       "4    0.176308\n",
       "5    0.114438\n",
       "1    0.039826\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the proportions of the samples in the original and in the test and train set should be similar\n",
    "housing['income_cat'].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income_cat\n",
       "3    0.350533\n",
       "2    0.318798\n",
       "4    0.176357\n",
       "5    0.114341\n",
       "1    0.039971\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set['income_cat'].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income_cat\n",
       "3    0.350594\n",
       "2    0.318859\n",
       "4    0.176296\n",
       "5    0.114462\n",
       "1    0.039789\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set['income_cat'].value_counts() / len(strat_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not complete proof, you should see that within the first 3 most significant digits, all\n",
    "of these samples have the same proprotion of samples in each of the 5 income categories, which \n",
    "is reassuring.\n",
    "\n",
    "We may be using the stratified train and test samples below, so we will drop the created \n",
    "category now in case we want to use these samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 10)\n",
      "(4128, 10)\n"
     ]
    }
   ],
   "source": [
    "# remove the income_cat attribute so data is back to original state\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop('income_cat', axis=1, inplace=True)\n",
    "    \n",
    "print(strat_train_set.shape)\n",
    "print(strat_test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "Splitting data into training and test sets is a first step to preparing it so we can use it \n",
    "to train ML models with.  In this section we look at some of the many other cleaning and scaling \n",
    "transformation you will likely need to perform (and many more) before real data is ready \n",
    "for ML training and fine-tuning.\n",
    "\n",
    "As done in the textbook, we first revert to a clean training set.  We will use the stratified \n",
    "train / test split sample we performed above.  We also begin to prepare for training by \n",
    "dropping the `median_house_value` attribute from our training data, and putting this into \n",
    "a separate series/vector.  Thus after this point, we only have the attributes we want to clean \n",
    "for training in the `housing` `DataFrame`, and we have the target labels in the `housing_labels`\n",
    "vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 9)\n",
      "(16512,)\n"
     ]
    }
   ],
   "source": [
    "housing = strat_train_set.drop('median_house_value', axis=1)\n",
    "housing_labels = strat_train_set['median_house_value'].copy()\n",
    "\n",
    "print(housing.shape)\n",
    "print(housing_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "One of the most troublesome and common things you will have to handle with real data is \n",
    "figuring out what to do with missing data.  There is a single attribute `total_bedrooms` that \n",
    "is missing a value from a few of the samples.  \n",
    "\n",
    "The textbook discusses 3 options.  These are by no means the only way to handle missing data.\n",
    "\n",
    "1. Get rid of the corresponding districts (rows)\n",
    "2. Get rid of the whole attribute `total_bedrooms` (column)\n",
    "3. Impute (make up) values in some way\n",
    "\n",
    "Missing data is important.  We often don't want to just drop the samples, if we don't have a lot \n",
    "of data.  Also dropping the column seems extreme, especially if the feature attribute seems like \n",
    "it might be useful in forming our hypothesis.  But if we have lots of data we might just do 1 \n",
    "to get a real clean data set, or if we have a columns that has a lot of missing items, it might \n",
    "be useless to try and fill them in with guesses, so we might do 2.\n",
    "\n",
    "Strategies for imputing data range from the basic (just fill in the the average or median value,\n",
    "or fill in with a constant like 0), to more complex.  For example, for important attributes, we\n",
    "might want to try and fill in the missing values with good approximations or guesses to what \n",
    "they should have been.  In effect, we want to create another ML model to guess these values using \n",
    "the samples we already have.  We could train a ML classifier just on this attribute with only the\n",
    "samples that have this data, and use this to provide estimates for the missing values.  \n",
    "For example we might use a simple ML method, like compute similarity of districts, and use the \n",
    "K nearest neighbors, say the most similar 3 districts, to average their `total_bedrooms` and use \n",
    "that as our guess for one of missing districts bedroom total.\n",
    "\n",
    "Below we show the `python` code for these 3 options, though all are commented out.  We will\n",
    "actually use option 3, but demonstrate using `Scikit-Learn` functions to do this kind of\n",
    "filling in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16354.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.575635</td>\n",
       "      <td>35.639314</td>\n",
       "      <td>28.653404</td>\n",
       "      <td>2622.539789</td>\n",
       "      <td>534.914639</td>\n",
       "      <td>1419.687379</td>\n",
       "      <td>497.011810</td>\n",
       "      <td>3.875884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.001828</td>\n",
       "      <td>2.137963</td>\n",
       "      <td>12.574819</td>\n",
       "      <td>2138.417080</td>\n",
       "      <td>412.665649</td>\n",
       "      <td>1115.663036</td>\n",
       "      <td>375.696156</td>\n",
       "      <td>1.904931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.499900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.800000</td>\n",
       "      <td>33.940000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1443.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>784.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>2.566950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.510000</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2119.000000</td>\n",
       "      <td>433.000000</td>\n",
       "      <td>1164.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>3.541550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.010000</td>\n",
       "      <td>37.720000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3141.000000</td>\n",
       "      <td>644.000000</td>\n",
       "      <td>1719.000000</td>\n",
       "      <td>602.000000</td>\n",
       "      <td>4.745325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39320.000000</td>\n",
       "      <td>6210.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>5358.000000</td>\n",
       "      <td>15.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  16512.000000  16512.000000        16512.000000  16512.000000   \n",
       "mean    -119.575635     35.639314           28.653404   2622.539789   \n",
       "std        2.001828      2.137963           12.574819   2138.417080   \n",
       "min     -124.350000     32.540000            1.000000      6.000000   \n",
       "25%     -121.800000     33.940000           18.000000   1443.000000   \n",
       "50%     -118.510000     34.260000           29.000000   2119.000000   \n",
       "75%     -118.010000     37.720000           37.000000   3141.000000   \n",
       "max     -114.310000     41.950000           52.000000  39320.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \n",
       "count    16354.000000  16512.000000  16512.000000   16512.000000  \n",
       "mean       534.914639   1419.687379    497.011810       3.875884  \n",
       "std        412.665649   1115.663036    375.696156       1.904931  \n",
       "min          2.000000      3.000000      2.000000       0.499900  \n",
       "25%        295.000000    784.000000    279.000000       2.566950  \n",
       "50%        433.000000   1164.000000    408.000000       3.541550  \n",
       "75%        644.000000   1719.000000    602.000000       4.745325  \n",
       "max       6210.000000  35682.000000   5358.000000      15.000100  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the missing data of the total_bedrooms feature\n",
    "\n",
    "# option 1: get rid of any districts with this feature missing\n",
    "# housing.dropna(subset=['total_bedrooms'])\n",
    "\n",
    "# option 2: get rid of of the whole attribute, don't use it for training\n",
    "# housing.drop('total_bedrooms', axis=1)\n",
    "\n",
    "\n",
    "# option 3: fill the missing data with some value, the median in this case\n",
    "#median = housing['total_bedrooms'].median()\n",
    "#housing['total_bedrooms'].fillna(median, inplace=True)\n",
    "\n",
    "# i#f you uncommented one of these, you would find that now either we only have a count of 16354\n",
    "# for all attributes, or that we no longre have a total_bedrooms attribute, or for option 3,\n",
    "# that now total_bedrooms has a full count of 16512 like all the other attributes\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have performed option 3 by hand, but if we had other column attributes also with \n",
    "missing data, we would have to handle all of them individually.\n",
    "\n",
    "The `Scikit-Learn` `Imputer` functions allow for more sophisticated missing value corrections.\n",
    "The `SimpleImputer` can fill in missing values (for all or selected columns) using the median,\n",
    "mean, a constant, or some other strategies. \n",
    "\n",
    "Since the `Imputer` only works for numerical attributes, we have to first create a view of our\n",
    "data set without the `ocean_proximity` categorical attribute.  Then we can impute the missing\n",
    "values by filling in the mean.\n",
    "\n",
    "The fit function actually does not do anything (we will talk more about the `Scikit-Learn` \n",
    "API below).  Since we are imputing using the median value, fit simply computes and returns \n",
    "the median for each numerical attribute in the data set it is fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SimpleImputer(strategy='median')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Scikit-Learn function for data cleaning\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# imputer only works with numerical attributes, so create copy of data without categorical features\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "imputer.fit(housing_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-118.51   ,   34.26   ,   29.     , 2119.     ,  433.     ,\n",
       "       1164.     ,  408.     ,    3.54155])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for a median imputer, this should simply be the median of each feature\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-118.51   ,   34.26   ,   29.     , 2119.     ,  433.     ,\n",
       "       1164.     ,  408.     ,    3.54155])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which should equal the calculated median values\n",
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually perform the imputation, call the transform function.  These steps could have \n",
    "been performed together by calling `fit_transform()` on the imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to actually do the imputation with this sklearn object, have to perform transform\n",
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result is a plain NumPy array, if want features of Pandas dataframe, we can put back into a df\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice now that `total_bedrooms` has a count equal to all th eother attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "      <td>16512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.575635</td>\n",
       "      <td>35.639314</td>\n",
       "      <td>28.653404</td>\n",
       "      <td>2622.539789</td>\n",
       "      <td>533.939438</td>\n",
       "      <td>1419.687379</td>\n",
       "      <td>497.011810</td>\n",
       "      <td>3.875884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.001828</td>\n",
       "      <td>2.137963</td>\n",
       "      <td>12.574819</td>\n",
       "      <td>2138.417080</td>\n",
       "      <td>410.806260</td>\n",
       "      <td>1115.663036</td>\n",
       "      <td>375.696156</td>\n",
       "      <td>1.904931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.499900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.800000</td>\n",
       "      <td>33.940000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1443.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>784.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>2.566950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.510000</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2119.000000</td>\n",
       "      <td>433.000000</td>\n",
       "      <td>1164.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>3.541550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.010000</td>\n",
       "      <td>37.720000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3141.000000</td>\n",
       "      <td>641.000000</td>\n",
       "      <td>1719.000000</td>\n",
       "      <td>602.000000</td>\n",
       "      <td>4.745325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39320.000000</td>\n",
       "      <td>6210.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>5358.000000</td>\n",
       "      <td>15.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  16512.000000  16512.000000        16512.000000  16512.000000   \n",
       "mean    -119.575635     35.639314           28.653404   2622.539789   \n",
       "std        2.001828      2.137963           12.574819   2138.417080   \n",
       "min     -124.350000     32.540000            1.000000      6.000000   \n",
       "25%     -121.800000     33.940000           18.000000   1443.000000   \n",
       "50%     -118.510000     34.260000           29.000000   2119.000000   \n",
       "75%     -118.010000     37.720000           37.000000   3141.000000   \n",
       "max     -114.310000     41.950000           52.000000  39320.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \n",
       "count    16512.000000  16512.000000  16512.000000   16512.000000  \n",
       "mean       533.939438   1419.687379    497.011810       3.875884  \n",
       "std        410.806260   1115.663036    375.696156       1.904931  \n",
       "min          2.000000      3.000000      2.000000       0.499900  \n",
       "25%        296.000000    784.000000    279.000000       2.566950  \n",
       "50%        433.000000   1164.000000    408.000000       3.541550  \n",
       "75%        641.000000   1719.000000    602.000000       4.745325  \n",
       "max       6210.000000  35682.000000   5358.000000      15.000100  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice there are no longer any missing values in total_bedrooms\n",
    "housing_tr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn API Design\n",
    "\n",
    "We will probably have a whole lecture video/section on the `Scikit-Learn` ML library and its \n",
    "API later on in the course.  But we have been using some utility functions.  The `SimpleImputer`\n",
    "is an example of an object that is an estimator and a transformer.  \n",
    "\n",
    "The `Scikit-Learn` API is well designed and powerful for many reasons.  One of the main reasons \n",
    "is because of the **consistency** of the defined interface, and that it uses duck typing instead \n",
    "of true object-oriented inheritance to define interfaces that can easily be fit into the \n",
    "`sklearn` framework.\n",
    "\n",
    "- **Estimators** An object that can estimate some parameters based on a dataset is called an\n",
    "_estimator_.  The imputer we used is an estimator.  The estimation is performed by calling the\n",
    "`fit()` function, which  takes a dataset (that is row/column like in nature) as its \n",
    "input.  For supervised learning algorithms, a second parameter is passed into the `fit()`,\n",
    "function, the target labels that the estimator is to fit the model to.\n",
    "\n",
    "- **Transformors** Some estimators can also transform a dataset; these are called transformers.  \n",
    "A transformer has a `transform()` function that takes a dataset as a parameter and returns \n",
    "the transformed dataset.  This transformation generally relies on the learned parameters\n",
    "that are determined by the estimator from the `fit()` function.\n",
    "All transformers are estimators.  All have a convenience function called `fit_transform()`\n",
    "that is equivalent to calling `fit()` followed by `transform()` on the estimator-transformer.\n",
    "\n",
    "- **Predictors** Some (but not all) estimators are capable of making predictions.  This is the\n",
    "basis of building the hypothesis function $h$ we discussed earlier.  The hypothesis funciton\n",
    "is learned by fitting and transforming the dataset to build a model.  Then predictions can be\n",
    "made on new datasets by calling the `predict()` method that takes a dataset of new instances\n",
    "and returns the corresponding predictions.\n",
    "\n",
    "As we will do below to build some pipelines, you can create your own estimator-transformer-predictor object by simply defining a class that implements function with these names taking \n",
    "the expected parameters for input (this id duck typing, you don't inherit from a base class, but \n",
    "you conform to a defined API).\n",
    "\n",
    "Some other aspects of the `Scikit-Learn` API design:\n",
    "\n",
    "- **Inspection** All hyperparameters are accessible directly via public instance variables\n",
    "(e.g. `imputer.strategy`).  All estimators learned parameters are also accessible via public \n",
    "instance variables with an underscore suffix (e.g. `imputer.statistics_`)\n",
    "\n",
    "- **Nonproliferation of classes**  Originally datasets were only represented as `numpy` arrays,\n",
    "or `scipy` sparce matrices.  You may notice if you use the lirbary extensively that \n",
    "now `pandas` dataframes can be passed in as datasets for input, as they conform to \n",
    "an interface to access data by row samples / column attributes.  But any transformations \n",
    "will always be returned as a regular `numpy` array or `scipy` sparse matrix.\n",
    "\n",
    "- **Composition** Existing building blocks are reused as much as possible.  For example \n",
    "a data analysis pipeline is a well known concept in data scinece.  a `Pipeline` can be created\n",
    "in `sklearn` from an arbitrary sequence of transformers follwed by a final estimator (beacause \n",
    "the result of calling `fit()` on a transform is a transformed dataset of the same shape, which \n",
    "can be passed to the next transformer in the pipeline).\n",
    "\n",
    "- **Sensible Defaults** `Scikit-Learn` provides reasonable default falues for most parameters, \n",
    "making it easy to create a baseline working system quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Text and Categorical Attributes\n",
    "\n",
    "We had to leave out the `ocean_proximity` attribute before for the imputer because the \n",
    "imputer can only handle numeric data. But categorical data is common, and useful.  But\n",
    "most machine learning estimators can only use numeric data.\n",
    "\n",
    "Thus while we want to use categorical data, we have to convert it into numerical representations\n",
    "if we want to use if for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 9)\n",
      "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
      "       'total_bedrooms', 'population', 'households', 'median_income',\n",
      "       'ocean_proximity'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(housing.shape)\n",
    "print(housing.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12655</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15502</th>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14053</th>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20496</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18125</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5830</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17989</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ocean_proximity\n",
       "12655          INLAND\n",
       "15502      NEAR OCEAN\n",
       "2908           INLAND\n",
       "14053      NEAR OCEAN\n",
       "20496       <1H OCEAN\n",
       "1481         NEAR BAY\n",
       "18125       <1H OCEAN\n",
       "5830        <1H OCEAN\n",
       "17989       <1H OCEAN\n",
       "4861        <1H OCEAN"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for ml algorithms need to transform categorical information to numerical labels/categories\n",
    "housing_cat = housing[['ocean_proximity']]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw before, the `ocen_proximity` category is relatively clean, and it contains \n",
    "a finite set of 5 well defined categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ocean_proximity\n",
       "<1H OCEAN     7277\n",
       "INLAND        5262\n",
       "NEAR OCEAN    2124\n",
       "NEAR BAY      1847\n",
       "ISLAND           2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again there are some subtle issues we may need to think about when converting \n",
    "categorical data to a numeric representation.  The simplest is to assign an \n",
    "arbitrary integer value to each category.  The `sklearn` `OrdinalEncoder` transformer\n",
    "can do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use Scikit-learn preprocessors for this type of transformation\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [4.],\n",
       "       [1.],\n",
       "       [4.],\n",
       "       [0.],\n",
       "       [3.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see by looking before that 0 was assigned to `<1H OCEAN`, 4 was assigned to `NEAR OCEAN`,\n",
    "etc.  As an example of the transparency we discussed before, the category transformation\n",
    "parameters are available from this encoder as a public instance variable.  The order of the list\n",
    "corresponds to the assigned encoding, 0th element first, followed by 1, 2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of the categories that were encoded\n",
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subtle issue is that ML algorithms will assume or infer some similarity based on \n",
    "the numerical ordering.  It will assume `<1H OCEAN` is more similar to `INLAND` than others.\n",
    "There is maybe a natural ordering to this category, based on distance to (or on) ocean, so \n",
    "we might want to force an ordering of closest to furthest:\n",
    "\n",
    "`[ISLAND, NEAR OCEAN, NEAR BAY, <1H OCEAN, INLAND]`\n",
    "\n",
    "\n",
    "Given you know all of the unique category identifiers, and you have a particular order you need, \n",
    "you can pass in this list to the encoder to have it assign and enforce the particular needed \n",
    "ordering:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.],\n",
       "       [1.],\n",
       "       [4.],\n",
       "       [1.],\n",
       "       [3.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [3.],\n",
       "       [3.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_categories = [['ISLAND', 'NEAR OCEAN', 'NEAR BAY', '<1H OCEAN', 'INLAND']]\n",
    "ordinal_encoder_ordered = OrdinalEncoder(categories=ordered_categories)\n",
    "housing_cat_ordered = ordinal_encoder_ordered.fit_transform(housing_cat)\n",
    "\n",
    "housing_cat_ordered[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['ISLAND', 'NEAR OCEAN', 'NEAR BAY', '<1H OCEAN', 'INLAND'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of the categories that were encoded\n",
    "ordinal_encoder_ordered.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But while this may be the correct solution for this categorical item, some categories really\n",
    "have no natural ordering or relationship between them.  For example you might consider there \n",
    "is a natural order of the category \n",
    "\n",
    "`[FRESHMAN, SOPHMORE, JUNIOR, SENIOR]`\n",
    "\n",
    "in that sequence (or reversed).  But is there any inherent order in say a job title description\n",
    "category for a set of business data?\n",
    "\n",
    "`[SALESMAN, RECEPTIONIST, CUSTODIAL, MANAGER, HUMAN RESOURCES, INFORMATION TECHNOLOGY]'\n",
    "\n",
    "A common solution for categorical data with no inherant order is to encode it using a \n",
    "1-hot encoding.  So for example, our attribute with 5 categories would get converted into \n",
    "5 column attributes, each representing one of the original categories.  And only 1 of\n",
    "these columns will be set to 1 for each sample, all others will be 0.\n",
    "\n",
    "We can use an `sklearn` `OneHotEncoder` to encode into this kind of categorical representation.\n",
    "The following example should make this clear what we are doing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16512x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16512 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# might want to use one-hot-encoding instead, especially for categorical information without an obvious ordering of\n",
    "# the numerical mappings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So notice the encoding transformation results in an array with 5 columns.  Each row only has one of \n",
    "the columns set to 1.  The `categories_` shos that column 0 encodes whether less than 1 hour to \n",
    "ocean or not, column 1 is whether INLAND or not, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transformers\n",
    "\n",
    "Here we revisit the `Scikit-Learn` API with an example of what we mean by making it easy \n",
    "to add our own transformers to the language using Duck typing.\n",
    "This is the beginning of some more advanced understanding of using the `Scikit-Learn` library. \n",
    "We can basically provide our own transformer/estimators by simply creating a class and\n",
    "defining `fit()`, `transform()` and `fit_transform()` methods.  If the class conform to this\n",
    "API, can then be used in `Scikit-Learn` pipelines.\n",
    "\n",
    "Despite what I said, we do also do some formal OO inheritance, inheriting from the \n",
    "`BaseEstimator` and the `TransformerMixin` from `sklearn`.  The allows us to inherit\n",
    "some basic functions.  We could have implemented the `fit_transform()` method ourself, \n",
    "just by calling our own `fit()` followed by `transform()`.  The `TransformerMixin` \n",
    "basically provides this basic implementation of `fit_transform()`.  The \n",
    "`BaseEstimator` addes in `get_params()` and `set_params()` methods to our class that \n",
    "we may discuss later.\n",
    "\n",
    "For example, we might want to have a formal transformer to fit into the `Scikit-Learn` api \n",
    "so that we can add in the combined attributes we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# indexes of the attributes we are using to create new combined attributes with\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Create a new Transformer class to add in combined attributes to the housing\n",
    "    dataset.  This transformer inherits from both the sklearn BaseEstimator, \n",
    "    and the TransformerMixin.  The former automatically creates a `fit_transform()` \n",
    "    from our `fit()` and `transform()` methods.  The latter addes in \n",
    "    `get_params()` and `set_params()` methods to the transformer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        \"\"\"Class meta-parameter add_bedrooms_per_room.  By default this \n",
    "        transformer adds this attribute, but we can exclude by setting this \n",
    "        class metaparameter to false.\n",
    "        \"\"\"\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Define a fit() function for this transformer.  We don't really have\n",
    "        anything to do to \"fit\" this transformer in our case.\n",
    "        \"\"\"\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"The purpose of this transformer is to add in the combined attributes\n",
    "        rooms_per_household, population_per_household, and bedrooms_per_room if \n",
    "        desired.\n",
    "        \"\"\"\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        \n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            # we return the original array X with newly created features concatenated to end \n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            # we return the original array only with the default 2 features concatenated to end\n",
    "            return np.c_[X, rooms_per_household, population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using the transformer to add the 2 attributes, but not the bedrooms_per_room, to the housing data\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "print(housing.shape)\n",
    "print(housing_extra_attribs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Another data preparation we often must deal with is scaling.  We already mentioned that \n",
    "some machine learning methods are sensitive to having feature attributes of different scales.\n",
    "Others can have issues when an attribute has a strange, far from normal distribution.  \n",
    "\n",
    "There are 2 common ways to get all attributes to the same scale: **min-max scaling** and \n",
    "**standardization**.\n",
    "\n",
    "We can implement both of these by hand relatively easily.  Min-max scaling (also commonly \n",
    "called **normalization**) is quite simple, values are shifted and rescaled so they end up\n",
    "ranging from 0 to 1 (or alternatively from some common minimum value to some common maximum\n",
    "value).\n",
    "\n",
    "Using vectorized operations, we can do this for all numerical attributes of a numpy array\n",
    "very simply, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-121.46    38.52    29.    3873.     797.    2237.     706.       2.174]\n",
      " [-117.23    33.09     7.    5320.     855.    2015.     768.       6.337]\n",
      " [-119.04    35.37    44.    1618.     310.     667.     300.       2.875]\n",
      " [-117.13    32.75    24.    1877.     519.     898.     483.       2.226]\n",
      " [-118.7     34.28    27.    3536.     646.    1837.     580.       4.496]]\n",
      "Minimum of each feature:  [-124.35   32.54    1.      6.      2.      3.      2.      0.5 ]\n",
      "[[   2.89     5.98    28.    3867.     795.    2234.     704.       1.674]\n",
      " [   7.12     0.55     6.    5314.     853.    2012.     766.       5.837]\n",
      " [   5.31     2.83    43.    1612.     308.     664.     298.       2.375]\n",
      " [   7.22     0.21    23.    1871.     517.     895.     481.       1.726]\n",
      " [   5.65     1.74    26.    3530.     644.    1834.     578.       3.997]]\n",
      "Minimum of each feature after shifting:  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Maximum of each feature:  [   10.04     9.41    51.   39314.    6208.   35679.    5356.      14.5 ]\n",
      "[[0.288 0.635 0.549 0.098 0.128 0.063 0.131 0.115]\n",
      " [0.709 0.058 0.118 0.135 0.137 0.056 0.143 0.403]\n",
      " [0.529 0.301 0.843 0.041 0.05  0.019 0.056 0.164]\n",
      " [0.719 0.022 0.451 0.048 0.083 0.025 0.09  0.119]\n",
      " [0.563 0.185 0.51  0.09  0.104 0.051 0.108 0.276]]\n",
      "Maximum of each feature after shifting:  [1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# use the previous translation with missing values filled in, turn into a numpy array\n",
    "housing_num = housing_tr.values\n",
    "\n",
    "# show first 5 samples \n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(housing_num[:5])\n",
    "\n",
    "# to normalize each column/feature subtracts the minimum value, thus the range is\n",
    "# shifted from 0 to max\n",
    "print(\"Minimum of each feature: \", housing_num.min(axis=0))\n",
    "housing_num = housing_num - housing_num.min(axis=0)\n",
    "print(housing_num[:5])\n",
    "print(\"Minimum of each feature after shifting: \", housing_num.min(axis=0))\n",
    "\n",
    "# now if we divide each column by the max, we will rescale all values to the range from 0 to 1:\n",
    "print(\"Maximum of each feature: \", housing_num.max(axis=0))\n",
    "housing_num = housing_num / housing_num.max(axis=0)\n",
    "print(housing_num[:5])\n",
    "print(\"Maximum of each feature after shifting: \", housing_num.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you look at the histograms of these features, you will see that the distributions have \n",
    "not changed.  All are now normalized to a range from 0 to 1, but data is still distributed \n",
    "exactly in the same shape as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to a dataframe to visualize\n",
    "housing_num = pd.DataFrame(housing_num, columns=housing_tr.columns)\n",
    "housing_num.hist(bins=50, figsize=(20,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization does not guarantee values of all attributes are bounded within some specific\n",
    "range.  Thus for attributes with big outlier values, scaling issues could still be a problem.\n",
    "The result after standardization is that the distribution will now have a mean of 0\n",
    "and a standard deviaiton of 1. But as you can see below, the shape of the distributions\n",
    "are again still the same, we are simply are scaling and shifting the value.  But now most\n",
    "values will be around 0, and a majority of values will be within the range from -1 to 1.0.\n",
    "\n",
    "So the steps to do this are the same, but we subtract the `mean()` instead of the minimum for \n",
    "the shift, and we divide by the `std()` instead of the max when we scale.\n",
    "\n",
    "Notice for example with the population attribute, we can end up with values still in a big\n",
    "range, from about -3 to 30 wheres housing median age is pretty much constrained to the range\n",
    "-2 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing_tr.values\n",
    "\n",
    "# shift all attributes to a mean of 0\n",
    "housing_num = housing_num - housing_num.mean(axis=0)\n",
    "\n",
    "# rescale the distributions so they all have a standard deviation of 1\n",
    "housing_num = housing_num / housing_num.std(axis=0)\n",
    "\n",
    "# convert back to a dataframe to visualize\n",
    "housing_num = pd.DataFrame(housing_num, columns=housing_tr.columns)\n",
    "housing_num.hist(bins=50, figsize=(20,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Pipelines\n",
    "\n",
    "To finish up this discussion on preparing the data for Machine Learning, lets build a pipeline\n",
    "to perform needed transformations.  We will use this pipeline for our training and tuning\n",
    "examples in the next presentation.  First recall that our variable `housing` and `housing_label`\n",
    "have been set aside.  They were created from the stratified training sample when we \n",
    "talked about test / train splits.  We will first create a small pipeline for the numerical \n",
    "attributes.  Lets pull the numerical attributes from the clean housing data set again before \n",
    "we begin our pipeline transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to recall, housing has the original attributes, and we havin't done the \n",
    "# data cleaning or scaling on any of it yet.  This is a training set, created from\n",
    "# the stratified sampling train / test split.  Notice that there are 16512 samples,\n",
    "# though total_bedrooms has missing data, with 16354 samples only.\n",
    "print(housing.shape)\n",
    "housing.describe()\n",
    "print(housing.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the numerical attribute features for our numerical pipeline\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "print(housing.shape)\n",
    "print(housing.columns)\n",
    "print(housing_num.shape)\n",
    "print(housing_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First an example pipeline that handles the numerical columns only. \n",
    "\n",
    "This pipeline imputes missing values for our `total_bedrooms` attribute.  It is using the \n",
    "`CombinedAttributesAdder` transformer we created ourselvs to add in the additnal custom\n",
    "features that we want to create.  And we use a `StandardScalar` to perform standardized \n",
    "scaling on all numerical attributes, resulting in them all having a mean of 0 and a \n",
    "standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data before the scaling and transformations.  Notice the mean and standard deviations, and min \n",
    "# and max values range before we do the scaling.  Also notice we are missing values from \n",
    "# total_bedrooms\n",
    "print(housing_num.shape)\n",
    "housing_num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data after the scaling and transformations.  Notice the mean and std of the numerical features\n",
    "# are all 0 and 1 respectively.\n",
    "# first transform back to a DataFrame so we can call the describe() function\n",
    "cols = housing_num.columns.append(pd.Index(['rooms_per_household', 'population_per_household', 'bedrooms_per_room']))\n",
    "housing_num_tr = pd.DataFrame(housing_num_tr, columns=cols)\n",
    "print(housing_num_tr.shape)\n",
    "housing_num_tr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only done our data preparation and transformations for the numerical \n",
    "attributes.  Lets not forget the categorical attribute. \n",
    "\n",
    "It would be convenient to have a single transformer pipeline to handle all columns.  \n",
    "There is a relatively new transformer in `sklearn` that can do just that for us,\n",
    "the `ColumnTransformer`.  Basically it again takes a sequence of transformer objects, but\n",
    "we also specify which columns each transformer/pipeline are applied to.\n",
    "\n",
    "You can pass in a list of integer column indexes if working with `numpy` arrays.\n",
    "But this transformer also understands `pandas` `DataFrame` objects, in which case you \n",
    "can pass in lists of column names as well.\n",
    "\n",
    "Also note, the `num_pipeline` pipeline transformer we created previously is itself still \n",
    "a transformer, albeit one that combines several smaller transformers in a sequence.\n",
    "But here our full data cleaning transformation pipeline combines the transformer for \n",
    "numerical attributes, and the one to handle our categeorical attribute, into a single\n",
    "transformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# a full pipeline.  We use previous pipeline for numerical attributes, and add in a column transformer to transform the \n",
    "# categorical ocean_proximity to a one-hot-encoded set of columns\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = ['ocean_proximity']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "    ('cat', OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the full data preparation transformation pipeline is a new dataset\n",
    "with one-hot encoding columns for the `ocean_proximity` attribute, and all \n",
    "attributes imputed for missing values, and all attributes scaled using standard\n",
    "scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing_prepared.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
