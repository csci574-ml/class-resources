{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:43.433874Z",
     "start_time": "2019-11-19T13:53:42.746201Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:43.440516Z",
     "start_time": "2019-11-19T13:53:43.435205Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 8) # set default figure size, 12in by 10in\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will be covering anomaly detection which is widely used in fraud detection (e.g. ‘has this credit card been stolen?’). Given a large number of data points, we may sometimes want to figure out which ones vary significantly from the average. For example, in manufacturing, we may want to detect defects or anomalies. We show how a dataset can be modeled using a Gaussian distribution, and how the model can be used for anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W9 01: Problem Motivation (Anomaly Detection)\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=UqqPm-Q4aMo)\n",
    "\n",
    "**NOTE**: The original series of lectures has gone away.  This link appears to be all of the following parts combined\n",
    "into a single video.  I don't have video time indexes into the parts yet so will have to search for the corresponding\n",
    "parts currently.\n",
    "\n",
    "Imagine you are a a manufacturer and are performing quality control on your manufacturing process. You might\n",
    "gather data with features like:\n",
    "\n",
    "- $x_0 = $ heat generated\n",
    "- $x_1 = $ vibration intensity.\n",
    "\n",
    "We might have data that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:44.127307Z",
     "start_time": "2019-11-19T13:53:43.442169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(4.1, 1.7, 'anomaly')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just make a random dataset example where both features are distributed in a normal distribution\n",
    "m = 30 # number of samples to generate\n",
    "mu = 0.0\n",
    "sigma = 0.5\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.normal(mu, sigma, (m, 2)) + 3.0\n",
    "\n",
    "# plot the ok data\n",
    "plt.plot(X[:,0], X[:,1], 'rx');\n",
    "plt.axis([1, 5, 1, 5]);\n",
    "plt.xlabel('$x_0$ (heat)', fontsize=16);\n",
    "plt.ylabel('$x_1$ (vibration)', fontsize=16);\n",
    "\n",
    "# new item x_test\n",
    "x_test1 = np.array([3.3, 2.8])\n",
    "plt.plot(x_test1[0], x_test1[1], 'gx', markersize=10)\n",
    "plt.text(x_test1[0]+0.1, x_test1[1]-0.1, 'ok', fontsize=16)\n",
    "\n",
    "x_test2 = np.array([4.0, 1.8])\n",
    "plt.plot(x_test2[0], x_test2[1], 'gx', markersize=10)\n",
    "plt.text(x_test2[0]+0.1, x_test2[1]-0.1, 'anomaly', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formaly, in anomaly detection we are given some dataset: $\\{ x^{(1)}, x^{(2)}, \\cdots, x^{(m)} \\}$\n",
    "\n",
    "We assume that the given dataset is normal, that it represents usual samples of the dataset.\n",
    "\n",
    "We want an algorithm that tells us if a new sample $x_{test}$ is \"anomalous\", where this will mean that\n",
    "it looks out of the expected range or pattern for the dataset given the nonanomalous data.\n",
    "\n",
    "In one sense the training set is unlabeled, though you can also think of it as a set of data all labeled\n",
    "as ok.  \n",
    "\n",
    "We want to build a model $p(x)$, which is the probability of $x$, where $x$ is the features of the dataset.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "p(x_{test}) < \\epsilon \\to \\text{flag as anomalous}\\\\\n",
    "p(x_{test}) \\ge \\epsilon \\to \\text{ok}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "The model $p(x)$ needs to estimate the \"density\" of the data, so that points in the dense part of the\n",
    "distribution are assigned a high probability density, and those that are far from other examples are\n",
    "assigned a low probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection example\n",
    "\n",
    "- Fraud detection:\n",
    "  - $x^{(i)} = $ features of user $i$'s activities (how often log in, typing speed, etc.)\n",
    "  - Model $p(x)$ from data.\n",
    "  - Identify unusual users by checking which have $p(x) < \\epsilon$\n",
    "  \n",
    "- Manufacturing\n",
    "- Monitoring computers in a data center.\n",
    "  - $x^{(i)} = $ features of machine $i$\n",
    "  - memory use, number of disk accesses/sec, CPU load, CPU load / network traffic\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W9 02: Gaussian Distribution\n",
    "\n",
    "[YouTube Video Link]()\n",
    "\n",
    "## Gaussian (Normal) distribution\n",
    "\n",
    "A review of the gaussian distribution.  Say $x \\in \\mathbb{R}$ If $x$ is a random variable distributed according\n",
    "to a Gaussian distribution, with a mean $\\mu$ and variance $\\sigma^2$ (or equivalently standard deviation $\\sigma$,\n",
    "then the continuous probability density function of the random variable is given by the formula:\n",
    "\n",
    "\\begin{equation}\n",
    "n(x; \\mu, \\sigma) = \\frac{1}{ \\sqrt{2 \\pi \\sigma} } e^{- \\frac{1}{2 \\sigma^2} (x - \\mu)^2 }\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We normally don't have to know the formula to work with a gaussian distribution.  In Python, it is\n",
    "best to use the `scipy.stats` classes that are defined to work with standard probability distributions.\n",
    "The `scipy.stats.norm` class defines the normal or gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:44.543370Z",
     "start_time": "2019-11-19T13:53:44.128693Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\m'\n",
      "/tmp/ipykernel_8508/971420216.py:20: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  plt.plot(x, n_standard.pdf(x), 'b-', label='standard normal distribution $\\mu=0, \\sigma=1$')\n",
      "/tmp/ipykernel_8508/971420216.py:21: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  plt.plot(x, n_1.pdf(x), 'g-', label='normal distribution $\\mu=%0.1f, \\sigma=%0.1f$' % (mu_1, sigma_1))\n",
      "/tmp/ipykernel_8508/971420216.py:22: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  plt.plot(x, n_2.pdf(x), 'r-', label='normal distribution $\\mu=%0.1f, \\sigma=%0.1f$' % (mu_2, sigma_2))\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# the default when creating a normal distribution from scipy.stats is to create a standard\n",
    "# normal distribution with mu=0 and sigma=1, so we don't really have to pass in the\n",
    "# metaparameters here if we want a standard normal distribution, we are given one by default\n",
    "n_standard = norm()\n",
    "\n",
    "# create some other normal distributions\n",
    "mu_1 = 5.0\n",
    "sigma_1 = 0.4\n",
    "n_1 = norm(mu_1, sigma_1)\n",
    "\n",
    "mu_2 = -2.0\n",
    "sigma_2 = 2.2\n",
    "n_2 = norm(mu_2, sigma_2)\n",
    "\n",
    "\n",
    "# plot the standard normal distribution and the other ones for comparison\n",
    "x = np.linspace(-8.0, 8.0, 1000)\n",
    "plt.plot(x, n_standard.pdf(x), 'b-', label='standard normal distribution $\\mu=0, \\sigma=1$')\n",
    "plt.plot(x, n_1.pdf(x), 'g-', label='normal distribution $\\mu=%0.1f, \\sigma=%0.1f$' % (mu_1, sigma_1))\n",
    "plt.plot(x, n_2.pdf(x), 'r-', label='normal distribution $\\mu=%0.1f, \\sigma=%0.1f$' % (mu_2, sigma_2))\n",
    "plt.xlabel('normal random variable x', fontsize=16);\n",
    "plt.ylabel('$f(x)$ probability density function PDF', fontsize=16);\n",
    "plt.legend();\n",
    "plt.title('Gaussian (Normal) distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we use the `.pdf()` function of the `scipy.stats.norm` object to plot the probability density function\n",
    "of the normal curves.   A standard normal distribution has a mean of 0 and a standard deviation/variance of 1.\n",
    "Notices that for other normal distributions, the `norm` object takes the mean $\\mu$ and the standard deviation\n",
    "$\\sigma$ as meta parameters that define the distributions.  When the standard deviation is small, the\n",
    "distribution will be skinny and tall.  When the standard deviation is large, the density will be more flat\n",
    "and spread out.  But in all cases, the are probability density functions, so the area under the curves\n",
    "all add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:44.574548Z",
     "start_time": "2019-11-19T13:53:44.544803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999999999999998, 1.0178191320905743e-08)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.integrate import quad as integrate\n",
    "\n",
    "integrate(n_standard.pdf, -np.infty, np.infty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, we can also draw or generate random samples from a normal distribution. I actually did that above\n",
    "to create the example data, using the `np.random.normal()` function.  You can also use `scipy.stats.norm`\n",
    "function to draw a random sample from a normal distribution, using the `.rvs()` function, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:45.159552Z",
     "start_time": "2019-11-19T13:53:44.577255Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8508/2226870861.py:7: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(samples, hist=True, rug=True, label='estimated density function');\n"
     ]
    }
   ],
   "source": [
    "# draw random samples\n",
    "NUM_SAMPLES = 1000\n",
    "samples = n_standard.rvs(NUM_SAMPLES)\n",
    "\n",
    "# visualize the resulting density of the random samples, using seaborn built in scatter plotting and density\n",
    "# visualization capabilities\n",
    "sns.distplot(samples, hist=True, rug=True, label='estimated density function');\n",
    "\n",
    "# put the true pdf density function onto the estimated density plot\n",
    "x = np.linspace(-4.0, 4.0, 1000)\n",
    "plt.plot(x, n_standard.pdf(x), 'r-', label='actual standard normal pdf')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation ##\n",
    "\n",
    "The parameter estimation problem is basically, given a set of data that you suspect may be distributed according to a\n",
    "Gaussian distribution, how do you determine the values of the mean $\\mu$ and the standard deviation $\\sigma$ that best\n",
    "fit or explain the data you have been given.  We can use a simple maximum likelihood estimate to determine which values\n",
    "for the $\\mu, \\sigma$ parameters fit the data.  To estimate $\\mu$, simply calculate the mean of the data points\n",
    "you have been given:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)}\n",
    "$$\n",
    "\n",
    "Likewise, to estimate the variance $\\sigma^2$, sum up the squares of each data point subtracted from your estimated\n",
    "mean value:\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x^{(i)} - \\mu)^2\n",
    "$$\n",
    "\n",
    "Just to show that this approximation works, and to generate a set of data and figure similar to the one from our\n",
    "companion video, lets create a set of 20 points drawn from a normal distribution with a mean of 1.5 and\n",
    "a standard deviation of 0.75 using python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:45.454900Z",
     "start_time": "2019-11-19T13:53:45.160860Z"
    }
   },
   "outputs": [],
   "source": [
    "m = 20 # number of data points\n",
    "mu_actual = 1.5 # the actual mean we will use to generate some example data points\n",
    "sigma_actual = 0.75 # the actual standard deviation we will use to generate random data points\n",
    "\n",
    "x = np.random.normal(loc=mu_actual, scale=sigma_actual, size = (m,))\n",
    "plt.plot(x, np.zeros( (m,)), 'rx');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the actual mean and standard deviation we used for the points are 1.5 and 0.75 respectively.  We can get\n",
    "mean likelihood estimates of the values for our randomly generated data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:45.460155Z",
     "start_time": "2019-11-19T13:53:45.456829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3382921172106135 0.8480846198163837 0.7192475223691002\n"
     ]
    }
   ],
   "source": [
    "mu_estimate = np.sum(x) / m\n",
    "variance_estimate = np.sum( (x - mu_estimate)**2.0 ) / m\n",
    "sigma_estimate = np.sqrt(variance_estimate)\n",
    "print(mu_estimate, sigma_estimate, variance_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:45.544582Z",
     "start_time": "2019-11-19T13:53:45.461600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3382921172106135 0.8480846198163837 0.7192475223691002\n"
     ]
    }
   ],
   "source": [
    "# the .mean(), .std() and .var() functions of numpy arrays calculate the same thing \n",
    "print(x.mean(), x.std(), x.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W9 03: Anomaly Detection Algorithm\n",
    "\n",
    "[YouTube Video Link]()\n",
    "\n",
    "## Density Estimation ##\n",
    "\n",
    "One way to estimate the probability of a given observation being an anomaly or not is to make an independence assumption on\n",
    "each of the $n$ features, and calculate the probability of getting each feature given our observed data.  To do this, we\n",
    "use the previous method and assume that each of our features is normally distributed, and use a maximum likelihood estimate\n",
    "to determine the mean and variance parameters for each of our features independently.  Given these estimations, we can use them\n",
    "to calculate the probability of each observed feature, and multiplying all of these probabilities together (which works\n",
    "because of the assumption of independence) gives us an overall estimate of the probability of seeing the new observation.\n",
    "\n",
    "The final probability (or density) estimate can be stated compactly using the product formula\n",
    "\n",
    "$$\n",
    "\\prod_{j=1}^n p(x_j; \\mu_j, \\sigma_j^2) = p(x_1; \\mu_1, \\sigma_1^2) \\times p(x_2; \\mu_2, \\sigma_2^2) \\times \\ldots \\times p(x_n; \\mu_n, \\sigma_n^2)\n",
    "$$\n",
    "\n",
    "Where for each of our $n$ features, we have determined the maximum likelihood $\\mu_j, \\sigma_j^2$ mean and variance\n",
    "parameters.  The $\\prod$ symbol simply means we are multiplying each of the $n$ probabilities together.\n",
    "\n",
    "## Anomaly Detection Algorithm ##\n",
    "\n",
    "Putting these previous concepts together, we can build an anomaly detection system using the following algorithm:\n",
    "\n",
    "1. Choose features $x_i$ that you think might be indicative of anomalous examples.\n",
    "2. Fit parameters $\\mu_1, \\ldots, \\mu_n, \\sigma_1^2, \\ldots, \\sigma_n^2$\n",
    "$$\n",
    "\\mu_j = \\frac{1}{m} \\sum_{i=1}^{m} x_j^{(i)} \\\\\n",
    "\\sigma_j^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_j^{(i)} - \\mu_j)^2 \n",
    "$$\n",
    "3. Given a new example $x$, compute $p(x)$:\n",
    "$$\n",
    "p(x) = \\prod_{j=1}^n p(x_j; \\mu_j, \\sigma_j^2) = \\prod_{j=1}^{n} \\frac{1}{\\sqrt{2 \\pi} \\sigma_j} \\textrm{exp}(- \\frac{(x_j - \\mu_j)^2}{2 \\sigma_j^2})\n",
    "$$\n",
    "\n",
    "And finally, we will determine a new observation is an anomaly if the probability is below some threshold, \n",
    "e.g. if $p(x) < \\epsilon$.\n",
    "\n",
    "Lets give an example in Python similar to the one shown in our video.  Lets again generate a set of data, but with\n",
    "2 dimensions or features in this case.  As shown in the video, the mean and standard deviation we will use for our two\n",
    "features will be:\n",
    "$$\n",
    "\\mu_1 = 5, \\sigma_1 = 2\\\\\n",
    "\\mu_2 = 3, \\sigma_2 = 1\n",
    "$$\n",
    "\n",
    "Here we will show vectorized versions of computing the maximum likelihood estimates and probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:45.626538Z",
     "start_time": "2019-11-19T13:53:45.547851Z"
    }
   },
   "outputs": [],
   "source": [
    "m, n = 20, 2 # number of example data points ; number of features/dimensions\n",
    "mu_actual = np.array([5.0, 3.0])\n",
    "sigma_actual = np.array([2.0, 1.0])\n",
    "\n",
    "# we will generate a mxn array of values with mean 0 and std 1.0\n",
    "x = np.random.normal(loc=0.0, scale=1.0, size=(m, n))\n",
    "\n",
    "# and we will scale each dimension to have the desired mean/std.\n",
    "x[:,0] = (x[:,0] * sigma_actual[0]) + mu_actual[0]\n",
    "x[:,1] = (x[:,1] * sigma_actual[1]) + mu_actual[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:46.063583Z",
     "start_time": "2019-11-19T13:53:45.627934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7b8393b1f110>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display our randomly generated set of 20 data points in\n",
    "# our 2 dimensional feature space\n",
    "#plt.plot(x[:,0], x[:,1], 'rx', markersize=10, markeredgewidth=3, label='training data points')\n",
    "plt.scatter(x[:,0], x[:,1], c='r', marker='x', s=75, label='training data points')\n",
    "plt.xlabel('$x_0$', fontsize=20)\n",
    "plt.ylabel('$x_1$', fontsize=20)\n",
    "plt.axis([-2, 12, -2, 8])\n",
    "\n",
    "# display location of 2 new observations we will use to demonstrate\n",
    "# calculating the density or probability estimate\n",
    "x_new = np.array([[5.5, 2.9],\n",
    "                  [2.5, 0.8]])\n",
    "#plt.plot(x_new[:,0], x_new[:,1], 'bo', markersize=10, markeredgewidth=3, label='new observations')\n",
    "plt.scatter(x_new[:,0], x_new[:,1], c='b', marker='o', s=75, label='new observations')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:46.070019Z",
     "start_time": "2019-11-19T13:53:46.064832Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function that compute the probability/density of an observation\n",
    "def probability_density_function(x, mu, sigma):\n",
    "    \"\"\"Calculate the probability of the given observation x for a normal distribution with mean mu and\n",
    "    standard deviation sigma.  These are known as computing the probability density function, or pdf.\n",
    "    This is equivalent to the scipy libraries stats.norm.pdf() function.\n",
    "    \"\"\"\n",
    "    return (1.0 / (np.sqrt(2.0 * np.pi) * sigma) ) * np.exp(- (x - mu)**2.0 / (2.0 * sigma**2.0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:46.166644Z",
     "start_time": "2019-11-19T13:53:46.071420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum likelihood mean and variance for our 2 dimensions:\n",
      "[6.04844122 2.96691212]\n",
      "[1.54294066 1.10933966]\n",
      "\n",
      "A new observation that is not analomous: [5.5 2.9]\n",
      "Probability for dimension 0: 0.2427309421015668\n",
      "Probability for dimension 1: 0.35896781649658716\n",
      "Product of independent probabilities: 0.08713259628235895\n",
      "\n",
      "A new observation that IS analomous: [2.5 0.8]\n",
      "Probability for dimension 0: 0.018367945262696094\n",
      "Probability for dimension 1: 0.05337234663482837\n",
      "Product of independent probabilities: 0.0009803403415301697\n"
     ]
    }
   ],
   "source": [
    "# calculate the maximum likelihood mean and variance for each dimension\n",
    "print( \"The maximum likelihood mean and variance for our 2 dimensions:\" )\n",
    "print( np.mean(x, axis=0) )\n",
    "print( np.std(x, axis=0) )\n",
    "mu_estimate = np.mean(x, axis=0)\n",
    "sigma_estimate = np.std(x, axis=0)\n",
    "\n",
    "# calculate the density or probability estimate for the new observations\n",
    "# first an example of the probability calculation for the point 0, for dimension 0 and 1\n",
    "print( \"\" )\n",
    "print( \"A new observation that is not analomous:\", x_new[0,:] )\n",
    "p_0 = probability_density_function(x_new[0, 0], mu_estimate[0], sigma_estimate[0])\n",
    "print( \"Probability for dimension 0:\", p_0 )\n",
    "p_1 = probability_density_function(x_new[0, 1], mu_estimate[1], sigma_estimate[1])\n",
    "print( \"Probability for dimension 1:\", p_1 )\n",
    "print( \"Product of independent probabilities:\", p_0 * p_1 )\n",
    "\n",
    "# now the same calculation for our anamolous point 1\n",
    "print( \"\" )\n",
    "print( \"A new observation that IS analomous:\", x_new[1,:] )\n",
    "p_0 = probability_density_function(x_new[1, 0], mu_estimate[0], sigma_estimate[0])\n",
    "print( \"Probability for dimension 0:\", p_0 )\n",
    "p_1 = probability_density_function(x_new[1, 1], mu_estimate[1], sigma_estimate[1])\n",
    "print( \"Probability for dimension 1:\", p_1 )\n",
    "print( \"Product of independent probabilities:\", p_0 * p_1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:46.297383Z",
     "start_time": "2019-11-19T13:53:46.167771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A new observation that is not analomous: [5.5 2.9]\n",
      "Probability for dimension 0: 0.24273094210156682\n",
      "Probability for dimension 1: 0.35896781649658716\n",
      "Product of independent probabilities: 0.08713259628235896\n",
      "\n",
      "A new observation that IS analomous: [2.5 0.8]\n",
      "Probability for dimension 0: 0.018367945262696087\n",
      "Probability for dimension 1: 0.05337234663482838\n",
      "Product of independent probabilities: 0.0009803403415301693\n"
     ]
    }
   ],
   "source": [
    "# we can get the exact same results using the pdf function from scipy, which we should normally do rather\n",
    "# than implementing the function by hand as I have done above\n",
    "import scipy.stats\n",
    "print(\"\")\n",
    "print(\"A new observation that is not analomous:\", x_new[0,:])\n",
    "p_0 = scipy.stats.norm.pdf(x_new[0, 0], loc=mu_estimate[0], scale=sigma_estimate[0])\n",
    "print(\"Probability for dimension 0:\", p_0)\n",
    "p_1 = scipy.stats.norm.pdf(x_new[0, 1], loc=mu_estimate[1], scale=sigma_estimate[1])\n",
    "print(\"Probability for dimension 1:\", p_1)\n",
    "print(\"Product of independent probabilities:\", p_0 * p_1)\n",
    "\n",
    "# now the same calculation for our anamolous point 1\n",
    "print(\"\")\n",
    "print(\"A new observation that IS analomous:\", x_new[1,:])\n",
    "p_0 = scipy.stats.norm.pdf(x_new[1, 0], loc=mu_estimate[0], scale=sigma_estimate[0])\n",
    "print(\"Probability for dimension 0:\", p_0)\n",
    "p_1 = scipy.stats.norm.pdf(x_new[1, 1], loc=mu_estimate[1], scale=sigma_estimate[1])\n",
    "print(\"Probability for dimension 1:\", p_1)\n",
    "print(\"Product of independent probabilities:\", p_0 * p_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:46.414742Z",
     "start_time": "2019-11-19T13:53:46.298508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities of all points for each dimension:\n",
      "[[0.24273094 0.35896782]\n",
      " [0.01836795 0.05337235]]\n",
      "\n",
      "The computed product of the probabilites for our points:\n",
      "[0.0871326  0.00098034]\n"
     ]
    }
   ],
   "source": [
    "# or to vectorize the above\n",
    "print(\"probabilities of all points for each dimension:\")\n",
    "p = scipy.stats.norm.pdf(x_new, loc=mu_estimate, scale=sigma_estimate)\n",
    "print(p)\n",
    "\n",
    "print(\"\")\n",
    "print(\"The computed product of the probabilites for our points:\")\n",
    "p = np.prod(p, axis=1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:46.510925Z",
     "start_time": "2019-11-19T13:53:46.416275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n"
     ]
    }
   ],
   "source": [
    "# and finally, we can select an epsilon threshold, and determine which points are anomalous and which not\n",
    "# using the given threshold\n",
    "epsilon = 0.005\n",
    "print(p < epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cells showed how we can compute the anomaly detection algoritm.  Lets visualize what an epsilon value of 0.005 means\n",
    "for our given set of 20 data points.  Here we will plot what is essentially the decision boundary for our given set of\n",
    "data, above which the threshold will determine points are \"anamolous\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:47.299946Z",
     "start_time": "2019-11-19T13:53:46.512227Z"
    }
   },
   "outputs": [],
   "source": [
    "# display our randomly generated set of 20 data points in\n",
    "# our 2 dimensional feature space\n",
    "#plt.plot(x[:,0], x[:,1], 'rx', markersize=10, markeredgewidth=3, label='training data points')\n",
    "plt.scatter(x[:,0], x[:,1], c='r', marker='x', s=75, label='training data points')\n",
    "plt.xlabel('$x_0$', fontsize=20)\n",
    "plt.ylabel('$x_1$', fontsize=20)\n",
    "plt.axis([-2, 12, -2, 8])\n",
    "\n",
    "# display location of 2 new observations we will use to demonstrate\n",
    "# calculating the density or probability estimate\n",
    "x_new = np.array([[5.5, 2.9],\n",
    "                  [2.5, 0.8]])\n",
    "#plt.plot(x_new[:,0], x_new[:,1], 'bo', markersize=10, markeredgewidth=3, label='new observations')\n",
    "plt.scatter(x_new[:,0], x_new[:,1], c='b', marker='o', s=75, label='new observations')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# and create a mesh to detect decision boundary for a given epsilon and plot it\n",
    "epsilon = 0.005\n",
    "x_min, x_max = -2.0, 12.0\n",
    "y_min, y_max = -2.0, 8.0\n",
    "h = .01  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "gridpoints = np.c_[xx.ravel(), yy.ravel()]\n",
    "p = scipy.stats.norm.pdf(gridpoints, loc=mu_estimate, scale=sigma_estimate)\n",
    "p = (np.prod(p, axis=1) < epsilon) * 1.0\n",
    "Z = p.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:48.405188Z",
     "start_time": "2019-11-19T13:53:47.301369Z"
    }
   },
   "outputs": [],
   "source": [
    "# display our randomly generated set of 20 data points in\n",
    "# our 2 dimensional feature space\n",
    "#plt.plot(x[:,0], x[:,1], 'rx', markersize=10, markeredgewidth=3, label='training data points')\n",
    "plt.scatter(x[:,0], x[:,1], c='r', marker='x', s=75, label='training data points')\n",
    "plt.xlabel('$x_0$', fontsize=20)\n",
    "plt.ylabel('$x_1$', fontsize=20)\n",
    "plt.axis([-2, 12, -2, 8])\n",
    "\n",
    "# display location of 2 new observations we will use to demonstrate\n",
    "# calculating the density or probability estimate\n",
    "x_new = np.array([[5.5, 2.9],\n",
    "                  [2.5, 0.8]])\n",
    "#plt.plot(x_new[:,0], x_new[:,1], 'bo', markersize=10, markeredgewidth=3, label='new observations')\n",
    "plt.scatter(x_new[:,0], x_new[:,1], c='b', marker='o', s=75, label='new observations')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# and create a mesh to detect decision boundary for a given epsilon and plot it\n",
    "epsilon = 0.005\n",
    "x_min, x_max = -2.0, 12.0\n",
    "y_min, y_max = -2.0, 8.0\n",
    "h = .01  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "gridpoints = np.c_[xx.ravel(), yy.ravel()]\n",
    "p = scipy.stats.norm.pdf(gridpoints, loc=mu_estimate, scale=sigma_estimate)\n",
    "p = np.prod(p, axis=1)\n",
    "Z = p.reshape(xx.shape)\n",
    "levels = np.arange(0.0, 0.11, 0.005)\n",
    "plt.contourf(xx, yy, Z, levels, cmap=plt.cm.RdYlBu_r, alpha=0.4);\n",
    "plt.colorbar();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W9 04: Developing and Evaluating an Anomaly Detection System\n",
    "\n",
    "[YouTube Video Link]()\n",
    "\n",
    "To evaluate an anomaly detection system, we might get some labeled data.  Often this training set is\n",
    "a bit skewed, we will often have a large colleciton of nonanomalous normal examples, with relatively few\n",
    "anomalous examples.\n",
    "\n",
    "\n",
    "## Training Example\n",
    "\n",
    "Say we have data with 10000 good (normal) examples, and 20 flawed (anomalous) examples.\n",
    "\n",
    "- Training set: use 6000 good examples ($y = 0$)\n",
    "- CV: 2000 good examples ($y = 0$), 10 anomalous ($y = 1$)\n",
    "- Test: 2000 good examples ($y = 0$), 10 anomalous ($y = 1$)\n",
    "\n",
    "## Algorithm Evaluation\n",
    "\n",
    "- Fit model $p(x)$ on training set $\\{x^{(1)}, \\cdots, x^{(m)}  \\}$\n",
    "- On cross validation/test example $x$, predict\n",
    "\n",
    "\\begin{equation}\n",
    "y = \n",
    "\\begin{cases}\n",
    "0 & p(x) < \\epsilon \\text{anomaly}\\\\\n",
    "1 & p(x) \\ge \\epsilon \\text{normal}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Because the data is very skewed, classification accuracy would not be a very good metric to use\n",
    "to evaluate your anomaly detector.  By predicting all items are not anomalous, you will get\n",
    "pretty good accuracy if there are very few anomalous items.\n",
    "\n",
    "Possible evaluation metrics:\n",
    "- True postivie, false positive, false negative, true negative\n",
    "- Precision/Recall\n",
    "- $F_1$ -score\n",
    "\n",
    "Can also use cross validation set to choose parameter $\\epsilon$.\n",
    "\n",
    "Lets give an example of how to compute these metrics.  First of all, we will make up some\n",
    "data again with 2 features.  I will generate anomalies by randomly choosing some values that are\n",
    "below a threshold, and removing them from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:48.415322Z",
     "start_time": "2019-11-19T13:53:48.406536Z"
    }
   },
   "outputs": [],
   "source": [
    "# same method as before, but generate 10020 points so we can pull out 20 anomalous points.\n",
    "np.random.seed(42) # set seed so example is reproducable\n",
    "m, n = 10020, 2 # number of example data points ; number of features/dimensions\n",
    "mu_actual = np.array([5.0, 3.0])\n",
    "sigma_actual = np.array([2.0, 1.0])\n",
    "\n",
    "# we will generate a mxn array of values with mean 0 and std 1.0\n",
    "x = np.random.normal(loc=0.0, scale=1.0, size=(m, n))\n",
    "\n",
    "# and we will scale each dimension to have the desired mean/std.\n",
    "x[:,0] = (x[:,0] * sigma_actual[0]) + mu_actual[0]\n",
    "x[:,1] = (x[:,1] * sigma_actual[1]) + mu_actual[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:48.567815Z",
     "start_time": "2019-11-19T13:53:48.418449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.00686592 3.00814818] [2.02279433 0.99329815]\n",
      "2.53267379519127e-06 0.07920067447773657\n",
      "0.0003677434885608923\n",
      "50\n",
      "[ 104  131  580  978 1152 1447 1727 1858 1991 2319 2389 2425 2435 2688\n",
      " 3445 3720 3805 3838 4082 4100 4124 4885 5367 5383 5544 5590 5669 5713\n",
      " 5821 6122 6354 6456 6612 6722 6973 7290 7327 7581 7921 8690 8718 8873\n",
      " 9059 9107 9407 9425 9515 9745 9837 9926]\n",
      "[4885 5383 2319  131 5713 8718 7327 3805 1447 9059 1152 7921 1727 6456\n",
      "  978 4124 8873 6456 3838 6612]\n",
      "(20, 2)\n",
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "# need to calculate the density estimate, so we can choose some values at random to be the anomalies\n",
    "# get parameter estimates\n",
    "mu_estimate = np.mean(x, axis=0)\n",
    "sigma_estimate = np.std(x, axis=0)\n",
    "print(mu_estimate, sigma_estimate)\n",
    "\n",
    "# compute density estimate\n",
    "p = scipy.stats.norm.pdf(x, loc=mu_estimate, scale=sigma_estimate)\n",
    "p = np.prod(p, axis=1)\n",
    "print(p.min(), p.max())\n",
    "\n",
    "# determine the value for the 50 lowest scores, so we can randomly choose 20 of these 50 as anomalies\n",
    "p_sorted = np.copy(p)\n",
    "p_sorted.sort()\n",
    "candidate_cutoff = p_sorted[50]\n",
    "print(candidate_cutoff)\n",
    "print(np.sum(p < candidate_cutoff))\n",
    "\n",
    "# find indexes of the 50 candidates in original data\n",
    "candidate_indexes = np.where(p < candidate_cutoff)[0]\n",
    "print(candidate_indexes)\n",
    "\n",
    "# randomly choose 20 of the candidate indexes to be anomalies\n",
    "anomaly_indexes = np.random.choice(candidate_indexes, 20)\n",
    "print(anomaly_indexes)\n",
    "\n",
    "# extract these items for anomalies, and remove them from the training set x\n",
    "x_anomalies = np.copy(x[anomaly_indexes])\n",
    "print(x_anomalies.shape)\n",
    "\n",
    "# bug? not sure why we end up with 10001 here, but none of the items\n",
    "# removed appear to be in x so?\n",
    "x = np.delete(x, anomaly_indexes, axis=0)\n",
    "x = x[:10000,:]\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:49.132210Z",
     "start_time": "2019-11-19T13:53:48.568924Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the points and anomalies\n",
    "plt.plot(x[:,0], x[:,1], 'bo', alpha=0.1);\n",
    "plt.plot(x_anomalies[:,0], x_anomalies[:,1], 'ro', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:49.137901Z",
     "start_time": "2019-11-19T13:53:49.133809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2) (2000, 2) (2000, 2)\n",
      "(10, 2) (10, 2)\n"
     ]
    }
   ],
   "source": [
    "# we will split into train, cv and test sets as shown in the video\n",
    "x_train = x[:6000, :]\n",
    "x_cv = x[6000:8000, :]\n",
    "x_test = x[8000:, :]\n",
    "print(x_train.shape, x_cv.shape, x_test.shape)\n",
    "\n",
    "x_anomalies_cv = x_anomalies[:10, :]\n",
    "x_anomalies_test = x_anomalies[10:, :]\n",
    "print(x_anomalies_cv.shape, x_anomalies_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:49.219746Z",
     "start_time": "2019-11-19T13:53:49.140025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2010, 2) (2010, 2)\n"
     ]
    }
   ],
   "source": [
    "# put the anomalies into the cv and test, and create a y labels for each\n",
    "x_cv = np.concatenate((x_cv, x_anomalies_cv), axis=0)\n",
    "x_test = np.concatenate((x_test, x_anomalies_test), axis=0)\n",
    "print(x_cv.shape, x_test.shape)\n",
    "\n",
    "# create y values, to make it easier to calculate evaluation metrics\n",
    "# anomalies are \"positive\" or 1 cases, and normal are \"negative\" or 0 cases \n",
    "m, n = x_cv.shape\n",
    "y_cv = np.zeros(m)\n",
    "y_cv[2000:] = 1\n",
    "\n",
    "m, n = x_test.shape\n",
    "y_test = np.zeros(m)\n",
    "y_test[2000:] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:49.345753Z",
     "start_time": "2019-11-19T13:53:49.221227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.730093667765102e-05 0.07919897520096435\n"
     ]
    }
   ],
   "source": [
    "# retrain the density estimation only on the \"normal\" 6000 training items\n",
    "p = scipy.stats.norm.pdf(x_train, loc=mu_estimate, scale=sigma_estimate)\n",
    "p = np.prod(p, axis=1)\n",
    "print(p.min(), p.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the previous was just to create a random data set with 10000 normal items and 20 anomalies.\n",
    "We have also created our density estimates using a 6000 test subset of the data.\n",
    "\n",
    "### True/False Positive/Negative rates\n",
    "\n",
    "[reference](https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative)\n",
    "\n",
    "Lets first look at calculating true/false positives and negatives as an evaluation method.  Lets try\n",
    "to evalute using the method and the cross validation test set to try and set the optimal value\n",
    "for epsilon $\\epsilon$, which is our threshold for the anomaly detector.\n",
    "\n",
    "The true vs. false and positive vs. negative is basically like a confusion matrix, but for a binary \n",
    "classifier there are only 2 output classes.  So for problems like this dataset, we expect to get a lot\n",
    "of true negatives (because we consider normal or no anomaly as negative examples with $y = 0$) here.\n",
    "So our interest is in the true positives we get, and the number of false negatives and positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:49.544858Z",
     "start_time": "2019-11-19T13:53:49.346977Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:49.612469Z",
     "start_time": "2019-11-19T13:53:49.546903Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_tfpn_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute True/False Positive/Negative rates\n",
    "    This could be vectorized, but lets be explicit to make sure we understand\n",
    "    what we are calculating.\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]\n",
    "    tp = 0 # true positives\n",
    "    tn = 0 # true negatives\n",
    "    fp = 0 # false positives\n",
    "    fn = 0 # false negatives\n",
    "    for i in range(m):\n",
    "        # a true positive is when y is positive and we predict positive\n",
    "        if y_true[i] == 1 and y_pred[i] == 1:\n",
    "            tp += 1\n",
    "        \n",
    "        # a true negative is when y is negative and we predict negative\n",
    "        if y_true[i] == 0 and y_pred[i] == 0:\n",
    "            tn += 1\n",
    "\n",
    "        # a false positive is when y is negative but we predict positive\n",
    "        if y_true[i] == 0 and y_pred[i] == 1:\n",
    "            fp += 1\n",
    "\n",
    "        # a false negative is when y is positive but we predict negative\n",
    "        if y_true[i] == 1 and y_pred[i] == 0:\n",
    "            fn += 1\n",
    "        \n",
    "    #print(\"True Positives : %5d   False Positives: %5d\" % (tp, fp))\n",
    "    #print(\"False Negatives: %5d   True Negatives:  %5d\" % (fn, tn))\n",
    "    return tp, fp, fn, tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:49.726318Z",
     "start_time": "2019-11-19T13:53:49.614069Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_tfpn_matrix(y_true, y_pred):\n",
    "    \"\"\"Visualize the true/false positive/negatives confusion matrix.\n",
    "    Here we use an actual confusin_matrix() function from sklearn.metrics\n",
    "    But to display in the standard way, we have to completely flip\n",
    "    the matrix around to get into the normal way one displays a the tfpn\n",
    "    confusion matrix.\n",
    "    \"\"\"\n",
    "    c = confusion_matrix(y_true, y_predicted)\n",
    "    c = np.fliplr(c)\n",
    "    c = np.flipud(c)\n",
    "    c = c.T\n",
    "    sns.heatmap(c, annot=True, cbar=False, fmt='0.0f', cmap=plt.cm.Blues, annot_kws={\"size\": 16}, linewidths=4);\n",
    "    plt.xlabel('True Label', fontsize=16)\n",
    "    plt.ylabel('Predicted Label', fontsize=16)\n",
    "    plt.xticks([0.5, 1.5], labels=['Positive', 'Negative'])\n",
    "    plt.yticks([0.5, 1.5], labels=['Positive', 'Negative'])\n",
    "    plt.text(0.3, 0.4, 'True Positive (TP)', fontsize=16);\n",
    "    plt.text(1.3, 0.4, 'False Positive (FP)', fontsize=16);\n",
    "    plt.text(0.3, 1.4, 'False Negative (FN)', fontsize=16);\n",
    "    plt.text(1.3, 1.4, 'True Negative (TN)', color='white', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:49.884244Z",
     "start_time": "2019-11-19T13:53:49.729115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3566191452835629e-05 0.07920067447773657\n"
     ]
    }
   ],
   "source": [
    "# we compute probability on the cv data using the mu_estimate and sigma_estimate\n",
    "# from the training set calculated\n",
    "# previously\n",
    "p = scipy.stats.norm.pdf(x_cv, loc=mu_estimate, scale=sigma_estimate)\n",
    "p = np.prod(p, axis=1)\n",
    "print(p.min(), p.max())\n",
    "\n",
    "# as an example, choose an epsilon and calculate the true/false positive/negatives\n",
    "epsilon = 1e-04\n",
    "y_predicted = (p < epsilon) * 1.0 # multiplying by 1 turns the boolean results into 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:50.140089Z",
     "start_time": "2019-11-19T13:53:49.885485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives :     4   False Positives:     1\n",
      "False Negatives:     6   True Negatives:   1999\n"
     ]
    }
   ],
   "source": [
    "tp, fp, fn, tn = compute_tfpn_metrics(y_cv, y_predicted)\n",
    "print(\"True Positives : %5d   False Positives: %5d\" % (tp, fp))\n",
    "print(\"False Negatives: %5d   True Negatives:  %5d\" % (fn, tn))\n",
    "\n",
    "plot_tfpn_matrix(y_cv, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we ended up only getting 4 of the anomalies correct of the 10, and we\n",
    "misclassified one of the normal items as an anomaly.  \n",
    "\n",
    "Tuning this kind of threshold represents a tradeoff.  Is it worse to let 1 true anomaly get through\n",
    "the system (false negatives here)?  Or are false positives to costly to deal with (e.g. because\n",
    "we are annoying customers too much who aren't commiting fraud, or we are pulling too many\n",
    "engines for inspection that are actually ok).\n",
    "\n",
    "We can try a range of epsilon values, and see the FN/FP ratios that result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:50.196327Z",
     "start_time": "2019-11-19T13:53:50.141385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing epsilon 0.00000100:\n",
      "True Positives :     0   False Positives:     0\n",
      "False Negatives:    10   True Negatives:   2000\n",
      "\n",
      "\n",
      "Testing epsilon 0.00000359:\n",
      "True Positives :     0   False Positives:     0\n",
      "False Negatives:    10   True Negatives:   2000\n",
      "\n",
      "\n",
      "Testing epsilon 0.00001292:\n",
      "True Positives :     0   False Positives:     0\n",
      "False Negatives:    10   True Negatives:   2000\n",
      "\n",
      "\n",
      "Testing epsilon 0.00004642:\n",
      "True Positives :     2   False Positives:     1\n",
      "False Negatives:     8   True Negatives:   1999\n",
      "\n",
      "\n",
      "Testing epsilon 0.00016681:\n",
      "True Positives :     4   False Positives:     1\n",
      "False Negatives:     6   True Negatives:   1999\n",
      "\n",
      "\n",
      "Testing epsilon 0.00059948:\n",
      "True Positives :    10   False Positives:    12\n",
      "False Negatives:     0   True Negatives:   1988\n",
      "\n",
      "\n",
      "Testing epsilon 0.00215443:\n",
      "True Positives :    10   False Positives:    48\n",
      "False Negatives:     0   True Negatives:   1952\n",
      "\n",
      "\n",
      "Testing epsilon 0.00774264:\n",
      "True Positives :    10   False Positives:   201\n",
      "False Negatives:     0   True Negatives:   1799\n",
      "\n",
      "\n",
      "Testing epsilon 0.02782559:\n",
      "True Positives :    10   False Positives:   716\n",
      "False Negatives:     0   True Negatives:   1284\n",
      "\n",
      "\n",
      "Testing epsilon 0.10000000:\n",
      "True Positives :    10   False Positives:  2000\n",
      "False Negatives:     0   True Negatives:      0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first try\n",
    "for epsilon in np.logspace(-6, -1, 10):\n",
    "    print('Testing epsilon %0.8f:' % epsilon)\n",
    "    y_predicted = (p < epsilon) * 1.0 # multiplying by 1 turns the boolean results into 1 or 0\n",
    "    tp, fp, fn, tn = compute_tfpn_metrics(y_cv, y_predicted)\n",
    "    print(\"True Positives : %5d   False Positives: %5d\" % (tp, fp))\n",
    "    print(\"False Negatives: %5d   True Negatives:  %5d\" % (fn, tn))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:50.361040Z",
     "start_time": "2019-11-19T13:53:50.197508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing epsilon 0.00004000:\n",
      "True Positives :     1   False Positives:     1\n",
      "False Negatives:     9   True Negatives:   1999\n",
      "\n",
      "\n",
      "Testing epsilon 0.00006421:\n",
      "True Positives :     3   False Positives:     1\n",
      "False Negatives:     7   True Negatives:   1999\n",
      "\n",
      "\n",
      "Testing epsilon 0.00008842:\n",
      "True Positives :     4   False Positives:     1\n",
      "False Negatives:     6   True Negatives:   1999\n",
      "\n",
      "\n",
      "Testing epsilon 0.00011263:\n",
      "True Positives :     4   False Positives:     1\n",
      "False Negatives:     6   True Negatives:   1999\n",
      "\n",
      "\n",
      "Testing epsilon 0.00013684:\n",
      "True Positives :     4   False Positives:     1\n",
      "False Negatives:     6   True Negatives:   1999\n",
      "\n",
      "\n",
      "Testing epsilon 0.00016105:\n",
      "True Positives :     4   False Positives:     1\n",
      "False Negatives:     6   True Negatives:   1999\n",
      "\n",
      "\n",
      "Testing epsilon 0.00018526:\n",
      "True Positives :     6   False Positives:     1\n",
      "False Negatives:     4   True Negatives:   1999\n",
      "\n",
      "\n",
      "Testing epsilon 0.00020947:\n",
      "True Positives :     7   False Positives:     1\n",
      "False Negatives:     3   True Negatives:   1999\n",
      "\n",
      "\n",
      "Testing epsilon 0.00023368:\n",
      "True Positives :     7   False Positives:     2\n",
      "False Negatives:     3   True Negatives:   1998\n",
      "\n",
      "\n",
      "Testing epsilon 0.00025789:\n",
      "True Positives :     7   False Positives:     2\n",
      "False Negatives:     3   True Negatives:   1998\n",
      "\n",
      "\n",
      "Testing epsilon 0.00028211:\n",
      "True Positives :     9   False Positives:     3\n",
      "False Negatives:     1   True Negatives:   1997\n",
      "\n",
      "\n",
      "Testing epsilon 0.00030632:\n",
      "True Positives :    10   False Positives:     5\n",
      "False Negatives:     0   True Negatives:   1995\n",
      "\n",
      "\n",
      "Testing epsilon 0.00033053:\n",
      "True Positives :    10   False Positives:     6\n",
      "False Negatives:     0   True Negatives:   1994\n",
      "\n",
      "\n",
      "Testing epsilon 0.00035474:\n",
      "True Positives :    10   False Positives:     6\n",
      "False Negatives:     0   True Negatives:   1994\n",
      "\n",
      "\n",
      "Testing epsilon 0.00037895:\n",
      "True Positives :    10   False Positives:     7\n",
      "False Negatives:     0   True Negatives:   1993\n",
      "\n",
      "\n",
      "Testing epsilon 0.00040316:\n",
      "True Positives :    10   False Positives:     8\n",
      "False Negatives:     0   True Negatives:   1992\n",
      "\n",
      "\n",
      "Testing epsilon 0.00042737:\n",
      "True Positives :    10   False Positives:    10\n",
      "False Negatives:     0   True Negatives:   1990\n",
      "\n",
      "\n",
      "Testing epsilon 0.00045158:\n",
      "True Positives :    10   False Positives:    10\n",
      "False Negatives:     0   True Negatives:   1990\n",
      "\n",
      "\n",
      "Testing epsilon 0.00047579:\n",
      "True Positives :    10   False Positives:    10\n",
      "False Negatives:     0   True Negatives:   1990\n",
      "\n",
      "\n",
      "Testing epsilon 0.00050000:\n",
      "True Positives :    10   False Positives:    10\n",
      "False Negatives:     0   True Negatives:   1990\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we really want something between 0.00004 and 0.00050\n",
    "# second try\n",
    "for epsilon in np.linspace(0.00004, 0.0005, 20):\n",
    "    print('Testing epsilon %0.8f:' % epsilon)\n",
    "    y_predicted = (p < epsilon) * 1.0 # multiplying by 1 turns the boolean results into 1 or 0\n",
    "    tp, fp, fn, tn = compute_tfpn_metrics(y_cv, y_predicted)\n",
    "    print(\"True Positives : %5d   False Positives: %5d\" % (tp, fp))\n",
    "    print(\"False Negatives: %5d   True Negatives:  %5d\" % (fn, tn))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we perhaps want something like $\\epsilon = 0.00028211$ where we only caught almost all of\n",
    "the anomalies while only have 3 false positives.  Lets see how that setting of epsilon works\n",
    "for the held back final test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:50.372908Z",
     "start_time": "2019-11-19T13:53:50.362100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.53267379519127e-06 0.07916662594378182\n",
      "True Positives :     7   False Positives:     7\n",
      "False Negatives:     3   True Negatives:   1993\n"
     ]
    }
   ],
   "source": [
    "# calculate density on the test set now\n",
    "p = scipy.stats.norm.pdf(x_test, loc=mu_estimate, scale=sigma_estimate)\n",
    "p = np.prod(p, axis=1)\n",
    "print(p.min(), p.max())\n",
    "\n",
    "# and check the fn/fp ratio\n",
    "epsilon = 0.00028211\n",
    "y_predicted = (p < epsilon) * 1.0 # multiplying by 1 turns the boolean results into 1 or 0\n",
    "tp, fp, fn, tn = compute_tfpn_metrics(y_cv, y_predicted)\n",
    "print(\"True Positives : %5d   False Positives: %5d\" % (tp, fp))\n",
    "print(\"False Negatives: %5d   True Negatives:  %5d\" % (fn, tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "[reference](https://towardsdatascience.com/precision-vs-recall-386cf9f89488)\n",
    "\n",
    "Calculating precision and recall is just taking the true/false positive/negatives and calculating\n",
    "some actual ratios.  We can calculate Precision, Recall and Accuracy\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Precision} = \\frac{\\text{True Positive (TP)}}{\\text{Actual Results}} = \\frac{\\text{True Positive (TP)}}{\\text{True Positive (TP) + False Positive (FP)}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Recall} = \\frac{\\text{True Positive (TP)}}{\\text{Predicted Results}} = \\frac{\\text{True Positive (TP)}}{\\text{True Positive (TP) + False Negative (FN)}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{\\text{True Positive (TP)} + \\text{True Negative (TN)}}{\\text{Total}} \n",
    "\\end{equation}\n",
    "\n",
    "Often, we think that precision and recall both indicate accuracy of the model. While that is somewhat true, there is a deeper, distinct meaning of each of these terms. Precision means the percentage of your results which are relevant. On the other hand, recall refers to the percentage of total relevant results correctly classified by your algorithm.\n",
    "\n",
    "Lets redo the last epsilon meta search, but calculate precision and recall scores instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:50.505441Z",
     "start_time": "2019-11-19T13:53:50.374055Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_precision_recall(y_true, y_predicted):\n",
    "    tp, fp, fn, tn = compute_tfpn_metrics(y_true, y_predicted)\n",
    "    precision = float(tp) / float(tp + fp)\n",
    "    recall = float(tp) / float(tp + fn)\n",
    "    m = y_true.shape[0]\n",
    "    accuracy = float(tp + tn) / float(m)\n",
    "    return precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:50.701640Z",
     "start_time": "2019-11-19T13:53:50.507412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3566191452835629e-05 0.07920067447773657\n",
      "Testing epsilon 0.00004000:\n",
      "precision: 0.50000, recall: 0.10000, accuracy: 0.99502\n",
      "\n",
      "\n",
      "Testing epsilon 0.00006421:\n",
      "precision: 0.75000, recall: 0.30000, accuracy: 0.99602\n",
      "\n",
      "\n",
      "Testing epsilon 0.00008842:\n",
      "precision: 0.80000, recall: 0.40000, accuracy: 0.99652\n",
      "\n",
      "\n",
      "Testing epsilon 0.00011263:\n",
      "precision: 0.80000, recall: 0.40000, accuracy: 0.99652\n",
      "\n",
      "\n",
      "Testing epsilon 0.00013684:\n",
      "precision: 0.80000, recall: 0.40000, accuracy: 0.99652\n",
      "\n",
      "\n",
      "Testing epsilon 0.00016105:\n",
      "precision: 0.80000, recall: 0.40000, accuracy: 0.99652\n",
      "\n",
      "\n",
      "Testing epsilon 0.00018526:\n",
      "precision: 0.85714, recall: 0.60000, accuracy: 0.99751\n",
      "\n",
      "\n",
      "Testing epsilon 0.00020947:\n",
      "precision: 0.87500, recall: 0.70000, accuracy: 0.99801\n",
      "\n",
      "\n",
      "Testing epsilon 0.00023368:\n",
      "precision: 0.77778, recall: 0.70000, accuracy: 0.99751\n",
      "\n",
      "\n",
      "Testing epsilon 0.00025789:\n",
      "precision: 0.77778, recall: 0.70000, accuracy: 0.99751\n",
      "\n",
      "\n",
      "Testing epsilon 0.00028211:\n",
      "precision: 0.75000, recall: 0.90000, accuracy: 0.99801\n",
      "\n",
      "\n",
      "Testing epsilon 0.00030632:\n",
      "precision: 0.66667, recall: 1.00000, accuracy: 0.99751\n",
      "\n",
      "\n",
      "Testing epsilon 0.00033053:\n",
      "precision: 0.62500, recall: 1.00000, accuracy: 0.99701\n",
      "\n",
      "\n",
      "Testing epsilon 0.00035474:\n",
      "precision: 0.62500, recall: 1.00000, accuracy: 0.99701\n",
      "\n",
      "\n",
      "Testing epsilon 0.00037895:\n",
      "precision: 0.58824, recall: 1.00000, accuracy: 0.99652\n",
      "\n",
      "\n",
      "Testing epsilon 0.00040316:\n",
      "precision: 0.55556, recall: 1.00000, accuracy: 0.99602\n",
      "\n",
      "\n",
      "Testing epsilon 0.00042737:\n",
      "precision: 0.50000, recall: 1.00000, accuracy: 0.99502\n",
      "\n",
      "\n",
      "Testing epsilon 0.00045158:\n",
      "precision: 0.50000, recall: 1.00000, accuracy: 0.99502\n",
      "\n",
      "\n",
      "Testing epsilon 0.00047579:\n",
      "precision: 0.50000, recall: 1.00000, accuracy: 0.99502\n",
      "\n",
      "\n",
      "Testing epsilon 0.00050000:\n",
      "precision: 0.50000, recall: 1.00000, accuracy: 0.99502\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute density on cv set\n",
    "p = scipy.stats.norm.pdf(x_cv, loc=mu_estimate, scale=sigma_estimate)\n",
    "p = np.prod(p, axis=1)\n",
    "print(p.min(), p.max())\n",
    "\n",
    "# we really want something between 0.00004 and 0.00050\n",
    "# second try\n",
    "for epsilon in np.linspace(0.00004, 0.0005, 20):\n",
    "    print('Testing epsilon %0.8f:' % epsilon)\n",
    "    y_predicted = (p < epsilon) * 1.0 # multiplying by 1 turns the boolean results into 1 or 0\n",
    "    precision, recall, accuracy = compute_precision_recall(y_cv, y_predicted)\n",
    "    print('precision: %0.5f, recall: %0.5f, accuracy: %0.5f' % (precision, recall, accuracy))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:51.439801Z",
     "start_time": "2019-11-19T13:53:50.702756Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\e'\n",
      "/tmp/ipykernel_8508/2925431399.py:17: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  plt.xlabel('epsilon $\\epsilon$')\n"
     ]
    }
   ],
   "source": [
    "# it can also be useful to plot recall and precision as functions of the parameter (epsilon).\n",
    "# basically the location they cross represents the most balanced trade off you can achieve\n",
    "\n",
    "# we really want something between 0.00004 and 0.00050\n",
    "# second try\n",
    "NUM_TESTS = 100\n",
    "epsilons = np.linspace(0.00004, 0.0005, NUM_TESTS)\n",
    "precision = np.empty(NUM_TESTS)\n",
    "recall = np.empty(NUM_TESTS)\n",
    "accuracy = np.empty(NUM_TESTS)\n",
    "for i, epsilon in enumerate(epsilons):\n",
    "    y_predicted = (p < epsilon) * 1.0 # multiplying by 1 turns the boolean results into 1 or 0\n",
    "    precision[i], recall[i], accuracy[i] = compute_precision_recall(y_cv, y_predicted)\n",
    "\n",
    "plt.plot(epsilons, precision, label='Precision')\n",
    "plt.plot(epsilons, recall, label='Recall')\n",
    "plt.xlabel('epsilon $\\epsilon$')\n",
    "plt.ylabel('score')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $F_1$ score\n",
    "\n",
    "In most problems you could either give a higher priority to maximizing precision, or recall, depending\n",
    "on the problem.  The $F_1$ score is a simpler metric which takes both into account.  The $F_1$ score\n",
    "is simply the harmonic mean of precision and recall:\n",
    "\n",
    "\\begin{equation}\n",
    "F_1 = 2 \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:51.443826Z",
     "start_time": "2019-11-19T13:53:51.441104Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_f1_score(y_true, y_predicted):\n",
    "    precision, recall, accuracy = compute_precision_recall(y_true, y_predicted)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:53:51.631968Z",
     "start_time": "2019-11-19T13:53:51.445037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3566191452835629e-05 0.07920067447773657\n",
      "Testing epsilon 0.00004000:\n",
      "f1: 0.16667\n",
      "\n",
      "\n",
      "Testing epsilon 0.00006421:\n",
      "f1: 0.42857\n",
      "\n",
      "\n",
      "Testing epsilon 0.00008842:\n",
      "f1: 0.53333\n",
      "\n",
      "\n",
      "Testing epsilon 0.00011263:\n",
      "f1: 0.53333\n",
      "\n",
      "\n",
      "Testing epsilon 0.00013684:\n",
      "f1: 0.53333\n",
      "\n",
      "\n",
      "Testing epsilon 0.00016105:\n",
      "f1: 0.53333\n",
      "\n",
      "\n",
      "Testing epsilon 0.00018526:\n",
      "f1: 0.70588\n",
      "\n",
      "\n",
      "Testing epsilon 0.00020947:\n",
      "f1: 0.77778\n",
      "\n",
      "\n",
      "Testing epsilon 0.00023368:\n",
      "f1: 0.73684\n",
      "\n",
      "\n",
      "Testing epsilon 0.00025789:\n",
      "f1: 0.73684\n",
      "\n",
      "\n",
      "Testing epsilon 0.00028211:\n",
      "f1: 0.81818\n",
      "\n",
      "\n",
      "Testing epsilon 0.00030632:\n",
      "f1: 0.80000\n",
      "\n",
      "\n",
      "Testing epsilon 0.00033053:\n",
      "f1: 0.76923\n",
      "\n",
      "\n",
      "Testing epsilon 0.00035474:\n",
      "f1: 0.76923\n",
      "\n",
      "\n",
      "Testing epsilon 0.00037895:\n",
      "f1: 0.74074\n",
      "\n",
      "\n",
      "Testing epsilon 0.00040316:\n",
      "f1: 0.71429\n",
      "\n",
      "\n",
      "Testing epsilon 0.00042737:\n",
      "f1: 0.66667\n",
      "\n",
      "\n",
      "Testing epsilon 0.00045158:\n",
      "f1: 0.66667\n",
      "\n",
      "\n",
      "Testing epsilon 0.00047579:\n",
      "f1: 0.66667\n",
      "\n",
      "\n",
      "Testing epsilon 0.00050000:\n",
      "f1: 0.66667\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute density on cv set\n",
    "p = scipy.stats.norm.pdf(x_cv, loc=mu_estimate, scale=sigma_estimate)\n",
    "p = np.prod(p, axis=1)\n",
    "print(p.min(), p.max())\n",
    "\n",
    "# we really want something between 0.00004 and 0.00050\n",
    "# second try\n",
    "for epsilon in np.linspace(0.00004, 0.0005, 20):\n",
    "    print('Testing epsilon %0.8f:' % epsilon)\n",
    "    y_predicted = (p < epsilon) * 1.0 # multiplying by 1 turns the boolean results into 1 or 0\n",
    "    f1 = compute_f1_score(y_cv, y_predicted)\n",
    "    print('f1: %0.5f' % (f1))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\e'\n",
      "/tmp/ipykernel_8508/1752542942.py:20: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  plt.xlabel('epsilon $\\epsilon$')\n"
     ]
    }
   ],
   "source": [
    "# it can also be useful to plot recall and precision as functions of the parameter (epsilon).\n",
    "# basically the location they cross represents the most balanced trade off you can achieve\n",
    "\n",
    "# we really want something between 0.00004 and 0.00050\n",
    "# second try\n",
    "NUM_TESTS = 100\n",
    "epsilons = np.linspace(0.00004, 0.0005, NUM_TESTS)\n",
    "precision = np.empty(NUM_TESTS)\n",
    "recall = np.empty(NUM_TESTS)\n",
    "accuracy = np.empty(NUM_TESTS)\n",
    "f1 = np.empty(NUM_TESTS)\n",
    "for i, epsilon in enumerate(epsilons):\n",
    "    y_predicted = (p < epsilon) * 1.0 # multiplying by 1 turns the boolean results into 1 or 0\n",
    "    precision[i], recall[i], accuracy[i] = compute_precision_recall(y_cv, y_predicted)\n",
    "    f1[i] = compute_f1_score(y_cv, y_predicted)\n",
    "    \n",
    "plt.plot(epsilons, precision, label='Precision')\n",
    "plt.plot(epsilons, recall, label='Recall')\n",
    "plt.plot(epsilons, f1, label='$F_1$')\n",
    "plt.xlabel('epsilon $\\epsilon$')\n",
    "plt.ylabel('score')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W9 05: Anomaly Detection vs. Supervised Learning\n",
    "\n",
    "[YouTube Video Link]()\n",
    "\n",
    "\n",
    "Why don't we just use a supervised learning algorithm instead of the anomaly detection if we have\n",
    "some examples of anomalous values.\n",
    "\n",
    "**Anomaly Detection**\n",
    "\n",
    "- very small number of positive examples ($y = 1$).  0-20 is common\n",
    "- Large number of negative ($y = 0$) examples.\n",
    "- Many different \"types\" of anomalies.  Hard for any algorithm to learn from positive examples what the\n",
    "  anomalies look like; future anomalies may look nothing like any of the anomalous examples we've seen\n",
    "  so far.\n",
    "\n",
    "**Supervised Learning**\n",
    "\n",
    "- Normally have a reasonalbly large number of positive and negative examples.\n",
    "- Enough positive examples for algorithm to get a sense of what positive examples are like, future\n",
    "  positive examples likely to be similar to ones in training set.\n",
    "  \n",
    "Key difference is usually in anomaly detection we have such a small number of positive examples that it\n",
    "is not possible for a supervised learning method to learn that much from the positive examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W9 06: Choosing what Features to Use\n",
    "\n",
    "[YouTube Video Link]()\n",
    "\n",
    "- Non-gaussian features may be problematic, can we transform it to be more \"gaussian\" looking\n",
    "  - $\\log(x)$ transform\n",
    "  - or $\\log(x + 1)$ or $\\log(x + c)$\n",
    "  - $\\sqrt{x}$ or $\\sqrt[4]{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W9 07: Multivariate Gaussian Distribution\n",
    "\n",
    "[YouTube Video Link]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W9 08: Anomaly Detection using Multivariate Gaussian Distribution\n",
    "\n",
    "[YouTube Video Link]()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
