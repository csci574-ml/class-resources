{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:34.808206Z",
     "start_time": "2019-11-04T19:00:34.223304Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:34.812684Z",
     "start_time": "2019-11-04T19:00:34.809604Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (10, 8) # set default figure size, 10in by 8in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, you will be learning about unsupervised learning. While supervised learning algorithms need labeled examples (x,y), unsupervised learning algorithms need only the input (x). You will learn about clusteringâ€”which is used for market segmentation, text summarization, among many other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W8 01: Unsupervised Learning\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=jLzihAZ0YQE&list=PLiPvV5TNogxIS4bHQVW4pMkj4CHA8COdX&index=91)\n",
    "\n",
    "In an unsupervised learning problem, we are given data that does not have any labels associated with it.  So what we want\n",
    "from unsupervised learning algorithms is to discover some sort of structure or organization or pattern in our data.  For example,\n",
    "the easiest type of structure to understand is to try and find clusters in the data of items that appear related.  Such clusters\n",
    "can be useful in many applications to identify and process the members of a cluster in some specific way, such as clusters of\n",
    "different types customers and their buying habits.\n",
    "\n",
    "Up to this point we have been studying supervised learning methods.  In supervised learning, for example to\n",
    "perform a classification task, we are given a traing set of data, and all of the $m$ samples in the training set are\n",
    "labeled:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Training set:} \\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), (x^{(3)}, y^{(3)}), \\ldots, (x^{(m)}, y^{(m)}) \\} \n",
    "\\end{equation}\n",
    "\n",
    "Here the $y^{(m)}$ are the labels for the data.  For example if we have the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:35.329905Z",
     "start_time": "2019-11-04T19:00:34.815401Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[0.5, 0.5],\n",
    "              [1.0, 0.5],\n",
    "              [0.75, 1.0],\n",
    "              [1.9, 0.25],\n",
    "              [1.6, 0.75],\n",
    "              [1.25, 1.25],\n",
    "              [0.5, 1.6],\n",
    "              [0.5, 2.25],\n",
    "              [3.1, 1.1],\n",
    "              [2.9, 1.5],\n",
    "              [2.1, 2.1],\n",
    "              [2.1, 2.75],\n",
    "              [1.5, 3.1],\n",
    "              [3.5, 1.9],\n",
    "              [3.0, 2.1],\n",
    "              [3.0, 3.0],\n",
    "              [2.0, 3.5],\n",
    "              [2.5, 3.5]])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "neg_idx = np.where(y == 0)\n",
    "pos_idx = np.where(y == 1)\n",
    "\n",
    "# plot the example figure\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# plot the points in our two categories, y=0 and y=1, using markers to indicated\n",
    "# the category or output\n",
    "neg_handle = plt.plot(x[neg_idx,0], x[neg_idx,1], 'bo', markersize=8, fillstyle='none', markeredgewidth=1, label='0 negative class') \n",
    "pos_handle = plt.plot(x[pos_idx,0], x[pos_idx,1], 'rx', markersize=8, markeredgewidth=1, label='1 positive class') \n",
    "\n",
    "# add some labels and titles\n",
    "plt.axis([0, 4, 0, 4])\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Supervised Learning: Classification of labeled data');\n",
    "plt.legend([neg_handle[0], pos_handle[0]], ['0 negative class', '1 positive class']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the $y$ vector holds the binary classification labels, and the data we are given to train with is\n",
    "in one of two classes, $0$ negative class or $1$ positive class.\n",
    "\n",
    "For unsupervised learning we are given $m$ unlabeled samples of data to use:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Training set:} \\{ x^{(1)}, x^{(2)}, x^{(3)}, \\ldots, x^{(m)} \\} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:35.518499Z",
     "start_time": "2019-11-04T19:00:35.333420Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the example figure\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# plot the points in our unlabeled data\n",
    "plt.plot(x[:,0], x[:,1], 'ko', markersize=8, fillstyle='full')\n",
    "\n",
    "# add some labels and titles\n",
    "plt.axis([0, 4, 0, 4])\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Unsupervised Learning');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unsupervised learning, we give the algorithm some data and we ask the algorithm to \"find some structure\"\n",
    "in the data.\n",
    "\n",
    "For example, given this data set, we might want the algorithm to find some likely clusters of the data, points that\n",
    "may be similar or of related categories.\n",
    "\n",
    "An algorithm that finds clusters is called a clustering algorithm.  The previous might have 2 clusters, or there\n",
    "might be even 3 or 4 good clusters.\n",
    "\n",
    "## Applications of Clustering\n",
    "\n",
    "- marked segmentation\n",
    "- social network analysis (coherent groups of people that form organically)\n",
    "- organize computing clusters\n",
    "- astronomical data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W8 02: K Means Algorithm\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=ZcBL9_86YxY&list=PLiPvV5TNogxIS4bHQVW4pMkj4CHA8COdX&index=92)\n",
    "\n",
    "The K-means algorithm is an example of a clustering unsupervised learning algorithm.  It is probably the simplest clustering\n",
    "algorithm, but it is still quite effective.  Thus it is still one of the most popular and most used clustering algorithms.\n",
    "\n",
    "K-means is an iterative algorithm.  We start by specifying how many clusters (e.g. K clusters) we want the algorithm to \n",
    "discover.  More formally, we can define the **K-means algorithm**\n",
    "\n",
    "- Input:\n",
    "  - $K$ (number of clusters)\n",
    "  - Training set of $m$ inputs $\\{x^{(1)}, x^{(2)}, \\ldots, x^{(m)}\\}$\n",
    "- Where $x^{(i)} \\in \\mathbb{R}^n$ (we drop the $x_0 = 1$ convention)  \n",
    "\n",
    "And the **K-means algorithm** pseudocode\n",
    "\n",
    "- Randomly initialize $K$ cluster centroids $\\mu_1, \\mu_2, \\ldots, \\mu_K \\in \\mathbb{R}^n$\n",
    "\n",
    "- Repeat {\n",
    "  - for $i = 1$ to $m$\n",
    "    - $c^{(i)}$ := index (from 1 to $K$) of cluster centroid closest to $x^{(i)}$\n",
    "  - for $k = 1$ to $K$\n",
    "    - $\\mu_k$ := average (mean) of points assigned to cluster $k$\n",
    "- }\n",
    "\n",
    "This basic algorithm for K-means clustering is really fairly simple, and it will help to understand it even further if we make\n",
    "a quick and basic implementation of the algorithm in Python code.  First of all, we will read in a small simple set of\n",
    "data that appears to be well separated into 2 clusters.  This dataset has $m = 32$ examples.  The dataset has only 2 features\n",
    "$n = 2$, thus all of the points are in 2 dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:35.674825Z",
     "start_time": "2019-11-04T19:00:35.519831Z"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.datasets import make_blobs\n",
    "#X, y = make_blobs(n_samples = 10, n_features=2, centers=2, cluster_std = 0.8, center_box=(2, 5))\n",
    "#np.savetxt('../data/lect-11-ex1data.csv', X, delimiter=',')\n",
    "X = np.loadtxt('../../data/lect-11-ex1data.csv', delimiter=',')\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], 'go')\n",
    "plt.xlabel(r'$x_1$', fontsize=20)\n",
    "plt.ylabel(r'$x_2$', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the K-means algorithm is to randomly initialize a set of centroids.  We usually initialze the centroids to be\n",
    "within the ranges of the data set.  So for example, if we find the minimum and maximum values for the data for each of the dimensions,\n",
    "we can use this to randomly initialize our centroids.  In this case, we are going to try and find $K = 2$ clusters, so we want to\n",
    "create two centroids within the range of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:35.843429Z",
     "start_time": "2019-11-04T19:00:35.676125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.0 6.0\n",
      "-5.0 5.0\n"
     ]
    }
   ],
   "source": [
    "# The number of clusters K we will find\n",
    "K = 2\n",
    "\n",
    "# The number of training data points, and the number of dimensions of our data set\n",
    "m, n = X.shape\n",
    "\n",
    "# randomly initialize K centroids\n",
    "min_x1, max_x1 = min(X[:, 0]), max(X[:, 0])\n",
    "min_x2, max_x2 = min(X[:, 1]), max(X[:, 1])\n",
    "print(min_x1, max_x1)\n",
    "print(min_x2, max_x2)\n",
    "\n",
    "# create K centroids mu, where each point is randomly chosen within the range of the data\n",
    "mu = np.zeros( (K, n) )\n",
    "for k in range(K):\n",
    "    mu[k, 0] = np.random.uniform(low = min_x1, high = max_x1)\n",
    "    mu[k, 1] = np.random.uniform(low = min_x2, high = max_x2)\n",
    "    \n",
    "# visualize the original data, with our randomly chosen initial centroid points\n",
    "plt.plot(X[:, 0], X[:, 1], 'go', label='training data')\n",
    "plt.plot(mu[:,0], mu[:,1], 'rx', markersize=15, label='centroids')\n",
    "plt.xlabel(r'$x_1$', fontsize=20)\n",
    "plt.ylabel(r'$x_2$', fontsize=20)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the video, the first step in the iterative part of the K-means algorithm is to assign each of the training data points to\n",
    "one of our $\\mu$ clusters.  As shown in the video, we do this by calculating the distance between each data point and our\n",
    "two centroids, and we assign the point to the closest centroid.  The measure used to calculate the distance can actually be\n",
    "calculated in different ways.  The simplest is to use the eucledian distance.  And since the distance can be negative\n",
    "depending on the order we evaluate points when calculating the distance, we usually take the square of the distance so that\n",
    "all values are positive (e.g. we get the magnitude of the distance), and we can thus compare directly and find the minimum.\n",
    "\n",
    "$$\n",
    "\\underset{k}{\\textrm{min}} \\;\\; \\| x^{(i)} - \\mu_k  \\|^2\n",
    "$$\n",
    "\n",
    "For example, lets calculate the distance between the first training data point and the two randomly generated centroids.  Keep \n",
    "in mind that in Python, our arrays are indexed starting at 0, so the first training data example will be at $i = 0$.  Also, with\n",
    "$K = 2$ cluster centroids, the $k$ clusters will range from $0$ to $1$.\n",
    "\n",
    "Lets start by defining a function that will take 2 $n$ dimensional points, and calculate the square of the distance between\n",
    "the two points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:35.847335Z",
     "start_time": "2019-11-04T19:00:35.844742Z"
    }
   },
   "outputs": [],
   "source": [
    "def distance(x, y):\n",
    "    # calculate the square of the distance between 2 n dimensional points (passed as numpy arrays)\n",
    "    # eucledian distance is sqrt( (x_1 - y_1)**2.0 + (x_2 - y_2)**2.0 ), but we then square this, so\n",
    "    # we simply need the sum of the differences squared\n",
    "    return np.sum( (x - y)**2.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:35.929082Z",
     "start_time": "2019-11-04T19:00:35.850106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 training example: (-3.500000, -5.000000)\n",
      "0 cluster centroid: (3.902589, 0.312585)\n",
      "square distance between input 0 and cluster 0: 83.021894\n",
      "\n",
      "0 training example: (-3.500000, -5.000000)\n",
      "1 cluster centroid: (-3.895291, 2.914031)\n",
      "square distance between input 0 and cluster 1: 62.788142\n"
     ]
    }
   ],
   "source": [
    "# distance from the 0th training example and 0th cluster\n",
    "i = 0\n",
    "k = 0\n",
    "print(\"%d training example: (%f, %f)\" % (i, X[i,0], X[i,1]))\n",
    "print(\"%d cluster centroid: (%f, %f)\" % (k, mu[k,0], mu[k,1]))\n",
    "print(\"square distance between input %d and cluster %d: %f\" % (i, k, distance(X[i,:], mu[k,:])))\n",
    "\n",
    "# distance from the 0th training example and 1th cluster\n",
    "i = 0\n",
    "k = 1\n",
    "print(\"\")\n",
    "print(\"%d training example: (%f, %f)\" % (i, X[i,0], X[i,1]))\n",
    "print(\"%d cluster centroid: (%f, %f)\" % (k, mu[k,0], mu[k,1]))\n",
    "print(\"square distance between input %d and cluster %d: %f\" % (i, k, distance(X[i,:], mu[k,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, the above function for calculating the distance basically does the same thing as calculating \n",
    "the norm between the two point vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:36.002798Z",
     "start_time": "2019-11-04T19:00:35.930537Z"
    }
   },
   "outputs": [],
   "source": [
    "def distance_norm(x, y):\n",
    "    # calculate the square of the distance between 2 n dimensional points (passed as numpy arrays)\n",
    "    # using the linear algebra vector norm to calculate the distance\n",
    "    return np.linalg.norm(x - y)**2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:36.075603Z",
     "start_time": "2019-11-04T19:00:36.004282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 training example: (-3.500000, -5.000000)\n",
      "0 cluster centroid: (3.902589, 0.312585)\n",
      "square distance between input 0 and cluster 0: 83.021894\n",
      "\n",
      "0 training example: (-3.500000, -5.000000)\n",
      "1 cluster centroid: (-3.895291, 2.914031)\n",
      "square distance between input 0 and cluster 1: 62.788142\n"
     ]
    }
   ],
   "source": [
    "# distance from the 0th training example and 0th cluster\n",
    "i = 0\n",
    "k = 0\n",
    "print(\"%d training example: (%f, %f)\" % (i, X[i,0], X[i,1]))\n",
    "print(\"%d cluster centroid: (%f, %f)\" % (k, mu[k,0], mu[k,1]))\n",
    "print(\"square distance between input %d and cluster %d: %f\" % (i, k, distance_norm(X[i,:], mu[k,:])))\n",
    "\n",
    "# distance from the 0th training example and 1th cluster\n",
    "i = 0\n",
    "k = 1\n",
    "print(\"\")\n",
    "print(\"%d training example: (%f, %f)\" % (i, X[i,0], X[i,1]))\n",
    "print(\"%d cluster centroid: (%f, %f)\" % (k, mu[k,0], mu[k,1]))\n",
    "print(\"square distance between input %d and cluster %d: %f\" % (i, k, distance_norm(X[i,:], mu[k,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the iterative algorithm is to calculate such distances between each training data item and every centroid, \n",
    "find the minimum, and assign the training data item to be in the cluster whose centroid it is closest too.  So for example,\n",
    "we can determine the closest centroid for each training data point like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:36.364007Z",
     "start_time": "2019-11-04T19:00:36.077213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point x[0] in cluster: 1\n",
      "point x[1] in cluster: 1\n",
      "point x[2] in cluster: 0\n",
      "point x[3] in cluster: 1\n",
      "point x[4] in cluster: 1\n",
      "point x[5] in cluster: 0\n",
      "point x[6] in cluster: 1\n",
      "point x[7] in cluster: 1\n",
      "point x[8] in cluster: 0\n",
      "point x[9] in cluster: 0\n",
      "point x[10] in cluster: 1\n",
      "point x[11] in cluster: 0\n",
      "point x[12] in cluster: 0\n",
      "point x[13] in cluster: 1\n",
      "point x[14] in cluster: 1\n",
      "point x[15] in cluster: 1\n",
      "point x[16] in cluster: 1\n",
      "point x[17] in cluster: 0\n",
      "point x[18] in cluster: 0\n",
      "point x[19] in cluster: 0\n",
      "point x[20] in cluster: 0\n",
      "point x[21] in cluster: 0\n",
      "point x[22] in cluster: 1\n",
      "point x[23] in cluster: 0\n",
      "point x[24] in cluster: 0\n",
      "point x[25] in cluster: 0\n",
      "point x[26] in cluster: 0\n",
      "point x[27] in cluster: 0\n",
      "point x[28] in cluster: 0\n",
      "point x[29] in cluster: 0\n",
      "point x[30] in cluster: 0\n",
      "point x[31] in cluster: 0\n"
     ]
    }
   ],
   "source": [
    "# This array will hold the index k of the cluster centroid each training data point is assigned too\n",
    "c = np.zeros(m)\n",
    "\n",
    "# for each training data point i\n",
    "for i in range(m):\n",
    "    # determine distance to cluster 0\n",
    "    min_distance = distance_norm(X[i,:], mu[0, :])\n",
    "    c[i] = 0\n",
    "    # find out if any other cluster centroid k=1,...K is closer\n",
    "    for k in range(1, K):\n",
    "        another_distance = distance_norm(X[i,:], mu[k, :])\n",
    "        if another_distance < min_distance:\n",
    "            min_distance = another_distance\n",
    "            c[i] = k\n",
    "            \n",
    "# the above loop represents the code needed to assign each point to the closest cluster mu.  Here were the\n",
    "# clusters that each point was assigned to\n",
    "for i in range(m):\n",
    "    print(\"point x[%d] in cluster: %d\" % (i, c[i]))\n",
    "            \n",
    "# lets visualize the resulting assignments of the points to the current cluster centroids\n",
    "cluster_0 = np.where(c == 0)[0]\n",
    "cluster_1 = np.where(c == 1)[0]\n",
    "\n",
    "#plt.figure(figsize=(8,16))\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.plot(X[cluster_0, 0], X[cluster_0, 1], 'ro', label='assg cluster 0')\n",
    "plt.plot(mu[0,0], mu[0,1], 'rx', markersize=15)\n",
    "plt.plot(X[cluster_1, 0], X[cluster_1, 1], 'bo', label='assg cluster 1')\n",
    "plt.plot(mu[1,0], mu[1,1], 'bx', markersize=15, label='centroids')\n",
    "plt.xlabel(r'$x_1$', fontsize=20)\n",
    "plt.ylabel(r'$x_2$', fontsize=20)\n",
    "plt.legend(loc=2)\n",
    "plt.axis([-5, 7, -7, 7]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have found which centroid each training data item is closest to, it is time to update the centroids.  We do this by calculating\n",
    "new mu centroids which are simply the average of all of the points assigned to that centroid.  For example, we can use numpy vector\n",
    "operations and the c array to find and average all of the points assigned to cluster $k = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:36.368938Z",
     "start_time": "2019-11-04T19:00:36.365172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.   -4.5 ]\n",
      " [-0.2  -4.6 ]\n",
      " [-0.5  -2.1 ]\n",
      " [ 0.   -2.  ]\n",
      " [-0.5  -0.6 ]\n",
      " [-0.2  -1.8 ]\n",
      " [ 0.4   1.9 ]\n",
      " [ 0.5   2.  ]\n",
      " [ 0.55  2.3 ]\n",
      " [ 0.6   2.5 ]\n",
      " [ 0.55  3.  ]\n",
      " [ 1.5   0.3 ]\n",
      " [ 2.    1.  ]\n",
      " [ 2.8   0.1 ]\n",
      " [ 2.7   1.5 ]\n",
      " [ 2.6   2.9 ]\n",
      " [ 2.7   5.  ]\n",
      " [ 3.    2.5 ]\n",
      " [ 5.5   1.5 ]\n",
      " [ 6.    2.  ]]\n",
      "[1.4   0.645]\n"
     ]
    }
   ],
   "source": [
    "cluster_0 = np.where(c == 0)[0]\n",
    "print(X[cluster_0])\n",
    "print(np.mean(X[cluster_0], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above idea to recalculate all $K$ centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:36.650668Z",
     "start_time": "2019-11-04T19:00:36.370174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.4         0.645     ]\n",
      " [-2.3        -1.35833333]]\n"
     ]
    }
   ],
   "source": [
    "# recalculate all cluster centroids\n",
    "for k in range(K):\n",
    "    cluster_pts = np.where(c == k)[0]\n",
    "    mu[k] = np.mean(X[cluster_pts], axis=0)\n",
    "\n",
    "\n",
    "# show the resulting new cluster centroids\n",
    "print(mu)\n",
    "\n",
    "\n",
    "# visualize the new centroid locations in relation to the assigned points in the clusters\n",
    "cluster_0 = np.where(c == 0)[0]\n",
    "cluster_1 = np.where(c == 1)[0]\n",
    "\n",
    "#plt.figure(figsize=(8,16))\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.plot(X[cluster_0, 0], X[cluster_0, 1], 'ro', label='assg cluster 0')\n",
    "plt.plot(mu[0,0], mu[0,1], 'rx', markersize=15)\n",
    "plt.plot(X[cluster_1, 0], X[cluster_1, 1], 'bo', label='assg cluster 1')\n",
    "plt.plot(mu[1,0], mu[1,1], 'bx', markersize=15, label='centroids')\n",
    "plt.xlabel(r'$x_1$', fontsize=20)\n",
    "plt.ylabel(r'$x_2$', fontsize=20)\n",
    "plt.legend(loc=1)\n",
    "plt.axis([-5, 7, -7, 7]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous steps to randomly initialize a set of centroids, then repeatedly assign points to closest centroid and move the\n",
    "centroids can easily be made into a function that performs the basic K-means algorithm.  We will leave this as an exercise\n",
    "for the student for now to try and bring these pieces together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means for non-separated clusters\n",
    "\n",
    "The previous example(s) had data that looked well separated.  However you can still run k-means clustering on\n",
    "data that is not so clearnly separated.  Often analysis of a market to do market segmentation can benefit from\n",
    "doing a k-means clustering even on data without real clear segmented groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W8 03: Optimization Objective\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=sHFUYHLOmUQ&list=PLiPvV5TNogxIS4bHQVW4pMkj4CHA8COdX&index=93)\n",
    "\n",
    "As this video discusses, we can formally define a cost function and optimization objective for the K-means algorithm.\n",
    "The cost function is high when the points in a cluster are far away from the cluster centroid, and it will be lower\n",
    "when the points in a cluster are close to the cluster centroid:\n",
    "\n",
    "$$\n",
    "J(c^{(1)}, \\ldots, c^{(m)}, \\mu_1, \\ldots, \\mu_K) = \\frac{1}{m} \\sum_{i=1}^m \\| x^{(i)} - \\mu_{c^{(i)}} \\|^2\n",
    "$$\n",
    "\n",
    "Thus for optimization we are trying to assign our points to clusters, which define the cluster centroids, that minimizes\n",
    "this cost objective function:\n",
    "\n",
    "$$\n",
    "\\underset{c^{(1)}, \\ldots, c^{(m)}, \\\\ \\mu_1, \\ldots, \\mu_K}{\\textrm{min}} \\;\\; J(c^{(1)}, \\ldots, c^{(m)}, \\mu_1, \\ldots, \\mu_K)\n",
    "$$\n",
    "\n",
    "In words, k-means is trying to find parameters $c^{(i)}$ and $\\mu_k$ that minimizes the sum of the squared distances\n",
    "between each point and its assigned centroid.  It should be obvious from the previous pseudocode that we\n",
    "assign the points to the centroid that it is closest too, thus we are minimizing the distance from each point to the\n",
    "current set of centroids.  K-means is another example of a greedy algorithm.  But in this case, the next step\n",
    "after cluster assignment of recalculating centroid locations based on the current assigned points in the cluster, can\n",
    "be shown to lead to $\\mu_k$ points that will end up having minimal overall summed up costs, once the algorithm\n",
    "has converged.\n",
    "\n",
    "So in other words, the cluster assignment step minimizes the cost function of the centroids with respect to\n",
    "the cluster assignments, while holding the centroids fixed.   Then in the second step to move the centroids,\n",
    "it chooses the values of $\\mu_k$ that minimize the cost function with respect to the changing centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W8 04: Random Initialization\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=8R5sqiZFCiE&list=PLiPvV5TNogxIS4bHQVW4pMkj4CHA8COdX&index=94)\n",
    "\n",
    "Above we showed simply picking $K$ random points within the range of the training data examples in order to randomly choose the\n",
    "initial cluster centroids.  In this video, the instructor illustrates a different method, which usually works a bit better, and\n",
    "it is actually a bit easier to understand.  If we want to discover $K$ cluster, we can simply choose $K$ of our input training\n",
    "data points at random to be our initial centroids.  We know that by picking 2 of the input data points that the centroids will\n",
    "automatically be within the range of the training data.  So for example, in Python, we could choose K points at random\n",
    "to be our centroids like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:36.655875Z",
     "start_time": "2019-11-04T19:00:36.651944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19 29]\n",
      "[[0.55 2.3 ]\n",
      " [3.   2.5 ]]\n"
     ]
    }
   ],
   "source": [
    "# choose the number of clusters we will be creating\n",
    "K = 2\n",
    "\n",
    "# this will choose 2 indexes in range 0 to m-1, that we will use as our initial points for the mu centroids\n",
    "# NOTE: in the next function, the replace=False ensures that the choice() function will not pick the same random\n",
    "# index.\n",
    "random_pts = np.random.choice(m, size=K, replace=False)\n",
    "print(random_pts)\n",
    "\n",
    "mu = X[random_pts]\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is the recommended way to choose the initial K centroid methods, and is what will normally be used\n",
    "by a K-means library like for example the scikit-learn K-means implementation.\n",
    "\n",
    "The optimization cost function defined by K-means is not guaranteed to have only 1 global minimum (unlike\n",
    "some previous cost functions we defined).  Thus when you run a K-means with random initial points, the\n",
    "clusters the algorithm finds and converges on can be different depending on the random starting locations\n",
    "picked.  Thus K-means is not deterministic, you can get different clustering results each time you run\n",
    "the algorithm.\n",
    "\n",
    "One solution to this is to run K-means multiple times with different starting random initializations.  At the\n",
    "end of K-means clustering, once the algorithm has converged, you can find the final cost of the discovered\n",
    "clusters.  Usually if you are hitting local minimum when clustering, if you run multiple times you can compare\n",
    "the final costs of the multiple runs, and usually the lower or lowest final costs achieved will be the better\n",
    "clusterings of your data.\n",
    "\n",
    "Somewhat backwards from what you might intuitively expect, local optimization tend to be more of a problem when\n",
    "the number of clusters K you want to determine is relatively small, say from 2 to 10 as a rule of thumb.  For these\n",
    "number of clusters it is usually a good idea to run 50 to 1000 or so K-means clustering attempts, keep track\n",
    "of the final cost of each, and examine/use the one that achieved the lowest cost at the end.\n",
    "\n",
    "However when you are trying to create larger number of clusters, often the minimum that exist are going to\n",
    "be all relatively close to the same, so one clustering, even if different from another one found, will have\n",
    "a similar overall cost.  Thus when trying to determine a large number of clusters/segments it is not as useful\n",
    "or necessary to run multiple times to watch out for local minima results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W8 05: Choosing the Number of Clusters\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=UAWDoxp-5Gk&list=PLiPvV5TNogxIS4bHQVW4pMkj4CHA8COdX&index=95)\n",
    "\n",
    "The most common method is still to choose the number of clusters you want by hand.  Sometimes the problem you are trying to\n",
    "cluster natually lends itself to a particular number of clusters you desire (e.g. we want to design t-shirts for 3 sizes, S, M and L).  Other times, you can do some visualization of the data, and get a rough idea of how many there might be, but often there can\n",
    "be different interpretations of this.\n",
    "\n",
    "\n",
    "It is often genuinly ambiguous the number of clusters in a data set.  Even if/when you can visualize, there will\n",
    "often be ambiguity, and different clustering can be supported based on the needs of the application.\n",
    "\n",
    "There are some things you can do to try and help to algorithmically pick a good size for K for your clustering.\n",
    "\n",
    "In the Elbow method, you compute the cluster for $K=1, 2, 3, ... N$, and look at the final cost function $J$ achieved\n",
    "for each $K$ clustering size.  If you plot the cost, often there will be some point where the cost changes from going\n",
    "down rapidly to going down much slower.  Often the \"Elbow\" of this curve, or somewhere around it, will be a good\n",
    "number of cluster for the data you have.\n",
    "\n",
    "However it is possible to get a much more ambiguous result, where there is no apparent elbow to your graph.\n",
    "In practice this will often be the case.  It can be worth a shot, but as often as not you won't get a good idea\n",
    "from this of what might be a good K size, thus you will have to result to other means.\n",
    "\n",
    "Another method is really application driven.  If you have some metric downstream for evaluating the effectiveness\n",
    "of your application, you can then compare that metric when you try different clustering values K, and use the\n",
    "clustering/segmentation that works best for the application domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering with Scikit-Learn\n",
    "\n",
    "`Scikit-Learn` has clustering algorithms in the `sklearn.cluster` sublibrary.\n",
    "\n",
    "We can get a basic clustring of the made up data set we had previously using $K = 2$\n",
    "clusters like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:36.988033Z",
     "start_time": "2019-11-04T19:00:36.658373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.96875 -2.49375]\n",
      " [ 1.99375  2.28125]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "139.7525\n"
     ]
    }
   ],
   "source": [
    "# fit a clustering estimator to the made up date\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster = KMeans(n_clusters=2);\n",
    "cluster.fit(X)\n",
    "\n",
    "# display/access the results, like the final cluster centers, the labels of the data, and the final cost\n",
    "centers = cluster.cluster_centers_\n",
    "print(centers)\n",
    "labels = cluster.labels_\n",
    "print(labels)\n",
    "cost = cluster.inertia_\n",
    "print(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:37.175324Z",
     "start_time": "2019-11-04T19:00:36.989769Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the final cluster labels found\n",
    "cluster_0 = np.where(labels == 0)[0]\n",
    "cluster_1 = np.where(labels == 1)[0]\n",
    "\n",
    "plt.plot(X[cluster_0, 0], X[cluster_0, 1], 'ro', label='assg cluster 0')\n",
    "plt.plot(centers[0,0], centers[0,1], 'rx', markersize=15)\n",
    "plt.plot(X[cluster_1, 0], X[cluster_1, 1], 'bo', label='assg cluster 1')\n",
    "plt.plot(centers[1,0], centers[1,1], 'bx', markersize=15, label='centroids')\n",
    "plt.xlabel(r'$x_1$', fontsize=20)\n",
    "plt.ylabel(r'$x_2$', fontsize=20)\n",
    "plt.legend(loc=2)\n",
    "plt.axis([-5, 7, -7, 7]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Local Minima\n",
    "\n",
    "In this case for our made up data the clusters are pretty well separated, and local minima are not so easy\n",
    "to find.  Lets run the cluster 1000 times, and keep the worst and best K-means clusters we find based on the\n",
    "cost (inertia) measure.\n",
    "\n",
    "By default `scikit-learn` actually performs the clustering 10 times (controlled by the `n_init`) parameter.\n",
    "If we want to see different results, we should set `n_init = 1` so that only 1 clustering is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:38.252096Z",
     "start_time": "2019-11-04T19:00:37.176515Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "clusters = []\n",
    "cost = np.empty(N)\n",
    "\n",
    "# now perform different clusterings, checking for ones that improve or make things worse\n",
    "for n in range(N):\n",
    "    cluster = KMeans(n_clusters=2, n_init=1)\n",
    "    cluster.fit(X)\n",
    "    clusters.append(cluster)\n",
    "    cost[n] = cluster.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:38.259616Z",
     "start_time": "2019-11-04T19:00:38.253427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.7525\n",
      "139.7525\n"
     ]
    }
   ],
   "source": [
    "# in this case, we are never seeing any solution other than the 1 minimum that is discovered\n",
    "print(cost.min())\n",
    "print(cost.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine K number of clusters\n",
    "\n",
    "Likewise lets try and illustrate the Elbow method, and fit the data with clusters of size $K = 1 \\cdots 10$\n",
    "Again the data looks like 2 clusters is pretty optimal, so we won't get a very useful result here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:38.667122Z",
     "start_time": "2019-11-04T19:00:38.261086Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_K = 10\n",
    "\n",
    "clusters = []\n",
    "cost = np.empty(MAX_K+1)\n",
    "\n",
    "# perform clusterings for K ranging from 2 to MAX_K.  If we were having local minima issues, we might also want\n",
    "# to run k-means multiple times for each K and find/choose the best cost achieved for the elbow graph\n",
    "for k in range(2, MAX_K+1):\n",
    "    cluster = KMeans(n_clusters=k)\n",
    "    cluster.fit(X)\n",
    "    clusters.append(cluster)\n",
    "    cost[k] = cluster.inertia_\n",
    "    \n",
    "# visualize the resulting costs as a function of K clustering size\n",
    "plt.plot(np.arange(2,MAX_K+1), cost[2:])\n",
    "plt.plot(np.arange(2,MAX_K+1), cost[2:], 'bo')\n",
    "plt.xlabel('K (clustering size)')\n",
    "plt.ylabel('J (cost or inertia) achieved')\n",
    "plt.title(\"Elbow plot of cost as a function of K clustering size\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering on Iris Data\n",
    "\n",
    "Using the simple data set does not give a great example of the potential for local minima and choosing K.\n",
    "Here we perform the previous again, but use the iris data set.  The iris data set is 4 dimensional, and we will\n",
    "use all 4 dimensions.  We will try clustering into 3 cluster, which is of course the number of categories\n",
    "we have for the original iris data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:38.686500Z",
     "start_time": "2019-11-04T19:00:38.668171Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:39.284430Z",
     "start_time": "2019-11-04T19:00:38.693387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.006      3.428      1.462      0.246     ]\n",
      " [6.85384615 3.07692308 5.71538462 2.05384615]\n",
      " [5.88360656 2.74098361 4.38852459 1.43442623]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1\n",
      " 1 1 2 2 1 1 1 1 2 1 2 1 2 1 1 2 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 1 1 2 1\n",
      " 1 2]\n",
      "78.8556658259773\n"
     ]
    }
   ],
   "source": [
    "# create a K-means clustering with K=3\n",
    "cluster = KMeans(n_clusters=3);\n",
    "cluster.fit(X)\n",
    "\n",
    "# display/access the results, like the final cluster centers, the labels of the data, and the final cost\n",
    "centers = cluster.cluster_centers_\n",
    "print(centers)\n",
    "labels = cluster.labels_\n",
    "print(labels)\n",
    "cost = cluster.inertia_\n",
    "print(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:39.575791Z",
     "start_time": "2019-11-04T19:00:39.285723Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize resulting clusters on the 2 dimensions of Petal length/width\n",
    "cluster_0 = np.where(labels == 0)[0]\n",
    "cluster_1 = np.where(labels == 1)[0]\n",
    "cluster_2 = np.where(labels == 2)[0]\n",
    "\n",
    "plt.plot(X[cluster_0, 0], X[cluster_0, 1], 'ro', label='assg cluster 0')\n",
    "plt.plot(centers[0,0], centers[0,1], 'rx', markersize=15)\n",
    "plt.plot(X[cluster_1, 0], X[cluster_1, 1], 'bo', label='assg cluster 1')\n",
    "plt.plot(centers[1,0], centers[1,1], 'bx', markersize=15, label='centroids')\n",
    "plt.plot(X[cluster_2, 0], X[cluster_2, 1], 'go', label='assg cluster 2')\n",
    "plt.plot(centers[2,0], centers[2,1], 'gx', markersize=15, label='centroids')\n",
    "plt.xlabel(r'petal width', fontsize=20)\n",
    "plt.ylabel(r'petal height', fontsize=20)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Local Minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:41.613491Z",
     "start_time": "2019-11-04T19:00:39.577866Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "clusters = []\n",
    "cost = np.empty(N)\n",
    "\n",
    "# now perform different clusterings, checking for ones that improve or make things worse\n",
    "for n in range(N):\n",
    "    cluster = KMeans(n_clusters=3, n_init=1)\n",
    "    cluster.fit(X)\n",
    "    clusters.append(cluster)\n",
    "    cost[n] = cluster.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:41.618208Z",
     "start_time": "2019-11-04T19:00:41.614814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.85144142614601\n",
      "145.45269176485027\n",
      "[ 78.85144143  78.85566583  78.85566583 142.7540625  145.45269176]\n"
     ]
    }
   ],
   "source": [
    "# For the iris data\n",
    "print(cost.min())\n",
    "print(cost.max())\n",
    "print(np.unique(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:41.905752Z",
     "start_time": "2019-11-04T19:00:41.620656Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6780/885447551.py:1: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(cost);\n"
     ]
    }
   ],
   "source": [
    "sns.distplot(cost);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it looks like we usually discover 3 or 4 unique mimina.  Most of the time we get a cost of a bit\n",
    "over 78.  But sometimes we get 142 or 145, which are probably not as optimal clusterings.  Lets plot one of\n",
    "the 145 cost clusterings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:41.911019Z",
     "start_time": "2019-11-04T19:00:41.907145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=3, n_init=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;KMeans<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html\">?<span>Documentation for KMeans</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KMeans(n_clusters=3, n_init=1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=3, n_init=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)\n",
    "clusters[318]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:41.977124Z",
     "start_time": "2019-11-04T19:00:41.912156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "[[4.73181818 2.92727273 1.77272727 0.35      ]\n",
      " [6.31458333 2.89583333 4.97395833 1.703125  ]\n",
      " [5.19375    3.63125    1.475      0.271875  ]]\n",
      "[2 0 0 0 2 2 0 2 0 0 2 2 0 0 2 2 2 2 2 2 2 2 2 2 0 0 2 2 2 0 0 2 2 2 0 2 2\n",
      " 2 0 2 2 0 0 2 2 0 2 0 2 2 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1]\n",
      "142.7540625\n"
     ]
    }
   ],
   "source": [
    "cluster_num = np.where(cost > 140)[0][0]\n",
    "print(cluster_num)\n",
    "cluster = clusters[cluster_num]\n",
    "\n",
    "centers = cluster.cluster_centers_\n",
    "print(centers)\n",
    "labels = cluster.labels_\n",
    "print(labels)\n",
    "cost = cluster.inertia_\n",
    "print(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:00:42.225829Z",
     "start_time": "2019-11-04T19:00:41.979202Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize resulting clusters on the 2 dimensions of Petal length/width\n",
    "cluster_0 = np.where(labels == 0)[0]\n",
    "cluster_1 = np.where(labels == 1)[0]\n",
    "cluster_2 = np.where(labels == 2)[0]\n",
    "\n",
    "plt.plot(X[cluster_0, 0], X[cluster_0, 1], 'ro', label='assg cluster 0')\n",
    "plt.plot(centers[0,0], centers[0,1], 'rx', markersize=15)\n",
    "plt.plot(X[cluster_1, 0], X[cluster_1, 1], 'bo', label='assg cluster 1')\n",
    "plt.plot(centers[1,0], centers[1,1], 'bx', markersize=15, label='centroids')\n",
    "plt.plot(X[cluster_2, 0], X[cluster_2, 1], 'go', label='assg cluster 2')\n",
    "plt.plot(centers[2,0], centers[2,1], 'gx', markersize=15, label='centroids')\n",
    "plt.xlabel(r'petal width', fontsize=20)\n",
    "plt.ylabel(r'petal height', fontsize=20)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is definitely not a good clustering, it has broken up the smaller separate group (which were the easier\n",
    "to classify Virginica samples) into 2, and group the other 2 into 1 big cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine K number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:01:59.018445Z",
     "start_time": "2019-11-04T19:01:58.769225Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_K = 10\n",
    "\n",
    "clusters = []\n",
    "cost = np.empty(MAX_K+1)\n",
    "\n",
    "# perform clusterings for K ranging from 2 to MAX_K.  If we were having local minima issues, we might also want\n",
    "# to run k-means multiple times for each K and find/choose the best cost achieved for the elbow graph\n",
    "for k in range(2, MAX_K+1):\n",
    "    cluster = KMeans(n_clusters=k, n_init=1)\n",
    "    cluster.fit(X)\n",
    "    clusters.append(cluster)\n",
    "    cost[k] = cluster.inertia_\n",
    "    \n",
    "# visualize the resulting costs as a function of K clustering size\n",
    "plt.plot(np.arange(2,MAX_K+1), cost[2:])\n",
    "plt.plot(np.arange(2,MAX_K+1), cost[2:], 'bo')\n",
    "plt.xlabel('K (clustering size)')\n",
    "plt.ylabel('J (cost or inertia) achieved')\n",
    "plt.title(\"Elbow plot of cost as a function of K clustering size\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
