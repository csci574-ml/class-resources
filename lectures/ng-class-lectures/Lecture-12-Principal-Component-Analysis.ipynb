{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:27.267704Z",
     "start_time": "2019-11-11T20:08:26.849793Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "# for 3d plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib widget\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:27.273678Z",
     "start_time": "2019-11-11T20:08:27.269099Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (12, 10) # set default figure size, 8in by 6in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, we introduce Principal Components Analysis, and show how it can be used for data compression to speed up learning algorithms as well as for visualizations of complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W8 06: Motivation I Data Compression\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=gHHVdOb-s04&list=PLiPvV5TNogxIS4bHQVW4pMkj4CHA8COdX&index=96)\n",
    "\n",
    "This video gives a good intuitive idea of how data compression by reducing dimensions works.  As shown in the video, a 2\n",
    "dimensional highly redundante (linear) feature can be basically reduced to 1 dimension with almost no loss of information.\n",
    "This type of reduction of dimensions is very important in practice.  Less dimensions means less features we have for our learning\n",
    "algorithms.  This reduces the time and effort needed to build models using the features.  \n",
    "\n",
    "\n",
    "It might seem a bit contrived, but it is really quite common, especially when working with big data, that a lot\n",
    "of features will have a large bit of overlap.  Such redundant features are often not going to add any predictive\n",
    "power to a classifier, and are thus best eliminated.\n",
    "\n",
    "For example, from the video we imagine a data set that has features both measuring the length of some item,\n",
    "but one was measured in cm and one in inches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:27.731862Z",
     "start_time": "2019-11-11T20:08:27.275044Z"
    }
   },
   "outputs": [],
   "source": [
    "# make some cm measurements at random, from 10 to 30 cm\n",
    "m = 14\n",
    "x1 = np.random.choice(np.arange(10, 31), m, replace=False)\n",
    "\n",
    "# convert to inches and round to nearest inch\n",
    "x2 = np.round(x1 / 2.54)\n",
    "\n",
    "#print(x1)\n",
    "#print(x2)\n",
    "\n",
    "# plot the points\n",
    "plt.plot(x1, x2, 'kx', markersize=10)\n",
    "plt.xlabel('$x_1$ (cm)', fontsize=16)\n",
    "plt.ylabel('$x_2$ (in)', fontsize=16);\n",
    "\n",
    "# plot a fitted line to the points\n",
    "theta = np.polyfit(x1, x2, 1)\n",
    "h = np.poly1d(theta)\n",
    "x = np.linspace(10,30,100)\n",
    "plt.plot(x, h(x), 'b-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We approximate the original data set by projecting each original point onto the line.\n",
    "\n",
    "Reduce data from 2D to 1D\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "x^{(1)} \\in \\mathbb{R}^2 &\\to z^{(1)} \\in \\mathbb{R} \\\\\n",
    "x^{(2)} \\in \\mathbb{R}^2 &\\to z^{(2)} \\in \\mathbb{R} \\\\\n",
    "\\vdots \\\\\n",
    "x^{(m)} \\in \\mathbb{R}^2 &\\to z^{(m)} \\in \\mathbb{R} \\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce data from 3D to 2D Example\n",
    "\n",
    "In a typical data reduction/compression we might be doing something more like reducing $1000D \\to 100D$\n",
    "for example.  \n",
    "\n",
    "When you visualize a projection from 3D to 2D, what will happen is that you will project the data\n",
    "to a 2-dimensional hyperplane.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:27.739124Z",
     "start_time": "2019-11-11T20:08:27.733860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n"
     ]
    }
   ],
   "source": [
    "# generate some 3d data by hand in 2 classes, just for this example\n",
    "mu_vec1 = np.array([0,0,0])\n",
    "cov_mat1 = np.array([[1,0.66,0],[0.66,1,0.5],[0,0.5,1]])\n",
    "X = np.random.multivariate_normal(mu_vec1, cov_mat1, 100)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:27.978108Z",
     "start_time": "2019-11-11T20:08:27.740541Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.rcParams['legend.fontsize'] = 10   \n",
    "ax.plot(X[:, 0], X[:, 1], X[:, 2], 'o', markersize=8, color='blue', alpha=0.25)\n",
    "\n",
    "plt.title('Random samples with correlation/covariation');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example is from [here](https://stackoverflow.com/questions/22867620/putting-arrowheads-on-vectors-in-matplotlibs-3d-plot) of plotting arrows using eigenvectors calculated from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:27.985472Z",
     "start_time": "2019-11-11T20:08:27.979389Z"
    }
   },
   "outputs": [],
   "source": [
    "# matplotlib still doesn't have an arrow patch for drawing vectors, so we add it\n",
    "class Arrow3D(FancyArrowPatch):\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)\n",
    "\n",
    "    def do_3d_projection(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arrow3D(FancyArrowPatch):\n",
    "\n",
    "    def __init__(self, x, y, z, *args, **kwargs):\n",
    "        super().__init__((0, 0), (0, 0), *args, **kwargs)\n",
    "        self._xyz = (x[0], y[0], z[0])\n",
    "        self._dxdydz = (x[1], y[1], z[1])\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        x1, y1, z1 = self._xyz\n",
    "        dx, dy, dz = self._dxdydz\n",
    "        x2, y2, z2 = (x1 + dx, y1 + dy, z1 + dz)\n",
    "\n",
    "        xs, ys, zs = proj3d.proj_transform((x1, x2), (y1, y2), (z1, z2), self.axes.M)\n",
    "        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n",
    "        super().draw(renderer)\n",
    "        \n",
    "    def do_3d_projection(self, renderer=None):\n",
    "        x1, y1, z1 = self._xyz\n",
    "        dx, dy, dz = self._dxdydz\n",
    "        x2, y2, z2 = (x1 + dx, y1 + dy, z1 + dz)\n",
    "\n",
    "        xs, ys, zs = proj3d.proj_transform((x1, x2), (y1, y2), (z1, z2), self.axes.M)\n",
    "        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n",
    "\n",
    "        return np.min(zs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:28.122913Z",
     "start_time": "2019-11-11T20:08:27.986782Z"
    }
   },
   "outputs": [],
   "source": [
    "# mean of the x,y,z features\n",
    "mean_x = np.mean(X[:,0])\n",
    "mean_y = np.mean(X[:,1])\n",
    "mean_z = np.mean(X[:,2])\n",
    "\n",
    "#eigenvectors and eigenvalues\n",
    "eig_val, eig_vec = np.linalg.eig(cov_mat1)\n",
    "\n",
    "################################\n",
    "#plotting eigenvectors\n",
    "################################    \n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# plot the raw data\n",
    "ax.plot(X[:,0], X[:,1], X[:,2], 'o', markersize=10, color='blue', alpha=0.25)\n",
    "\n",
    "# plot origin of computed eigenvectors, which is located at the mean of each feature/dimension\n",
    "ax.plot([mean_x], [mean_y], [mean_z], 'o', markersize=5, color='red', alpha=0.4)\n",
    "\n",
    "\n",
    "for d,v in enumerate(eig_vec[:2]):\n",
    "    #ax.plot([mean_x,v[0]], [mean_y,v[1]], [mean_z,v[2]], color='red', alpha=0.8, lw=3)\n",
    "    #I will replace this line with:\n",
    "    v = v * eig_val[d]\n",
    "    a = Arrow3D([mean_x, v[0]], [mean_y, v[1]], \n",
    "                [mean_z, v[2]], mutation_scale=20, \n",
    "                lw=3, arrowstyle=\"-|>\", color=\"green\")\n",
    "    ax.add_artist(a)\n",
    "    \n",
    "# label and title for the figure\n",
    "ax.set_xlabel('x_values')\n",
    "ax.set_ylabel('y_values')\n",
    "ax.set_zlabel('z_values')\n",
    "\n",
    "plt.title('Random samples with correlation/covariation and first 2 Eigenvectors');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:28.556181Z",
     "start_time": "2019-11-11T20:08:28.124926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.53105351  0.63177464  0.56466183]\n",
      " [-0.69741065 -0.05258816  0.71473972]]\n",
      "[1.69060845 0.83508536]\n",
      "original shape:    (100, 3)\n",
      "transformed shape: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "# use PCA to project from 3D to 2D\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca.fit(X)\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)\n",
    "\n",
    "# plot the 2D projection\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.25)\n",
    "plt.xlabel('$z_0$', fontsize=16)\n",
    "plt.ylabel('$z_1$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W8 07: Motivation II Data Visualization\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=nGl3D6jkoFs&list=PLiPvV5TNogxIS4bHQVW4pMkj4CHA8COdX&index=97)\n",
    "\n",
    "\n",
    "Another main use of dimensionality reduction is in order to visualize very high dimensional data in 2\n",
    "or 3 dimensional space and plots.\n",
    "\n",
    "The previous plot and subsequent reduction from 3D to 2D was an example.  But for visualization, usually what we\n",
    "mean is we have a huge set of features, and we want to project down to the most important 2 or 3 features and plot\n",
    "those features.\n",
    "\n",
    "Using dimensionality reduction, we can project high dimensional data into a 3D or 2D set of data that we can\n",
    "then successfully visualize in 3 or 2 dimensions.\n",
    "\n",
    "When you use PCA, the new features do not directly correspond to any of the original features, so you will need\n",
    "to infer what they might most directly relate to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W8 08: Principal Component Analysis\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=l8Ys0rshfz0&list=PLiPvV5TNogxIS4bHQVW4pMkj4CHA8COdX&index=98)\n",
    "\n",
    "\n",
    "The formal method for accomplishing dimensionality reduction through the technique known as Principle Component Analysis (PCA) has\n",
    "some surface similarity to performing a linear regression.  Intuitively, we are trying to find a lower dimensional hyperplane for our data\n",
    "to which when we project our original data we minimize the sum squared error of the distances we had to project the data in order\n",
    "to reduce the dimensionality (the projection error).\n",
    "\n",
    "Principal Component Analysis (PCA) attempts to find a projection to a lower dimensional hyperplane\n",
    "that  minimizes the sum squared projection error of the projection of the points onto the new lower \n",
    "dimensional hyperplane.  As shown in the example in the video, if our original data is \n",
    "$\\mathbb{R}^2$ (2 dimensional), the lower dimensional hyperplane is in this case a 1 dimensional line.\n",
    "\n",
    "PCA is not linear regression.  The cost function is similar, but in PCA we are minimizing the projection\n",
    "distance to the line/plane, instead of minimizing the distance from the point to the dependent variable we\n",
    "are trying to predict.\n",
    "\n",
    "Lets look at a quick example of using PCA.  The following example is taken directly from the\n",
    "`scikit-learn` library [example on using PCA](http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html).\n",
    "\n",
    "First of all, lets look at a rather famous data set, the Iris dataset.  This data set consists of\n",
    "only $m = 150$ data points.  The data points are labeled between 3 different species of Iris\n",
    "flowers (species Virginica, Versicolor and Setosa).  PCA is an unsupervised learning technique, so\n",
    "we don't need the labels.  Each of the 150 data points has $\\mathbb{R}^4$ 4 dimensional measurements associated with it (petal length and width, and sepal length and width).  We of course can't visualze\n",
    "all 4 dimensions simultaneously, so lets visualize 3 of the 4 data points:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:28.577463Z",
     "start_time": "2019-11-11T20:08:28.557996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:28.709627Z",
     "start_time": "2019-11-11T20:08:28.578615Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot 3 of the 4 dimnsions\n",
    "fig = plt.figure()\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.jet)\n",
    "\n",
    "# The data in the 4 dimensions are:\n",
    "# X[:, 0] = 'sepal length'\n",
    "# X[:, 1] = 'sepal width'\n",
    "# X[:, 2] = 'petal length'\n",
    "# X[:, 3] = 'petal width'\n",
    "# so if you want the labels on the axis to be correct if you plot different\n",
    "# dimensions, need to also change these labels.\n",
    "dims = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width']\n",
    "ax.xaxis.set_label_text(dims[0])\n",
    "ax.yaxis.set_label_text(dims[1])\n",
    "ax.zaxis.set_label_text(dims[2])\n",
    "\n",
    "# likewise, we are labeling the type of each of the 3 species clases\n",
    "# by putting a text label on the feature at the mean.  You would have to\n",
    "# change the code here ias well to specify which dimensions of X[,?] you are plotting\n",
    "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 0].mean(),\n",
    "              X[y == label, 1].mean(),\n",
    "              X[y == label, 2].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to try plotting some other combinations of dimensions.  Try using the `%matplotlib notebook`\n",
    "notebook directive to allow for interactive plots (you may need to install additional libraries/components\n",
    "to get this to work.\n",
    "\n",
    "Lets compress the data from 4 dimensions to its 2 principal component dimensions and see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:28.895693Z",
     "start_time": "2019-11-11T20:08:28.716197Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], c='b', label='Setosa')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c='g', label='Versicolor')\n",
    "plt.scatter(X[y==2, 0], X[y==2, 1], c='r', label='Virginica')\n",
    "\n",
    "plt.xlabel('z1')\n",
    "plt.ylabel('z2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:28.901957Z",
     "start_time": "2019-11-11T20:08:28.896739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.36138659 -0.08452251  0.85667061  0.3582892 ]\n",
      " [ 0.65658877  0.73016143 -0.17337266 -0.07548102]]\n",
      "[0.92461872 0.05306648]\n",
      "[5.84333333 3.05733333 3.758      1.19933333]\n"
     ]
    }
   ],
   "source": [
    "print(pca.components_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if PCA is working correctly, this represents the 2 dimensions from the original 4 with the least amount of projection error of the original 4 dimensional data.  As you can see, while Setosa is well separated\n",
    "from the other 2 classes, there is some overlaop in Versicolor and Virginica classes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W8 09: Principal Component Analysis Algorithm\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=u4H6-k7CXro&list=PLiPvV5TNogxIS4bHQVW4pMkj4CHA8COdX&index=99)\n",
    "\n",
    "This video starts out by noting that you should always perform mean feature scaling and mean normalization\n",
    "of the features.  You will notice that in the example I gave above I did not do this by hand.  This is\n",
    "because the `scikit-learn` PCA library automatically performs the normalization on the\n",
    "data you give it before running PCA.  You will notice, for example, that the 2 dimensional principal\n",
    "components are centered at the origin $0, 0$\n",
    "\n",
    "\n",
    "The video at this point goes into the details of how the PCA algorithm is computed.  As stated in the\n",
    "video, we won't look at any proofs that these procedures correctly comptue the hyperplanes that\n",
    "minimize the projection error (it is beyond the scope of our course).  But lets just compute\n",
    "the PCA by hand using Python functions, and compare our results to the PCA computation obtained\n",
    "through the `scikit-learn` library.  \n",
    "\n",
    "As in the previous example, lets continue using the iris data set with 4 dimensions to perform\n",
    "our example of computing the PCA algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:28.978722Z",
     "start_time": "2019-11-11T20:08:28.903348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 training examples\n",
      "4 features/dimensions\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "m, n = X.shape # m training examples, each n dimensional\n",
    "print(m, \"training examples\")\n",
    "print(n, \"features/dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before when we used PCA from the `scikit-learn` library, it perform feature normalization for us.  But\n",
    "if we want to do PCA ourself by hand, we need to perform normalization:\n",
    "\n",
    "$$\n",
    "\\mu_j = \\frac{1}{m} \\sum_{i=1}^{m} x_j^{(i)}\n",
    "$$\n",
    "\n",
    "Where the $\\mu_j$ is really the mean for each of the $n$ features of our data.  We replace each\n",
    "feature $x_j^{(i)}$ with $x_j - \\mu_j$ (e.g. we are simply subtracting the mean from each feature).\n",
    "We can do feature normalization by hand like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:29.061448Z",
     "start_time": "2019-11-11T20:08:28.980213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[5.84333333 3.05733333 3.758      1.19933333]\n"
     ]
    }
   ],
   "source": [
    "# we could use loops to calculate the mean of each of the 4 features, but we can\n",
    "# use a simply numpy operation to get the mean along the appropriate axis\n",
    "mu = np.mean(X, axis=0)\n",
    "\n",
    "# we should end up with the mean value of each of our 4 dimensions, compare this to the means\n",
    "# in the iris dataset documentation to verify\n",
    "print(mu.shape)\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the means of each dimension, we can perform mean normalization.  Again we'll use numpy\n",
    "operations to subtract the mean for each dimension from each data set item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:29.247904Z",
     "start_time": "2019-11-11T20:08:29.063358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "[-1.12502600e-15 -7.60872846e-16 -2.55203266e-15 -4.48530102e-16]\n"
     ]
    }
   ],
   "source": [
    "X_norm = X - mu\n",
    "print(X_norm.shape)\n",
    "\n",
    " # notice that the means are all now basically 0 in our mean normalized data\n",
    "print(np.mean(X_norm, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To also do the feature scaling, one simple way is to calculate the standard deviation of each of \n",
    "our dimensions in the data, then divide the values by this standard deviation.  This has the effect of\n",
    "scaling the standard deviation of the dimension to be 1.0.  For example, we can do a very similar \n",
    "procedure to calculate the sd of each dimension, the perform both normalization and scaling\n",
    "\n",
    "$$\n",
    "X_j^{(i)} \\leftarrow \\frac{X_j^{(i)} - \\mu_j}{s_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:29.343843Z",
     "start_time": "2019-11-11T20:08:29.249555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[0.82530129 0.43441097 1.75940407 0.75969263]\n"
     ]
    }
   ],
   "source": [
    "sd =  np.std(X, axis=0)\n",
    "\n",
    "# we should end up with the standard deviation value of each of our 4 dimensions, again you\n",
    "# can compare this to the reported sd for each dimension\n",
    "print(sd.shape)\n",
    "print(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:29.421171Z",
     "start_time": "2019-11-11T20:08:29.344922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "[-1.69031455e-15 -1.84297022e-15 -1.69864123e-15 -1.40924309e-15]\n",
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_norm_scaled = (X - mu) / sd\n",
    "print(X_norm_scaled.shape)\n",
    "\n",
    " # notice that the means are all now basically 0 in our mean normalized data\n",
    "print(np.mean(X_norm_scaled, axis=0))\n",
    "print(np.std(X_norm_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as shown, the final result of normalization and scaling is that all features now have a mean of 0 and\n",
    "a standard deviation of 1.0.  It appears that the `scikit-learn` PCA only performs mean normalization \n",
    "for you on the data you fit.  Thus, in the above cells I created arrays `X_norm` and `X_nrom_scaled`,\n",
    "but we will only use `X_norm` in the next cells when we recreate the PCA algorithm by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Algorithm (computing by hand)\n",
    "\n",
    "**Step 1**: The first step shown in the video to perform the **PCA algorithm** is to compute a \"covariance\n",
    "matrix\", that we will name `Sigma`.  The formula for computing the covariance matrix is:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{m} \\sum_{i=1}^{m} (x^{(i)}) (x^{(i)})^T\n",
    "$$\n",
    "\n",
    "**NOTE**: There is a typo first time this equation is shown in video around time 8:00.  The sum goes over \n",
    "the $m$ training input patterns.\n",
    "\n",
    "We can compute that using Python for a single dimension, like dimension 0, with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:29.486711Z",
     "start_time": "2019-11-11T20:08:29.422474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "[[ 0.55254444 -0.32904889  1.75278     0.74283778]\n",
      " [-0.32904889  0.19595378 -1.043808   -0.44237156]\n",
      " [ 1.75278    -1.043808    5.560164    2.356428  ]\n",
      " [ 0.74283778 -0.44237156  2.356428    0.99866711]]\n"
     ]
    }
   ],
   "source": [
    "# In python, our dimensions are 0, 1, 2 and 3, so the above formula sums from i = 0 to 3.\n",
    "# For example, if we start with dimension 0, and we multiple x^(0) x^(0)^T we get\n",
    "c = np.dot(X_norm[0, :].reshape( (n,1) ), X_norm[0, :].reshape( (n,1) ).T)\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of doing this for dimension 0 is a 4 x 4 covariance matrix for that sample $i$.  This basically measures the amount of variance between each dimension for input pattern 0.  To get the full covariance\n",
    "matrix, we need to do this for all 150 input patterns, and sum up and average the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:29.559891Z",
     "start_time": "2019-11-11T20:08:29.487837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "[[ 0.68480585 -0.03995745  1.2775052   0.51778114]\n",
      " [-0.03995745  0.19001925 -0.32049995 -0.1178793 ]\n",
      " [ 1.2775052  -0.32049995  3.13257043  1.30268152]\n",
      " [ 0.51778114 -0.1178793   1.30268152  0.58379067]]\n"
     ]
    }
   ],
   "source": [
    "Sigma1 = np.empty( (n, n) )\n",
    "for i in range(m):\n",
    "    Sigma1 += np.dot( X_norm[i,:].reshape( (n, 1) ), X_norm[i, :].reshape(n, 1).T )\n",
    "Sigma1 = Sigma1 / m # finally take average by dividing by m\n",
    "\n",
    "print(Sigma1.shape)\n",
    "print(Sigma1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:29.619979Z",
     "start_time": "2019-11-11T20:08:29.561307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "[[ 0.68112222 -0.04215111  1.26582     0.51282889]\n",
      " [-0.04215111  0.18871289 -0.32745867 -0.12082844]\n",
      " [ 1.26582    -0.32745867  3.09550267  1.286972  ]\n",
      " [ 0.51282889 -0.12082844  1.286972    0.57713289]]\n"
     ]
    }
   ],
   "source": [
    "# as shown later in the video, an equivalent way of computing\n",
    "# sigma using linear algebra matrix multiplication exclusively is\n",
    "Sigma2 = np.dot(X_norm.T, X_norm) / m\n",
    "print(Sigma2.shape)\n",
    "print(Sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:29.693890Z",
     "start_time": "2019-11-11T20:08:29.621266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00368363, 0.00219366, 0.0116852 , 0.00495225],\n",
       "       [0.00219366, 0.00130636, 0.00695872, 0.00294914],\n",
       "       [0.0116852 , 0.00695872, 0.03706776, 0.01570952],\n",
       "       [0.00495225, 0.00294914, 0.01570952, 0.00665778]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I was expecting maybe some rounding error, but probably closer than this.  Not sure\n",
    "# if we have a slight bug/problem in the above example or not.\n",
    "Sigma1 - Sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Once we have the `Sigma` covariance matrix, the next step shown was to calculated the \"eigenvectors\"\n",
    "of the matrix.  We will also use the singular value decomposition of the matrix from the numpy\n",
    "linalg library to do this computation.\n",
    "\n",
    "SVD computes the eigenvectors of the Sigma covariance matrix.  You can also use `np.linalg.eig()` to get the\n",
    "same eigenvectors.\n",
    "\n",
    "What SVD outputs is 3 matrices, $U$, $S$ and $V$.  What we really need for dimensionality reduction is\n",
    "the $U$ matrix. The $U$ matrix will be an $n \\times n$ shaped matrix, where $n$ is the\n",
    "number of features or dimensions of the data set we are using.\n",
    "The columns of the $U$ matrix will be exactly the principle component vectors for the\n",
    "set of data we gave that generates the covariance matrix Sigma.  On in other words, $U$ will have the\n",
    "principle component vectors of the new space we want to project our data onto.\n",
    "\n",
    "So for data reduction, if we want to reduce from $n$ to $k$ dimensions, we simply want to take the first $k$\n",
    "columns of the $U$ matrix to form the new basis for our projected space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:29.769624Z",
     "start_time": "2019-11-11T20:08:29.694942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "(4,)\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "U, S, V = np.linalg.svd(Sigma2)\n",
    "print(U.shape)\n",
    "print(S.shape)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the U matrix, these are the $u$ vectors that define the PCA space, and if we want only\n",
    "the first $k$ dimensions, we take the first $k$ column vectors of the U matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:29.844452Z",
     "start_time": "2019-11-11T20:08:29.772454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36138659 -0.65658877  0.58202985  0.31548719]\n",
      " [ 0.08452251 -0.73016143 -0.59791083 -0.3197231 ]\n",
      " [-0.85667061  0.17337266 -0.07623608 -0.47983899]\n",
      " [-0.3582892   0.07548102 -0.54583143  0.75365743]]\n",
      "[[-0.36138659 -0.65658877]\n",
      " [ 0.08452251 -0.73016143]\n",
      " [-0.85667061  0.17337266]\n",
      " [-0.3582892   0.07548102]]\n"
     ]
    }
   ],
   "source": [
    "print(U)\n",
    "# if we wanted only 2 dimensions, we could take the first 2 columns\n",
    "U_reduce =  U[:,:2]\n",
    "print(U_reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Finally to perform our reduction/projection, we want to use the $U_{reduce}$ matrix.  So to get the new vector Z, we multiple the transpose of U_reduce times the original input data X\n",
    "\n",
    "\n",
    "For the iris data set, this should give us the same results as we got from calling PCA from the\n",
    "`scikit-learn` library.  Lets calculate our z values and plot them to compare to our previous figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:29.926765Z",
     "start_time": "2019-11-11T20:08:29.846072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "Z = np.dot(X_norm, U_reduce)\n",
    "print(X_norm.shape)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:30.067629Z",
     "start_time": "2019-11-11T20:08:29.927875Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(Z[y==0, 0], Z[y==0, 1], c='b', label='Setosa')\n",
    "plt.scatter(Z[y==1, 0], Z[y==1, 1], c='g', label='Versicolor')\n",
    "plt.scatter(Z[y==2, 0], Z[y==2, 1], c='r', label='Virginica')\n",
    "\n",
    "plt.xlabel('$z_0$')\n",
    "plt.ylabel('$z_1$')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the $z_0$ dimension (which is our dimension 0 using python 0 based indexing) is\n",
    "flipped in this figure.  As mentioned in the figures, PCA can return z or the -z of a vector.  Thus, to\n",
    "get exactly the same figure as we had before, we just need to negate our z vector for the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:30.145448Z",
     "start_time": "2019-11-11T20:08:30.069510Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(-Z[y==0, 0], Z[y==0, 1], c='b', label='Setosa')\n",
    "plt.scatter(-Z[y==1, 0], Z[y==1, 1], c='g', label='Versicolor')\n",
    "plt.scatter(-Z[y==2, 0], Z[y==2, 1], c='r', label='Virginica')\n",
    "\n",
    "plt.xlabel('$z_0$')\n",
    "plt.ylabel('$z_1$')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely at this figure and compare it to the figure produced by using the `scikit-learn`\n",
    "implementation of PCA, you should see that we got exactly the same result.\n",
    "\n",
    "Also as we will later discuss, the other matrices from the SVD contain useful information.  For example, previously\n",
    "we printed out the `components_` and the `explained_variance_ratio_` returned from PCA.  The `components` should\n",
    "be equivalent to our $U_{reduced}$ reduced principal components matrix.  Likewise the `explained_variance_ratio_`\n",
    "is the same as the $S$ vector from the SVD (normalized to sum up to 1):\n",
    "\n",
    "```python\n",
    ">>> print(pca.components_)\n",
    "[[ 0.36138659 -0.08452251  0.85667061  0.3582892 ]\n",
    " [ 0.65658877  0.73016143 -0.17337266 -0.07548102]]\n",
    ">>> print(pca.explained_variance_ratio_)\n",
    "[0.92461872 0.05306648]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:30.219923Z",
     "start_time": "2019-11-11T20:08:30.146480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36138659 -0.65658877]\n",
      " [ 0.08452251 -0.73016143]\n",
      " [-0.85667061  0.17337266]\n",
      " [-0.3582892   0.07548102]]\n",
      "[0.92461872 0.05306648 0.01710261 0.00521218]\n"
     ]
    }
   ],
   "source": [
    "print(U_reduce)\n",
    "print(S / np.sum(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W8 11: Choosing the Number of Principal Components\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=oF2T9bisq48&list=PLiPvV5TNogxIS4bHQVW4pMkj4CHA8COdX&index=101)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in this video, we can use the S matrix from the singular value decomposition to see how much\n",
    "variance each of the new dimensions discovered by the PCA explains.  I didn't mention it above, but we\n",
    "can get the S matrix that `scikit-learn` calculates, as well as the ratio of the variance explained\n",
    "by each of the Z dimensions, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:30.303620Z",
     "start_time": "2019-11-11T20:08:30.220966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.22824171 0.24267075]\n",
      "[0.92461872 0.05306648]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained variance is basically the S matrix from the SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:30.376995Z",
     "start_time": "2019-11-11T20:08:30.304783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.20005343 0.24105294 0.0776881  0.02367619]\n"
     ]
    }
   ],
   "source": [
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance ratio is simply obtained by dividing each variance measure by the total sum of the variance\n",
    "measures.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:30.469487Z",
     "start_time": "2019-11-11T20:08:30.378111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92461872 0.05306648 0.01710261 0.00521218]\n"
     ]
    }
   ],
   "source": [
    "print(S / np.sum(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in this video, the ratio of the variances in PCA can be interpreted as the amount of\n",
    "information each dimension holds.  In the case of the iris data set PCA, the first dimension explains\n",
    "92.5% of the variance of the data, and the second dimension another 5.3%, for a total of almost 98%\n",
    "of the variance represented by the first 2 dimensions.  Another way that I think of these measures,\n",
    "the first 2 dimensions of our PCA actually only introduce about 2% of projection error when projecting\n",
    "the original 4 dimensional data onto the discovered 2 principle component dimensions.\n",
    "\n",
    "Sometimes you will see people plot the variance ratios.  This isn't too useful for our iris data with\n",
    "only 4 dimensions total, but when you have a large number of dimensions, it can visually let you\n",
    "know which dimensions are important, and which add little and mostly represent redundant information\n",
    "or noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:30.582146Z",
     "start_time": "2019-11-11T20:08:30.470852Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(4), S/np.sum(S), 'bo')\n",
    "plt.plot(range(4), S/np.sum(S), 'k--')\n",
    "plt.axis([-0.5, 3.5, 0.0, 1.0])\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Ratio of Variance Explained');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the video, typically we choose the number of dimensions $k$ when doing reduction or compression\n",
    "to retain some percentage of the variance.  Typically we might choose to retain $0.99$ or $0.95$ ratio of the\n",
    "variance.  This means we need to look at the explained variance, and choose the dimension $k$ that is at or\n",
    "exceeds the target.  So instead of plotting the ratio like we did before, it is often more informative to \n",
    "plot the cumulative sum of the ratio explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T20:08:30.690560Z",
     "start_time": "2019-11-11T20:08:30.583589Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.plot(range(4), np.cumsum(S/np.sum(S)), 'k-')\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Cumulative Sum of Variance Explained');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another think to note, in practice it is often the case that a lot of dimensions are only capturing noise or are\n",
    "highly correlated with other dimensions.  So often it can be surprising how many dimensions will not be needed\n",
    "to retain 95% or even 99% of the variance.  Often a large number of dimensions are possible to be dropped while still\n",
    "retaining a high percentage of the variance of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W8 10: Reconstruction from Compressed Representation\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=OAqFxmnmA2I&list=PLiPvV5TNogxIS4bHQVW4pMkj4CHA8COdX&index=100)\n",
    "\n",
    "\n",
    "So we can project back to the original high dimensional space using the information from our PCA.\n",
    "\n",
    "\\begin{equation}\n",
    "X_{approx} = U_{reduce} \\times Z\n",
    "\\end{equation}\n",
    "\n",
    "Where $Z$ is the projected $k$ dimensional space of the samples of the data.\n",
    "\n",
    "So we could reconstruct the 4D iris data set back from the 2D representation we showed above, though of course we\n",
    "wouldn't really be able to visualize what this does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W8 12: Advice for Applying PCA\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=cL6bDCnGkUM&list=PLiPvV5TNogxIS4bHQVW4pMkj4CHA8COdX&index=102)\n",
    "\n",
    "PCA can be used to speed up the learning time of an algorithm.  For a supervised learning problem with $m$\n",
    "samples we might often have very high dimensional (big) data.  So the features of each sample could be in the 10s\n",
    "or 100s of thousands of features (or even bigger).  For example when doing processing on images, each pixel\n",
    "will be 1 feature (or even 3 features if doing color images).\n",
    "\n",
    "For big data, it is often useful to use dimensionality reduction to reduce the representation to a much\n",
    "lower number of dimensions.  This helps both because a) many of the low variance dimensions are just capturing\n",
    "noise, and are thus not useful, if not even actively harmful, to a supervised learner in modeling the data.\n",
    "Also when the data is reduced down in dimension significantly, this significantly simplifies the task that\n",
    "the supervied learning mechanism must perform.\n",
    "\n",
    "**NOTE**: The mapping of $x^{(i)} \\to z^{(i)}$ should be defined by running PCA **only** on the training set.\n",
    "This mapping can be applied as well to the examples in the cross-validation and test sets when needed.  But the\n",
    "mapping should never be learned from the test or cv data, because if you do this you are leaking information\n",
    "from your test data potentially back to your training information.\n",
    "\n",
    "## Application of PCA\n",
    "\n",
    "- Compression\n",
    "  - Reduce memory/disk needed to store data\n",
    "  - Speed up learning algorithm\n",
    "- Visualization\n",
    "\n",
    "For compression we will choose $k$ to retain some percentage of the variation in the data, typically 99% or 95%.\n",
    "But for visualization, of course, the goal is to better understand the data.  So in that case we must choose to\n",
    "reduce to $k=2$ or $k=3$ so we can plot a visualization of the data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
